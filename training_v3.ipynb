{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c885d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "from ultralytics.models import YOLO\n",
    "import torch\n",
    "import yaml\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b959d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import DisplayPath\n",
    "Path = DisplayPath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a0d47a",
   "metadata": {},
   "source": [
    "## Step 2: Configure Paths & Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5bae1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset path (created by e2e_data_prep.ipynb)\n",
    "YOLO_DATASET = Path(\"datasets/ready/full_dataset\")\n",
    "RUNS_DIR = Path(\"runs/segment\")\n",
    "\n",
    "# Verify dataset exists\n",
    "if not YOLO_DATASET.exists():\n",
    "    raise FileNotFoundError(f\"Dataset not found at {YOLO_DATASET}. Run e2e_data_prep.ipynb first!\")\n",
    "\n",
    "print(\"Dataset:\")\n",
    "YOLO_DATASET.display()\n",
    "print(\"  Train:\")\n",
    "(YOLO_DATASET / 'train').display()\n",
    "print(\"  Val:\")\n",
    "(YOLO_DATASET / 'val').display()\n",
    "print(\"  Test:\")\n",
    "(YOLO_DATASET / 'test').display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dc79bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "IMG_SIZE = 640\n",
    "model_type = \"yolo11n-seg.pt\"\n",
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8e652f",
   "metadata": {},
   "source": [
    "## Step 3: Verify Dataset Structure\n",
    "\n",
    "Dataset is already prepared by e2e_data_prep.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0e39b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dataset structure\n",
    "print(\"=\"*60)\n",
    "print(\"DATASET VERIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "splits = ['train', 'val', 'test']\n",
    "stats = {}\n",
    "\n",
    "for split in splits:\n",
    "    img_dir = YOLO_DATASET / split / \"images\"\n",
    "    lbl_dir = YOLO_DATASET / split / \"labels\"\n",
    "    \n",
    "    if img_dir.exists() and lbl_dir.exists():\n",
    "        num_images = len(list(img_dir.glob(\"*\")))\n",
    "        num_labels = len(list(lbl_dir.glob(\"*.txt\")))\n",
    "        stats[split] = {'images': num_images, 'labels': num_labels}\n",
    "        print(f\"{split.upper():5s}: {num_images:4d} images, {num_labels:4d} labels\")\n",
    "    else:\n",
    "        stats[split] = {'images': 0, 'labels': 0}\n",
    "        print(f\"{split.upper():5s}: Missing!\")\n",
    "\n",
    "total_images = sum(s['images'] for s in stats.values())\n",
    "total_labels = sum(s['labels'] for s in stats.values())\n",
    "\n",
    "print(f\"{'TOTAL':5s}: {total_images:4d} images, {total_labels:4d} labels\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if total_images == 0:\n",
    "    raise RuntimeError(\"No dataset found! Run e2e_data_prep.ipynb to create the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e32ee53",
   "metadata": {},
   "source": [
    "## Step 3.5: Analyze Class Distribution\n",
    "\n",
    "Check the distribution of classes in the training set to identify imbalances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2371c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze class distribution in training set\n",
    "from src.data_utils import count_class_instances\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CLASS DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "class_names = {0: 'Red Ball', 1: 'Human', 2: 'Trashcan'}\n",
    "\n",
    "for split in ['train']:\n",
    "    counts = count_class_instances(YOLO_DATASET, split)\n",
    "    total = sum(counts.values())\n",
    "\n",
    "    print(f\"\\n{split.upper()}:\")\n",
    "    for class_id, count in counts.items():\n",
    "        percentage = (count / total * 100) if total > 0 else 0\n",
    "        print(f\"  {class_names[class_id]:12s} (class {class_id}): {count:5d} instances ({percentage:5.1f}%)\")\n",
    "    print(f\"  {'TOTAL':12s}           : {total:5d} instances\")\n",
    "    \n",
    "    # Calculate imbalance ratio\n",
    "    if counts[2] > 0:  # If trashcans exist\n",
    "        max_count = max(counts.values())\n",
    "        min_count = min(v for v in counts.values() if v > 0)\n",
    "        imbalance_ratio = max_count / min_count\n",
    "        print(f\"  Imbalance ratio: {imbalance_ratio:.1f}x\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c5dd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import augmentation utilities\n",
    "import albumentations as A\n",
    "from src.augmentation import augment_class_dataset, augment_class_with_occlusion\n",
    "\n",
    "# Define custom augmentation pipelines\n",
    "trashcan_pipeline = A.Compose([  # type: ignore\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.3),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.8),\n",
    "    A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.8),\n",
    "    A.GaussNoise(var_limit=(10.0, 50.0), p=0.5),\n",
    "    A.RandomScale(scale_limit=0.3, p=0.7),\n",
    "    A.Affine(rotate=(-20, 20), translate_percent=0.1, scale=(0.8, 1.2), shear=(-10, 10), p=0.7),\n",
    "], keypoint_params=A.KeypointParams(format='xy', remove_invisible=False))\n",
    "\n",
    "# Execute augmentation\n",
    "DO_AUGMENTATION = True \n",
    "if DO_AUGMENTATION:\n",
    "    # 1. Augment Trashcans (Class 2) - Strong augmentation\n",
    "    print(\"Augmenting Trashcans...\")\n",
    "    augment_class_dataset(\n",
    "        dataset_path=YOLO_DATASET,\n",
    "        class_id=2,\n",
    "        num_augmentations=20,\n",
    "        aug_config=trashcan_pipeline\n",
    "    )\n",
    "    \n",
    "    # 2. Augment Red Balls (Class 0) - Occlusion & Scaling (Hard)\n",
    "    # This creates smaller balls and balls with holes (occluded), updating labels automatically\n",
    "    print(\"Augmenting Red Balls with Occlusion...\")\n",
    "    augment_class_with_occlusion(\n",
    "        dataset_path=YOLO_DATASET,\n",
    "        class_id=0,\n",
    "        num_augmentations=5\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbed63c",
   "metadata": {},
   "source": [
    "### ðŸ”„ Reset: Clean Augmented Images\n",
    "\n",
    "If you accidentally ran augmentation multiple times, use this to remove all augmented copies and start fresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f63aac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.augmentation import clean_augmented_images\n",
    "\n",
    "# Execute cleanup (set to True to clean)\n",
    "CLEAN_AUGMENTED = False\n",
    "\n",
    "if CLEAN_AUGMENTED:\n",
    "    removed = clean_augmented_images(YOLO_DATASET)\n",
    "else:\n",
    "    print(\"Set CLEAN_AUGMENTED = True to remove all augmented images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b979e79",
   "metadata": {},
   "source": [
    "## Step 4: Create YOLO Configuration File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739a4bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {\n",
    "    'red ball': 0,\n",
    "    'human': 1,\n",
    "    'trashcan': 2\n",
    "}\n",
    "\n",
    "config = {\n",
    "    'path': str(YOLO_DATASET.absolute()),\n",
    "    'train': 'train/images',\n",
    "    'val': 'val/images',\n",
    "    'nc': len(classes),\n",
    "    'names': list(classes.keys())\n",
    "}\n",
    "\n",
    "config_path = YOLO_DATASET / 'data.yaml'\n",
    "with open(config_path, 'w') as f:\n",
    "    yaml.dump(config, f, default_flow_style=False)\n",
    "\n",
    "print(f\"âœ“ Configuration saved: {config_path}\")\n",
    "print(\"Dataset structure:\")\n",
    "YOLO_DATASET.display()\n",
    "print(\"  Train:\")\n",
    "(YOLO_DATASET / 'train').display()\n",
    "print(\"  Val:\")\n",
    "(YOLO_DATASET / 'val').display()\n",
    "print(\"  Test:\")\n",
    "(YOLO_DATASET / 'test').display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db1af91",
   "metadata": {},
   "source": [
    "## Step 4.5: Select Monitoring Images\n",
    "\n",
    "Select diverse validation images covering all classes to monitor training progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8f255d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_utils import select_diverse_monitoring_images\n",
    "\n",
    "# Select diverse monitoring images\n",
    "val_labels_dir = YOLO_DATASET / 'val' / 'labels'\n",
    "val_images_dir = YOLO_DATASET / 'val' / 'images'\n",
    "\n",
    "MONITOR_IMAGES = select_diverse_monitoring_images(\n",
    "    val_labels_dir, \n",
    "    val_images_dir, \n",
    "    images_per_class=3,\n",
    "    include_mixed=True\n",
    ")\n",
    "\n",
    "if len(MONITOR_IMAGES) == 0:\n",
    "    print(\"âš ï¸  Warning: No monitoring images found in validation set!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0cd691",
   "metadata": {},
   "source": [
    "## Step 4.6: Define Multi-Class Monitoring Callback\n",
    "\n",
    "Create a callback that visualizes segmentation progress for all classes at each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9e4bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.visualization import create_monitoring_callback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853894a7",
   "metadata": {},
   "source": [
    "## Step 5: Train Model with Multi-Class Monitoring\n",
    "\n",
    "Train YOLOv11 with:\n",
    "- Data augmentation on train set (reduced for abundant classes)\n",
    "- Trashcan pre-augmentation for class balance\n",
    "- Checkpoints saved for best model\n",
    "- Validation after each epoch\n",
    "- **Custom callback to monitor all classes segmentation progress**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0c5a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained model\n",
    "model = YOLO(model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4f6fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = 'ball_person_trashcan_model_v5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3f23d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup multi-class monitoring\n",
    "monitor_output_dir = RUNS_DIR / project_name / 'training_monitor'\n",
    "print(f\"Multi-class monitoring output: {monitor_output_dir}\")\n",
    "\n",
    "# Add callback\n",
    "callback_fn = create_monitoring_callback(\n",
    "    model=model,\n",
    "    monitor_images=MONITOR_IMAGES,\n",
    "    output_dir=monitor_output_dir,\n",
    "    project_name=project_name\n",
    ")\n",
    "\n",
    "model.add_callback('on_train_epoch_end', callback_fn)\n",
    "print(\"âœ“ Multi-class monitoring callback registered\")\n",
    "print(f\"   Monitoring {len(MONITOR_IMAGES)} images covering all classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a8ad51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "head_idx = next((i for i, m in enumerate(model.model.model) if 'Detect' in m.__class__.__name__ or 'Segment' in m.__class__.__name__), len(model.model.model) - 1)\n",
    "\n",
    "results = model.train(\n",
    "    data=str(config_path),\n",
    "    epochs=10,\n",
    "    freeze=list(range(head_idx)),\n",
    "    batch=BATCH_SIZE,\n",
    "    imgsz=IMG_SIZE,\n",
    "    device=DEVICE,\n",
    "    project=str(RUNS_DIR),\n",
    "    name=project_name,\n",
    "    exist_ok=True,\n",
    "    \n",
    "    # Checkpointing\n",
    "    save=True,\n",
    "    save_period=5,  # Save every 5 epochs\n",
    "    \n",
    "    # Validation\n",
    "    val=True,\n",
    "    \n",
    "    # Data augmentation\n",
    "    # **AUG_CONFIG,\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer='Adam',\n",
    "    lr0=0.001,\n",
    "    lrf=0.01,\n",
    "    momentum=0.937,\n",
    "    weight_decay=0.0005,\n",
    "    \n",
    "    # Loss weights - AjustÃ© pour dataset avec trashcans augmentÃ©es\n",
    "    # Avec l'augmentation massive des trashcans, on peut rÃ©duire cls\n",
    "    box=7.5,\n",
    "    cls=1.0,      # RÃ©duit de 20.0 Ã  1.0 car trashcans maintenant bien reprÃ©sentÃ©es\n",
    "    dfl=1.5,\n",
    "    \n",
    "    # Other\n",
    "    patience=20,  # Early stopping\n",
    "    workers=8,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573f1e2e",
   "metadata": {},
   "source": [
    "## Step 6: Load Best Model & Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcded3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = RUNS_DIR / project_name / 'weights' / 'best.pt'\n",
    "best_model_path.display()\n",
    "model = YOLO(best_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4317200b",
   "metadata": {},
   "source": [
    "## Step 7: Evaluate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02a0a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation metrics\n",
    "metrics = model.val()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VALIDATION METRICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Box mAP50: {metrics.box.map50:.4f}\")\n",
    "print(f\"Box mAP50-95: {metrics.box.map:.4f}\")\n",
    "print(f\"Mask mAP50: {metrics.seg.map50:.4f}\")\n",
    "print(f\"Mask mAP50-95: {metrics.seg.map:.4f}\")\n",
    "\n",
    "# Per-class metrics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PER-CLASS METRICS (Segmentation)\")\n",
    "print(\"=\"*60)\n",
    "class_names = ['red ball', 'human', 'trashcan']\n",
    "for i, class_name in enumerate(class_names):\n",
    "    try:\n",
    "        map50 = metrics.seg.map50_per_class[i] if hasattr(metrics.seg, 'map50_per_class') else 0\n",
    "        map_val = metrics.seg.map_per_class[i] if hasattr(metrics.seg, 'map_per_class') else 0\n",
    "        print(f\"{class_name:12s}: mAP50={map50:.4f}, mAP50-95={map_val:.4f}\")\n",
    "    except:\n",
    "        print(f\"{class_name:12s}: metrics not available\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d36b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best checkpoint\n",
    "model_dir = RUNS_DIR / project_name\n",
    "best_model = model_dir / 'weights' / 'best.pt'\n",
    "last_model = model_dir / 'weights' / 'last.pt'\n",
    "\n",
    "print(f\"Best model: \")\n",
    "best_model.display()\n",
    "print(f\"Last model: \")\n",
    "last_model.display()\n",
    "print(f\"Results: \")\n",
    "model_dir.display()\n",
    "print(f\"\\nMulti-class monitoring visualizations: \")\n",
    "(model_dir / 'training_monitor').display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f94a189",
   "metadata": {},
   "source": [
    "## Step 8: Visualize Training Progress Evolution\n",
    "\n",
    "Review monitoring visualizations showing how segmentation improved for all classes over epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac1cf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all monitoring visualizations\n",
    "monitor_dir = RUNS_DIR / project_name / 'training_monitor'\n",
    "\n",
    "if monitor_dir.exists():\n",
    "    viz_files = sorted(monitor_dir.glob(\"epoch_*.jpg\"))\n",
    "    print(f\"Found {len(viz_files)} monitoring visualizations:\")\n",
    "    for viz_file in viz_files:\n",
    "        viz_file.display()\n",
    "    \n",
    "    if len(viz_files) > 0:\n",
    "        print(f\"\\nðŸ’¡ Tip: Open the images in {monitor_dir} to see how segmentation evolved for all classes!\")\n",
    "        print(f\"   You can use an image viewer or VS Code to flip through them chronologically.\")\n",
    "        print(f\"   Each image shows detections with: Balls | Humans | Trashcans\")\n",
    "else:\n",
    "    print(\"No monitoring visualizations found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bc9c5e",
   "metadata": {},
   "source": [
    "## Step 9: Test on Sample Images (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f791323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on validation images (sample from val set)\n",
    "test_images = list((YOLO_DATASET / \"val\" / \"images\").glob(\"*\"))\n",
    "\n",
    "print(f\"Testing on {len(test_images)} sample images...\")\n",
    "\n",
    "for img_path in test_images:\n",
    "    results = model.predict(str(img_path), save=True, conf=0.1)\n",
    "    print(f\"  âœ“ {img_path.name}\")\n",
    "\n",
    "print(f\"\\nResults saved to:\")\n",
    "(RUNS_DIR / project_name).display()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
