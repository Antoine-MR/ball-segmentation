{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb0e6ee9",
   "metadata": {},
   "source": [
    "## Step 1: Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "446eb0fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ModelDependencyMissing: Your `inference` configuration does not support SAM model. Use pip install 'inference[sam]' to install missing requirements.To suppress this warning, set CORE_MODEL_SAM_ENABLED to False.\n",
      "ModelDependencyMissing: Your `inference` configuration does not support SAM2 model. Use pip install 'inference[sam]' to install missing requirements.To suppress this warning, set CORE_MODEL_SAM2_ENABLED to False.\n",
      "ModelDependencyMissing: Your `inference` configuration does not support SAM3 model. Install SAM3 dependencies and set CORE_MODEL_SAM3_ENABLED to True.\n",
      "ModelDependencyMissing: Your `inference` configuration does not support Gaze Detection model. Use pip install 'inference[gaze]' to install missing requirements.To suppress this warning, set CORE_MODEL_GAZE_ENABLED to False.\n",
      "ModelDependencyMissing: Your `inference` configuration does not support Gaze Detection model. Use pip install 'inference[gaze]' to install missing requirements.To suppress this warning, set CORE_MODEL_GAZE_ENABLED to False.\n",
      "ModelDependencyMissing: Your `inference` configuration does not support GroundingDINO model. Use pip install 'inference[grounding-dino]' to install missing requirements.To suppress this warning, set CORE_MODEL_GROUNDINGDINO_ENABLED to False.\n",
      "ModelDependencyMissing: Your `inference` configuration does not support GroundingDINO model. Use pip install 'inference[grounding-dino]' to install missing requirements.To suppress this warning, set CORE_MODEL_GROUNDINGDINO_ENABLED to False.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "from ultralytics.models import YOLO\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.detection import RoboflowDetector\n",
    "from src.segmentation import FastSAMSegmenter\n",
    "from src.pipeline import img_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0aafad",
   "metadata": {},
   "source": [
    "## Step 2: Configure Paths & Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db68ce80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n",
      "Input: datasets/raw/IRL_validation_pictures\n",
      "Output: datasets/ready/IRL_dataset\n",
      "Found 34 images\n"
     ]
    }
   ],
   "source": [
    "# Input path\n",
    "IRL_RAW = Path(\"datasets/raw/IRL_validation_pictures\")\n",
    "\n",
    "# Output paths\n",
    "IRL_READY = Path(\"datasets/ready/IRL_dataset\")\n",
    "IRL_IMAGES = IRL_READY / \"images\"\n",
    "IRL_LABELS = IRL_READY / \"labels\"\n",
    "\n",
    "# Intermediate outputs for ball detection+segmentation\n",
    "BALL_DET_OUTPUT = Path(\"datasets/preprocessed/irl_balls/detection\")\n",
    "BALL_SEG_OUTPUT = Path(\"datasets/preprocessed/irl_balls/segmentation\")\n",
    "BALL_TXT_OUTPUT = Path(\"datasets/preprocessed/irl_balls/labels\")\n",
    "\n",
    "# Create directories\n",
    "for dir_path in [IRL_IMAGES, IRL_LABELS, BALL_DET_OUTPUT, BALL_SEG_OUTPUT, BALL_TXT_OUTPUT]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Device\n",
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Input: {IRL_RAW}\")\n",
    "print(f\"Output: {IRL_READY}\")\n",
    "\n",
    "# Get image list\n",
    "img_paths = list(IRL_RAW.glob(\"*.jpg\")) + list(IRL_RAW.glob(\"*.jpeg\")) + \\\n",
    "            list(IRL_RAW.glob(\"*.JPG\")) + list(IRL_RAW.glob(\"*.JPEG\"))\n",
    "print(f\"Found {len(img_paths)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3672e5da",
   "metadata": {},
   "source": [
    "## Step 3: Load Models\n",
    "\n",
    "- **Ball**: Roboflow detector (red-ball-detection-new/1) + FastSAM segmenter\n",
    "- **Person**: YOLO-seg pretrained (yolo11n-seg.pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f52222e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'AzureExecutionProvider, CPUExecutionProvider'\n",
      "UserWarning: Specified provider 'OpenVINOExecutionProvider' is not in available provider names.Available providers: 'AzureExecutionProvider, CPUExecutionProvider'\n",
      "UserWarning: Specified provider 'CoreMLExecutionProvider' is not in available provider names.Available providers: 'AzureExecutionProvider, CPUExecutionProvider'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Ball detector (Roboflow): red-ball-detection-new/1\n",
      "âœ“ Ball segmenter: FastSAM\n",
      "âœ“ Person model: models/pretrained/yolo11n-seg.pt\n"
     ]
    }
   ],
   "source": [
    "# Ball detection + segmentation pipeline (Roboflow + FastSAM)\n",
    "# Model ID is hardcoded in RoboflowDetector, API key loaded from .env\n",
    "ball_detector = RoboflowDetector()\n",
    "ball_segmenter = FastSAMSegmenter()\n",
    "\n",
    "# Person model (pretrained YOLO-seg)\n",
    "PERSON_MODEL_PATH = Path('models/pretrained/yolo11n-seg.pt')\n",
    "person_model = YOLO(str(PERSON_MODEL_PATH))\n",
    "\n",
    "print(f\"âœ“ Ball detector (Roboflow): {ball_detector.DEFAULT_MODEL_ID}\")\n",
    "print(f\"âœ“ Ball segmenter: FastSAM\")\n",
    "print(f\"âœ“ Person model: {PERSON_MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71801c0",
   "metadata": {},
   "source": [
    "## Step 4: Segment Balls (Roboflow â†’ FastSAM â†’ YOLO txt)\n",
    "\n",
    "Process each image through the detection+segmentation pipeline.\n",
    "Output: YOLO polygon format in `txt_output_folder/irl_balls/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6014c371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 34 images for ball segmentation...\n",
      "Pipeline: Roboflow detection â†’ FastSAM segmentation â†’ YOLO txt\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ball segmentation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [06:13<00:00, 10.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Ball segmentation complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Processing {len(img_paths)} images for ball segmentation...\")\n",
    "print(f\"Pipeline: Roboflow detection â†’ FastSAM segmentation â†’ YOLO txt\")\n",
    "print()\n",
    "\n",
    "for img_path in tqdm(img_paths, desc=\"Ball segmentation\"):\n",
    "    img_pipeline(\n",
    "        img_path,\n",
    "        detect_fn=ball_detector.detect,\n",
    "        segment_fn=ball_segmenter.segment_bbox,\n",
    "        det_output_dir=BALL_DET_OUTPUT,\n",
    "        seg_output_dir=BALL_SEG_OUTPUT,\n",
    "        txt_output_dir=BALL_TXT_OUTPUT,\n",
    "        mode=\"bbox\"\n",
    "    )\n",
    "\n",
    "print(\"âœ“ Ball segmentation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf17493",
   "metadata": {},
   "source": [
    "## Step 5: Combine Ball + Person Masks\n",
    "\n",
    "- Parse ball polygons from txt files\n",
    "- Segment persons with YOLO-seg\n",
    "- Combine into PNG masks (ball=0, person=1)\n",
    "- **Ball has priority** over person in overlapping regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a05559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONF_THRESHOLD = 0.5  # Person confidence threshold\n",
    "\n",
    "# Statistics\n",
    "stats = {\"total\": 0, \"with_ball\": 0, \"with_person\": 0, \"empty\": 0}\n",
    "\n",
    "print(f\"Combining ball + person masks...\")\n",
    "print(f\"Confidence threshold (person): {CONF_THRESHOLD}\")\n",
    "print(f\"Class priority: ball > person\")\n",
    "print()\n",
    "\n",
    "for img_path in tqdm(img_paths, desc=\"Combining masks\"):\n",
    "    stats[\"total\"] += 1\n",
    "    \n",
    "    # Load image to get dimensions\n",
    "    img = Image.open(img_path)\n",
    "    h, w = img.height, img.width\n",
    "    \n",
    "    # Initialize combined mask (all background)\n",
    "    combined_mask = np.zeros((h, w), dtype=np.uint8)\n",
    "    has_detections = False\n",
    "    \n",
    "    # --- 1. Parse ball segmentation from txt (if exists) ---\n",
    "    ball_txt_path = BALL_TXT_OUTPUT / (img_path.stem + '.txt')\n",
    "    if ball_txt_path.exists():\n",
    "        with open(ball_txt_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            \n",
    "        for line in lines:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) < 7:  # Need at least class_id + 3 points (6 coords)\n",
    "                continue\n",
    "            \n",
    "            # Extract normalized coordinates\n",
    "            coords = [float(p) for p in parts[1:]]\n",
    "            \n",
    "            # Convert to pixel coordinates\n",
    "            points = []\n",
    "            for i in range(0, len(coords), 2):\n",
    "                x = int(coords[i] * w)\n",
    "                y = int(coords[i+1] * h)\n",
    "                points.append([x, y])\n",
    "            \n",
    "            # Fill polygon with ball class (0)\n",
    "            points_array = np.array(points, dtype=np.int32)\n",
    "            cv2.fillPoly(combined_mask, [points_array], 0)\n",
    "            has_detections = True\n",
    "        \n",
    "        stats[\"with_ball\"] += 1\n",
    "    \n",
    "    # --- 2. Segment Persons (class 1) ---\n",
    "    person_results = person_model.predict(\n",
    "        str(img_path), \n",
    "        classes=[0],  # Person class in COCO\n",
    "        conf=CONF_THRESHOLD, \n",
    "        device=DEVICE,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    if person_results[0].masks is not None:\n",
    "        for mask in person_results[0].masks.data:\n",
    "            mask_np = (mask.cpu().numpy() > 0.5).astype(np.uint8)\n",
    "            \n",
    "            # Resize if needed\n",
    "            if mask_np.shape != (h, w):\n",
    "                mask_np = cv2.resize(mask_np, (w, h), interpolation=cv2.INTER_NEAREST)\n",
    "            \n",
    "            # Add person ONLY where background (ensures ball priority)\n",
    "            person_area = (combined_mask == 0) & (mask_np == 1)\n",
    "            combined_mask[person_area] = 1\n",
    "            has_detections = True\n",
    "        \n",
    "        stats[\"with_person\"] += 1\n",
    "    \n",
    "    # Track empty images\n",
    "    if not has_detections:\n",
    "        stats[\"empty\"] += 1\n",
    "    \n",
    "    # Save mask (even if empty)\n",
    "    mask_img = Image.fromarray(combined_mask, mode='L')\n",
    "    mask_img.save(IRL_LABELS / (img_path.stem + '.png'))\n",
    "    \n",
    "    # Copy original image\n",
    "    shutil.copy(img_path, IRL_IMAGES / img_path.name)\n",
    "\n",
    "print(\"\\nâœ“ Processing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a694291",
   "metadata": {},
   "source": [
    "## Step 6: Display Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd7bdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"ðŸ“Š DATASET STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total images processed:    {stats['total']}\")\n",
    "print(f\"Images with ball(s):       {stats['with_ball']} ({stats['with_ball']/stats['total']*100:.1f}%)\")\n",
    "print(f\"Images with person(s):     {stats['with_person']} ({stats['with_person']/stats['total']*100:.1f}%)\")\n",
    "print(f\"Images with no detections: {stats['empty']} ({stats['empty']/stats['total']*100:.1f}%)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Verify dataset consistency\n",
    "num_images = len(list(IRL_IMAGES.glob(\"*\")))\n",
    "num_labels = len(list(IRL_LABELS.glob(\"*.png\")))\n",
    "\n",
    "print(f\"\\nâœ“ Dataset consistency check:\")\n",
    "print(f\"  Images: {num_images}\")\n",
    "print(f\"  Labels: {num_labels}\")\n",
    "print(f\"  Match: {'âœ“ YES' if num_images == num_labels else 'âœ— NO'}\")\n",
    "\n",
    "print(f\"\\nâœ“ Dataset ready at: {IRL_READY}\")\n",
    "print(f\"  - images/  ({num_images} files)\")\n",
    "print(f\"  - labels/  ({num_labels} .png masks)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
