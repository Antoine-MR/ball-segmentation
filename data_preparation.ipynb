{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75c058f3",
   "metadata": {},
   "source": [
    "# Segment IRL Validation picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2a87a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ModelDependencyMissing: Your `inference` configuration does not support SAM model. Use pip install 'inference[sam]' to install missing requirements.To suppress this warning, set CORE_MODEL_SAM_ENABLED to False.\n",
      "ModelDependencyMissing: Your `inference` configuration does not support SAM2 model. Use pip install 'inference[sam]' to install missing requirements.To suppress this warning, set CORE_MODEL_SAM2_ENABLED to False.\n",
      "ModelDependencyMissing: Your `inference` configuration does not support SAM3 model. Install SAM3 dependencies and set CORE_MODEL_SAM3_ENABLED to True.\n",
      "ModelDependencyMissing: Your `inference` configuration does not support Gaze Detection model. Use pip install 'inference[gaze]' to install missing requirements.To suppress this warning, set CORE_MODEL_GAZE_ENABLED to False.\n",
      "ModelDependencyMissing: Your `inference` configuration does not support Gaze Detection model. Use pip install 'inference[gaze]' to install missing requirements.To suppress this warning, set CORE_MODEL_GAZE_ENABLED to False.\n",
      "ModelDependencyMissing: Your `inference` configuration does not support GroundingDINO model. Use pip install 'inference[grounding-dino]' to install missing requirements.To suppress this warning, set CORE_MODEL_GROUNDINGDINO_ENABLED to False.\n",
      "ModelDependencyMissing: Your `inference` configuration does not support GroundingDINO model. Use pip install 'inference[grounding-dino]' to install missing requirements.To suppress this warning, set CORE_MODEL_GROUNDINGDINO_ENABLED to False.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "from ultralytics.models import YOLO\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.detection import RoboflowDetector\n",
    "from src.segmentation import FastSAMSegmenter\n",
    "from src.pipeline import img_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6449608e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ball detection + segmentation pipeline (Roboflow + FastSAM)\n",
    "# Model ID is hardcoded, API key is loaded from .env file\n",
    "ball_detector = RoboflowDetector()  # Uses default model_id and ROBOFLOW_API_KEY env var\n",
    "ball_segmenter = FastSAMSegmenter()\n",
    "\n",
    "# Person model (pretrained YOLO-seg)\n",
    "PERSON_MODEL_PATH = Path('models/pretrained/yolo11n-seg.pt')\n",
    "person_model = YOLO(str(PERSON_MODEL_PATH))\n",
    "\n",
    "print(f\"âœ“ Ball detector (Roboflow): {ball_detector.DEFAULT_MODEL_ID}\")\n",
    "print(f\"âœ“ Ball segmenter: FastSAM\")\n",
    "print(f\"âœ“ Person model: {PERSON_MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c747f257",
   "metadata": {},
   "source": [
    "## Step 6: Display Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdbc718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confidence threshold\n",
    "CONF_THRESHOLD = 0.5\n",
    "\n",
    "# Statistics\n",
    "stats = {\"total\": 0, \"with_ball\": 0, \"with_person\": 0, \"empty\": 0}\n",
    "\n",
    "print(f\"Combining ball + person masks...\")\n",
    "print(f\"Confidence threshold (person): {CONF_THRESHOLD}\")\n",
    "print(f\"Class priority: ball > person\")\n",
    "print()\n",
    "\n",
    "for img_path in tqdm(img_paths, desc=\"Combining masks\"):\n",
    "    stats[\"total\"] += 1\n",
    "    \n",
    "    # Load image to get dimensions\n",
    "    img = Image.open(img_path)\n",
    "    h, w = img.height, img.width\n",
    "    \n",
    "    # Initialize combined mask (all background)\n",
    "    combined_mask = np.zeros((h, w), dtype=np.uint8)\n",
    "    has_detections = False\n",
    "    \n",
    "    # --- 1. Load ball segmentation from txt (if exists) ---\n",
    "    ball_txt_path = BALL_TXT_OUTPUT / (img_path.stem + '.txt')\n",
    "    if ball_txt_path.exists():\n",
    "        # Parse YOLO polygon format: \"class_id x1 y1 x2 y2 ...\"\n",
    "        with open(ball_txt_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            \n",
    "        for line in lines:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) < 7:  # Need at least class_id + 3 points (6 coords)\n",
    "                continue\n",
    "            \n",
    "            # Extract normalized coordinates\n",
    "            coords = [float(p) for p in parts[1:]]\n",
    "            \n",
    "            # Convert to pixel coordinates\n",
    "            points = []\n",
    "            for i in range(0, len(coords), 2):\n",
    "                x = int(coords[i] * w)\n",
    "                y = int(coords[i+1] * h)\n",
    "                points.append([x, y])\n",
    "            \n",
    "            # Fill polygon with ball class (0)\n",
    "            points_array = np.array(points, dtype=np.int32)\n",
    "            cv2.fillPoly(combined_mask, [points_array], 0)\n",
    "            has_detections = True\n",
    "        \n",
    "        stats[\"with_ball\"] += 1\n",
    "    \n",
    "    # --- 2. Segment Persons (classe 1) ---\n",
    "    person_results = person_model.predict(\n",
    "        str(img_path), \n",
    "        classes=[0],  # Person class in COCO\n",
    "        conf=CONF_THRESHOLD, \n",
    "        device=DEVICE,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    if person_results[0].masks is not None:\n",
    "        for mask in person_results[0].masks.data:\n",
    "            mask_np = (mask.cpu().numpy() > 0.5).astype(np.uint8)\n",
    "            \n",
    "            # Resize if needed (preserve class ids with INTER_NEAREST)\n",
    "            if mask_np.shape != (h, w):\n",
    "                mask_np = cv2.resize(mask_np, (w, h), interpolation=cv2.INTER_NEAREST)\n",
    "            \n",
    "            # Add person mask ONLY where combined_mask is still 0 (background)\n",
    "            # This ensures ball (class 0) has priority\n",
    "            person_area = (combined_mask == 0) & (mask_np == 1)\n",
    "            combined_mask[person_area] = 1  # classe person\n",
    "            has_detections = True\n",
    "        \n",
    "        stats[\"with_person\"] += 1\n",
    "    \n",
    "    # Track empty images\n",
    "    if not has_detections:\n",
    "        stats[\"empty\"] += 1\n",
    "    \n",
    "    # Save mask (even if empty = all zeros)\n",
    "    mask_img = Image.fromarray(combined_mask, mode='L')\n",
    "    mask_img.save(IRL_LABELS / (img_path.stem + '.png'))\n",
    "    \n",
    "    # Copy original image\n",
    "    shutil.copy(img_path, IRL_IMAGES / img_path.name)\n",
    "\n",
    "print(\"\\nâœ“ Processing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851d7ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"ðŸ“Š DATASET STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total images processed:    {stats['total']}\")\n",
    "print(f\"Images with ball(s):       {stats['with_ball']} ({stats['with_ball']/stats['total']*100:.1f}%)\")\n",
    "print(f\"Images with person(s):     {stats['with_person']} ({stats['with_person']/stats['total']*100:.1f}%)\")\n",
    "print(f\"Images with no detections: {stats['empty']} ({stats['empty']/stats['total']*100:.1f}%)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Verify dataset consistency\n",
    "num_images = len(list(IRL_IMAGES.glob(\"*\")))\n",
    "num_labels = len(list(IRL_LABELS.glob(\"*.png\")))\n",
    "\n",
    "print(f\"\\nâœ“ Dataset consistency check:\")\n",
    "print(f\"  Images: {num_images}\")\n",
    "print(f\"  Labels: {num_labels}\")\n",
    "print(f\"  Match: {'âœ“ YES' if num_images == num_labels else 'âœ— NO'}\")\n",
    "\n",
    "print(f\"\\nâœ“ Dataset ready at: {IRL_READY}\")\n",
    "print(f\"  - images/  ({num_images} files)\")\n",
    "print(f\"  - labels/  ({num_labels} .png masks)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaee0bc4",
   "metadata": {},
   "source": [
    "## Step 5: Segment Persons & Combine Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1af9a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all images\n",
    "img_paths = list(IRL_RAW.glob(\"*.jpg\")) + list(IRL_RAW.glob(\"*.jpeg\")) + \\\n",
    "            list(IRL_RAW.glob(\"*.JPG\")) + list(IRL_RAW.glob(\"*.JPEG\"))\n",
    "\n",
    "print(f\"Processing {len(img_paths)} images for ball segmentation...\")\n",
    "print(f\"Pipeline: Roboflow detection â†’ FastSAM segmentation â†’ YOLO txt\")\n",
    "print()\n",
    "\n",
    "# Process each image with ball detection + segmentation pipeline\n",
    "for img_path in tqdm(img_paths, desc=\"Ball segmentation\"):\n",
    "    img_pipeline(\n",
    "        img_path,\n",
    "        detect_fn=ball_detector.detect,\n",
    "        segment_fn=ball_segmenter.segment_bbox,\n",
    "        det_output_dir=BALL_DET_OUTPUT,\n",
    "        seg_output_dir=BALL_SEG_OUTPUT,\n",
    "        txt_output_dir=BALL_TXT_OUTPUT,\n",
    "        mode=\"bbox\"  # Use bbox mode for FastSAM\n",
    "    )\n",
    "\n",
    "print(\"âœ“ Ball segmentation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2b4178",
   "metadata": {},
   "source": [
    "## Step 4: Segment Balls (Roboflow + FastSAM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108e6d94",
   "metadata": {},
   "source": [
    "## Step 3: Load Models & Configure Roboflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca805926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input path\n",
    "IRL_RAW = Path(\"datasets/raw/IRL_validation_pictures\")\n",
    "\n",
    "# Output paths\n",
    "IRL_READY = Path(\"datasets/ready/IRL_dataset\")\n",
    "IRL_IMAGES = IRL_READY / \"images\"\n",
    "IRL_LABELS = IRL_READY / \"labels\"\n",
    "\n",
    "# Intermediate outputs for ball detection+segmentation\n",
    "BALL_DET_OUTPUT = Path(\"detection_output_folder/irl_balls\")\n",
    "BALL_SEG_OUTPUT = Path(\"seg_output_folder/irl_balls\")\n",
    "BALL_TXT_OUTPUT = Path(\"txt_output_folder/irl_balls\")\n",
    "\n",
    "# Create directories\n",
    "for dir_path in [IRL_IMAGES, IRL_LABELS, BALL_DET_OUTPUT, BALL_SEG_OUTPUT, BALL_TXT_OUTPUT]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Device\n",
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Input: {IRL_RAW}\")\n",
    "print(f\"Output: {IRL_READY}\")\n",
    "print(f\"Found {len(list(IRL_RAW.glob('*.jpg')) + list(IRL_RAW.glob('*.jpeg')) + list(IRL_RAW.glob('*.JPG')) + list(IRL_RAW.glob('*.JPEG')))} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a432d4f",
   "metadata": {},
   "source": [
    "## Step 2: Configure Paths & Create Directories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31993df6",
   "metadata": {},
   "source": [
    "## Step 1: Setup & Imports"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
