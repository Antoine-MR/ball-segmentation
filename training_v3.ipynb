{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5c885d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "from ultralytics.models import YOLO\n",
    "import torch\n",
    "import yaml\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17b959d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import DisplayPath\n",
    "Path = DisplayPath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a0d47a",
   "metadata": {},
   "source": [
    "## Step 2: Configure Paths & Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a5bae1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "[datasets/ready/full_dataset](datasets/ready/full_dataset)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "[datasets/ready/full_dataset/train](datasets/ready/full_dataset/train)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Val:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "[datasets/ready/full_dataset/val](datasets/ready/full_dataset/val)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Test:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "[datasets/ready/full_dataset/test](datasets/ready/full_dataset/test)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Dataset path (created by e2e_data_prep.ipynb)\n",
    "YOLO_DATASET = Path(\"datasets/ready/full_dataset\")\n",
    "RUNS_DIR = Path(\"runs/segment\")\n",
    "\n",
    "# Verify dataset exists\n",
    "if not YOLO_DATASET.exists():\n",
    "    raise FileNotFoundError(f\"Dataset not found at {YOLO_DATASET}. Run e2e_data_prep.ipynb first!\")\n",
    "\n",
    "print(\"Dataset:\")\n",
    "YOLO_DATASET.display()\n",
    "print(\"  Train:\")\n",
    "(YOLO_DATASET / 'train').display()\n",
    "print(\"  Val:\")\n",
    "(YOLO_DATASET / 'val').display()\n",
    "print(\"  Test:\")\n",
    "(YOLO_DATASET / 'test').display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11dc79bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n",
      "GPU: NVIDIA GeForce RTX 3080 Laptop GPU\n",
      "CUDA: 12.8\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "BATCH_SIZE = 16\n",
    "IMG_SIZE = 640\n",
    "model_type = \"yolo11n-seg.pt\"\n",
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8976809f",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUG_CONFIG = {\n",
    "    'hsv_h': 0.015,  # Hue augmentation\n",
    "    'hsv_s': 0.7,    # Saturation\n",
    "    'hsv_v': 0.4,    # Value\n",
    "    'degrees': 10.0,  # Rotation\n",
    "    'translate': 0.1, # Translation\n",
    "    'scale': 0.5,     # Scaling\n",
    "    'shear': 0.0,     # Shearing\n",
    "    'perspective': 0.0, # Perspective\n",
    "    'flipud': 0.0,    # Vertical flip\n",
    "    'fliplr': 0.5,    # Horizontal flip\n",
    "    'mosaic': 1.0,    # Mosaic augmentation\n",
    "    'mixup': 0.0,     # Mixup augmentation\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8e652f",
   "metadata": {},
   "source": [
    "## Step 3: Verify Dataset Structure\n",
    "\n",
    "Dataset is already prepared by e2e_data_prep.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee0e39b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATASET VERIFICATION\n",
      "============================================================\n",
      "TRAIN:  998 images,  998 labels\n",
      "VAL  :   47 images,   47 labels\n",
      "TEST :  209 images,  209 labels\n",
      "TOTAL: 1254 images, 1254 labels\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Verify dataset structure\n",
    "print(\"=\"*60)\n",
    "print(\"DATASET VERIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "splits = ['train', 'val', 'test']\n",
    "stats = {}\n",
    "\n",
    "for split in splits:\n",
    "    img_dir = YOLO_DATASET / split / \"images\"\n",
    "    lbl_dir = YOLO_DATASET / split / \"labels\"\n",
    "    \n",
    "    if img_dir.exists() and lbl_dir.exists():\n",
    "        num_images = len(list(img_dir.glob(\"*\")))\n",
    "        num_labels = len(list(lbl_dir.glob(\"*.txt\")))\n",
    "        stats[split] = {'images': num_images, 'labels': num_labels}\n",
    "        print(f\"{split.upper():5s}: {num_images:4d} images, {num_labels:4d} labels\")\n",
    "    else:\n",
    "        stats[split] = {'images': 0, 'labels': 0}\n",
    "        print(f\"{split.upper():5s}: Missing!\")\n",
    "\n",
    "total_images = sum(s['images'] for s in stats.values())\n",
    "total_labels = sum(s['labels'] for s in stats.values())\n",
    "\n",
    "print(f\"{'TOTAL':5s}: {total_images:4d} images, {total_labels:4d} labels\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if total_images == 0:\n",
    "    raise RuntimeError(\"No dataset found! Run e2e_data_prep.ipynb to create the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b979e79",
   "metadata": {},
   "source": [
    "## Step 4: Create YOLO Configuration File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "739a4bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Configuration saved: datasets/ready/full_dataset/data.yaml\n",
      "Dataset structure:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "[datasets/ready/full_dataset](datasets/ready/full_dataset)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "[datasets/ready/full_dataset/train](datasets/ready/full_dataset/train)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Val:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "[datasets/ready/full_dataset/val](datasets/ready/full_dataset/val)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Test:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "[datasets/ready/full_dataset/test](datasets/ready/full_dataset/test)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classes = {\n",
    "    'red ball': 0,\n",
    "    'human': 1,\n",
    "    'trashcan': 2\n",
    "}\n",
    "\n",
    "config = {\n",
    "    'path': str(YOLO_DATASET.absolute()),\n",
    "    'train': 'train/images',\n",
    "    'val': 'val/images',\n",
    "    'nc': len(classes),\n",
    "    'names': list(classes.keys())\n",
    "}\n",
    "\n",
    "config_path = YOLO_DATASET / 'data.yaml'\n",
    "with open(config_path, 'w') as f:\n",
    "    yaml.dump(config, f, default_flow_style=False)\n",
    "\n",
    "print(f\"‚úì Configuration saved: {config_path}\")\n",
    "print(\"Dataset structure:\")\n",
    "YOLO_DATASET.display()\n",
    "print(\"  Train:\")\n",
    "(YOLO_DATASET / 'train').display()\n",
    "print(\"  Val:\")\n",
    "(YOLO_DATASET / 'val').display()\n",
    "print(\"  Test:\")\n",
    "(YOLO_DATASET / 'test').display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db1af91",
   "metadata": {},
   "source": [
    "## Step 4.5: Find Trashcan Images for Monitoring\n",
    "\n",
    "Identify validation images containing trashcans (class 2) to monitor training progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa8f255d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning 47 label files for trashcans...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úì Found trashcan in: de96513c1cdccc864e7b7e809162d06c.jpg\n",
      "  ‚úì Found trashcan in: 1383c4a58a26c7238fbae31ec8e4e660.jpg\n",
      "  ‚úì Found trashcan in: f7c36eabf5a95cf548a382f4a6b49050.jpg\n",
      "  ‚úì Found trashcan in: 380fca853fd511698abe9f0c24285f68.jpg\n",
      "  ‚úì Found trashcan in: 8dad2db8365784c4b162442bd59eb5c7.jpg\n",
      "  ‚úì Found trashcan in: d8c811abffbf5da563ee451592a24dcd.jpg\n",
      "  ‚úì Found trashcan in: 98a670c874d1cefcacd71b855d29769a.jpg\n",
      "  ‚úì Found trashcan in: b6fe1fc46e4ad868193c424070da1e34.jpg\n",
      "\n",
      "============================================================\n",
      "Selected 8 images for trashcan monitoring:\n",
      "  - de96513c1cdccc864e7b7e809162d06c.jpg\n",
      "  - 1383c4a58a26c7238fbae31ec8e4e660.jpg\n",
      "  - f7c36eabf5a95cf548a382f4a6b49050.jpg\n",
      "  - 380fca853fd511698abe9f0c24285f68.jpg\n",
      "  - 8dad2db8365784c4b162442bd59eb5c7.jpg\n",
      "  - d8c811abffbf5da563ee451592a24dcd.jpg\n",
      "  - 98a670c874d1cefcacd71b855d29769a.jpg\n",
      "  - b6fe1fc46e4ad868193c424070da1e34.jpg\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def find_trashcan_images(val_labels_dir, val_images_dir, max_images=8):\n",
    "    \"\"\"Find validation images that contain trashcan annotations (class 2)\"\"\"\n",
    "    trashcan_images = []\n",
    "    \n",
    "    # Scan all label files\n",
    "    label_files = list(val_labels_dir.glob(\"*.txt\"))\n",
    "    print(f\"Scanning {len(label_files)} label files for trashcans...\")\n",
    "    \n",
    "    for label_file in label_files:\n",
    "        try:\n",
    "            with open(label_file, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "                \n",
    "            # Check if any line starts with \"2 \" (trashcan class)\n",
    "            has_trashcan = any(line.strip().startswith('2 ') for line in lines)\n",
    "            \n",
    "            if has_trashcan:\n",
    "                # Find corresponding image\n",
    "                img_name = label_file.stem\n",
    "                \n",
    "                # Try different image extensions\n",
    "                for ext in ['.jpg', '.jpeg', '.png', '.JPG', '.JPEG', '.PNG']:\n",
    "                    img_path = val_images_dir / f\"{img_name}{ext}\"\n",
    "                    if img_path.exists():\n",
    "                        trashcan_images.append(str(img_path))\n",
    "                        print(f\"  ‚úì Found trashcan in: {img_path.name}\")\n",
    "                        break\n",
    "                \n",
    "                if len(trashcan_images) >= max_images:\n",
    "                    break\n",
    "        except Exception as e:\n",
    "            # Skip problematic files (e.g., too large)\n",
    "            continue\n",
    "    \n",
    "    return trashcan_images\n",
    "\n",
    "# Find trashcan images\n",
    "val_labels_dir = YOLO_DATASET / 'val' / 'labels'\n",
    "val_images_dir = YOLO_DATASET / 'val' / 'images'\n",
    "\n",
    "TRASHCAN_MONITOR_IMAGES = find_trashcan_images(val_labels_dir, val_images_dir, max_images=8)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Selected {len(TRASHCAN_MONITOR_IMAGES)} images for trashcan monitoring:\")\n",
    "for img_path in TRASHCAN_MONITOR_IMAGES:\n",
    "    print(f\"  - {Path(img_path).name}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "if len(TRASHCAN_MONITOR_IMAGES) == 0:\n",
    "    print(\"‚ö†Ô∏è  Warning: No trashcan images found in validation set!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0cd691",
   "metadata": {},
   "source": [
    "## Step 4.6: Define Trashcan Monitoring Callback\n",
    "\n",
    "Create a callback that visualizes trashcan segmentation progress at each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9e4bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trashcan_monitor_callback(model, monitor_images, output_dir, project_name):\n",
    "    \"\"\"Create a callback to monitor trashcan segmentation progress during training\"\"\"\n",
    "    \n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    def on_train_epoch_end(trainer):\n",
    "        \"\"\"Called at the end of each training epoch\"\"\"\n",
    "        if len(monitor_images) == 0:\n",
    "            return\n",
    "        \n",
    "        epoch = trainer.epoch\n",
    "        \n",
    "        # Skip first few epochs to save time\n",
    "        if epoch < 5:\n",
    "            return\n",
    "        \n",
    "        # Run predictions on monitor images\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Generating trashcan monitoring visualizations for epoch {epoch}...\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Save current training state\n",
    "        was_training = trainer.model.training\n",
    "        \n",
    "        try:\n",
    "            # Put model in eval mode and disable gradients\n",
    "            trainer.model.eval()\n",
    "            \n",
    "            # Create figure with subplots\n",
    "            n_images = len(monitor_images)\n",
    "            n_cols = min(3, n_images)\n",
    "            n_rows = (n_images + n_cols - 1) // n_cols\n",
    "            \n",
    "            fig = plt.figure(figsize=(6 * n_cols, 5 * n_rows))\n",
    "            gs = GridSpec(n_rows, n_cols, figure=fig, hspace=0.3, wspace=0.3)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for idx, img_path in enumerate(monitor_images):\n",
    "                    try:\n",
    "                        # Use the YOLO wrapper's predict method\n",
    "                        # Create temporary YOLO instance with current weights\n",
    "                        temp_model = YOLO(trainer.best if hasattr(trainer, 'best') else trainer.last)\n",
    "                        results = temp_model.predict(img_path, conf=0.25, verbose=False)\n",
    "                        \n",
    "                        # Get the plotted image\n",
    "                        result = results[0]\n",
    "                        plotted_img = result.plot()\n",
    "                        \n",
    "                        # Convert BGR to RGB\n",
    "                        plotted_img = cv2.cvtColor(plotted_img, cv2.COLOR_BGR2RGB)\n",
    "                        \n",
    "                        # Add to subplot\n",
    "                        row = idx // n_cols\n",
    "                        col = idx % n_cols\n",
    "                        ax = fig.add_subplot(gs[row, col])\n",
    "                        ax.imshow(plotted_img)\n",
    "                        ax.axis('off')\n",
    "                        \n",
    "                        # Count detections by class\n",
    "                        if result.masks is not None and len(result.boxes) > 0:\n",
    "                            classes_detected = result.boxes.cls.cpu().numpy()\n",
    "                            n_balls = int((classes_detected == 0).sum())\n",
    "                            n_humans = int((classes_detected == 1).sum())\n",
    "                            n_trashcans = int((classes_detected == 2).sum())\n",
    "                            \n",
    "                            title = f\"{Path(img_path).name}\\n\"\n",
    "                            title += f\"Balls: {n_balls} | Humans: {n_humans} | Trashcans: {n_trashcans}\"\n",
    "                        else:\n",
    "                            title = f\"{Path(img_path).name}\\nNo detections\"\n",
    "                        \n",
    "                        ax.set_title(title, fontsize=10)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"  ‚ö†Ô∏è  Error processing {Path(img_path).name}: {e}\")\n",
    "            \n",
    "            # Add main title\n",
    "            fig.suptitle(f\"Trashcan Monitoring - Epoch {epoch} - {project_name}\", \n",
    "                         fontsize=16, fontweight='bold', y=0.995)\n",
    "            \n",
    "            # Save figure\n",
    "            output_file = output_dir / f\"epoch_{epoch:03d}.jpg\"\n",
    "            plt.savefig(output_file, dpi=150, bbox_inches='tight')\n",
    "            plt.close(fig)\n",
    "            \n",
    "            print(f\"‚úì Saved monitoring visualization:\")\n",
    "            output_dir.display()\n",
    "            print(f\"{'='*60}\\n\")\n",
    "            \n",
    "        finally:\n",
    "            # Restore training mode\n",
    "            if was_training:\n",
    "                trainer.model.train()\n",
    "    \n",
    "    return on_train_epoch_end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853894a7",
   "metadata": {},
   "source": [
    "## Step 5: Train Model with Trashcan Monitoring\n",
    "\n",
    "Train YOLOv11 with:\n",
    "- Data augmentation on train set\n",
    "- Checkpoints saved for best model\n",
    "- Validation after each epoch\n",
    "- **Custom callback to monitor trashcan segmentation progress**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f0c5a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained model\n",
    "model = YOLO(model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a4f6fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = 'ball_person_trashcan_model_v3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3f23d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trashcan monitoring output: runs/segment/ball_person_trashcan_model_v3/trashcan_monitor\n",
      "‚úì Trashcan monitoring callback registered\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Setup trashcan monitoring\n",
    "monitor_output_dir = RUNS_DIR / project_name / 'trashcan_monitor'\n",
    "print(f\"Trashcan monitoring output: {monitor_output_dir}\")\n",
    "\n",
    "# Add callback\n",
    "callback_fn = create_trashcan_monitor_callback(\n",
    "    model=model,\n",
    "    monitor_images=TRASHCAN_MONITOR_IMAGES,\n",
    "    output_dir=monitor_output_dir,\n",
    "    project_name=project_name\n",
    ")\n",
    "\n",
    "model.add_callback('on_train_epoch_end', callback_fn)\n",
    "print(\"‚úì Trashcan monitoring callback registered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a8ad51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.246 available üòÉ Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.235 üöÄ Python-3.12.10 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 3080 Laptop GPU, 8192MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=datasets/ready/full_dataset/data.yaml, degrees=10.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22], half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.001, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11n-seg.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=ball_person_trashcan_model_v3, nbs=64, nms=False, opset=None, optimize=False, optimizer=Adam, overlap_mask=True, patience=20, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs/segment, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/home/tonino/projects/ball segmentation/runs/segment/ball_person_trashcan_model_v3, save_frames=False, save_json=False, save_period=5, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=segment, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    684025  ultralytics.nn.modules.head.Segment          [3, 32, 64, [64, 128, 256]]   \n",
      "YOLO11n-seg summary: 203 layers, 2,843,193 parameters, 2,843,177 gradients, 9.7 GFLOPs\n",
      "\n",
      "Transferred 510/561 items from pretrained weights\n",
      "Freezing layer 'model.0.conv.weight'\n",
      "Freezing layer 'model.0.bn.weight'\n",
      "Freezing layer 'model.0.bn.bias'\n",
      "Freezing layer 'model.1.conv.weight'\n",
      "Freezing layer 'model.1.bn.weight'\n",
      "Freezing layer 'model.1.bn.bias'\n",
      "Freezing layer 'model.2.cv1.conv.weight'\n",
      "Freezing layer 'model.2.cv1.bn.weight'\n",
      "Freezing layer 'model.2.cv1.bn.bias'\n",
      "Freezing layer 'model.2.cv2.conv.weight'\n",
      "Freezing layer 'model.2.cv2.bn.weight'\n",
      "Freezing layer 'model.2.cv2.bn.bias'\n",
      "Freezing layer 'model.2.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.2.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.2.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.2.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.2.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.2.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.3.conv.weight'\n",
      "Freezing layer 'model.3.bn.weight'\n",
      "Freezing layer 'model.3.bn.bias'\n",
      "Freezing layer 'model.4.cv1.conv.weight'\n",
      "Freezing layer 'model.4.cv1.bn.weight'\n",
      "Freezing layer 'model.4.cv1.bn.bias'\n",
      "Freezing layer 'model.4.cv2.conv.weight'\n",
      "Freezing layer 'model.4.cv2.bn.weight'\n",
      "Freezing layer 'model.4.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.5.conv.weight'\n",
      "Freezing layer 'model.5.bn.weight'\n",
      "Freezing layer 'model.5.bn.bias'\n",
      "Freezing layer 'model.6.cv1.conv.weight'\n",
      "Freezing layer 'model.6.cv1.bn.weight'\n",
      "Freezing layer 'model.6.cv1.bn.bias'\n",
      "Freezing layer 'model.6.cv2.conv.weight'\n",
      "Freezing layer 'model.6.cv2.bn.weight'\n",
      "Freezing layer 'model.6.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.0.cv3.conv.weight'\n",
      "Freezing layer 'model.6.m.0.cv3.bn.weight'\n",
      "Freezing layer 'model.6.m.0.cv3.bn.bias'\n",
      "Freezing layer 'model.6.m.0.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.0.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.0.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.0.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.0.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.0.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.0.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.0.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.0.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.0.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.0.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.0.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.7.conv.weight'\n",
      "Freezing layer 'model.7.bn.weight'\n",
      "Freezing layer 'model.7.bn.bias'\n",
      "Freezing layer 'model.8.cv1.conv.weight'\n",
      "Freezing layer 'model.8.cv1.bn.weight'\n",
      "Freezing layer 'model.8.cv1.bn.bias'\n",
      "Freezing layer 'model.8.cv2.conv.weight'\n",
      "Freezing layer 'model.8.cv2.bn.weight'\n",
      "Freezing layer 'model.8.cv2.bn.bias'\n",
      "Freezing layer 'model.8.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.8.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.8.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.8.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.8.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.8.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.8.m.0.cv3.conv.weight'\n",
      "Freezing layer 'model.8.m.0.cv3.bn.weight'\n",
      "Freezing layer 'model.8.m.0.cv3.bn.bias'\n",
      "Freezing layer 'model.8.m.0.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.8.m.0.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.8.m.0.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.8.m.0.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.8.m.0.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.8.m.0.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.8.m.0.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.8.m.0.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.8.m.0.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.8.m.0.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.8.m.0.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.8.m.0.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.9.cv1.conv.weight'\n",
      "Freezing layer 'model.9.cv1.bn.weight'\n",
      "Freezing layer 'model.9.cv1.bn.bias'\n",
      "Freezing layer 'model.9.cv2.conv.weight'\n",
      "Freezing layer 'model.9.cv2.bn.weight'\n",
      "Freezing layer 'model.9.cv2.bn.bias'\n",
      "Freezing layer 'model.10.cv1.conv.weight'\n",
      "Freezing layer 'model.10.cv1.bn.weight'\n",
      "Freezing layer 'model.10.cv1.bn.bias'\n",
      "Freezing layer 'model.10.cv2.conv.weight'\n",
      "Freezing layer 'model.10.cv2.bn.weight'\n",
      "Freezing layer 'model.10.cv2.bn.bias'\n",
      "Freezing layer 'model.10.m.0.attn.qkv.conv.weight'\n",
      "Freezing layer 'model.10.m.0.attn.qkv.bn.weight'\n",
      "Freezing layer 'model.10.m.0.attn.qkv.bn.bias'\n",
      "Freezing layer 'model.10.m.0.attn.proj.conv.weight'\n",
      "Freezing layer 'model.10.m.0.attn.proj.bn.weight'\n",
      "Freezing layer 'model.10.m.0.attn.proj.bn.bias'\n",
      "Freezing layer 'model.10.m.0.attn.pe.conv.weight'\n",
      "Freezing layer 'model.10.m.0.attn.pe.bn.weight'\n",
      "Freezing layer 'model.10.m.0.attn.pe.bn.bias'\n",
      "Freezing layer 'model.10.m.0.ffn.0.conv.weight'\n",
      "Freezing layer 'model.10.m.0.ffn.0.bn.weight'\n",
      "Freezing layer 'model.10.m.0.ffn.0.bn.bias'\n",
      "Freezing layer 'model.10.m.0.ffn.1.conv.weight'\n",
      "Freezing layer 'model.10.m.0.ffn.1.bn.weight'\n",
      "Freezing layer 'model.10.m.0.ffn.1.bn.bias'\n",
      "Freezing layer 'model.13.cv1.conv.weight'\n",
      "Freezing layer 'model.13.cv1.bn.weight'\n",
      "Freezing layer 'model.13.cv1.bn.bias'\n",
      "Freezing layer 'model.13.cv2.conv.weight'\n",
      "Freezing layer 'model.13.cv2.bn.weight'\n",
      "Freezing layer 'model.13.cv2.bn.bias'\n",
      "Freezing layer 'model.13.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.13.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.13.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.13.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.13.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.13.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.16.cv1.conv.weight'\n",
      "Freezing layer 'model.16.cv1.bn.weight'\n",
      "Freezing layer 'model.16.cv1.bn.bias'\n",
      "Freezing layer 'model.16.cv2.conv.weight'\n",
      "Freezing layer 'model.16.cv2.bn.weight'\n",
      "Freezing layer 'model.16.cv2.bn.bias'\n",
      "Freezing layer 'model.16.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.16.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.16.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.16.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.16.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.16.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.17.conv.weight'\n",
      "Freezing layer 'model.17.bn.weight'\n",
      "Freezing layer 'model.17.bn.bias'\n",
      "Freezing layer 'model.19.cv1.conv.weight'\n",
      "Freezing layer 'model.19.cv1.bn.weight'\n",
      "Freezing layer 'model.19.cv1.bn.bias'\n",
      "Freezing layer 'model.19.cv2.conv.weight'\n",
      "Freezing layer 'model.19.cv2.bn.weight'\n",
      "Freezing layer 'model.19.cv2.bn.bias'\n",
      "Freezing layer 'model.19.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.19.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.19.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.19.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.19.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.19.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.20.conv.weight'\n",
      "Freezing layer 'model.20.bn.weight'\n",
      "Freezing layer 'model.20.bn.bias'\n",
      "Freezing layer 'model.22.cv1.conv.weight'\n",
      "Freezing layer 'model.22.cv1.bn.weight'\n",
      "Freezing layer 'model.22.cv1.bn.bias'\n",
      "Freezing layer 'model.22.cv2.conv.weight'\n",
      "Freezing layer 'model.22.cv2.bn.weight'\n",
      "Freezing layer 'model.22.cv2.bn.bias'\n",
      "Freezing layer 'model.22.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.22.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.22.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.22.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.22.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.22.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.22.m.0.cv3.conv.weight'\n",
      "Freezing layer 'model.22.m.0.cv3.bn.weight'\n",
      "Freezing layer 'model.22.m.0.cv3.bn.bias'\n",
      "Freezing layer 'model.22.m.0.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.22.m.0.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.22.m.0.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.22.m.0.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.22.m.0.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.22.m.0.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.22.m.0.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.22.m.0.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.22.m.0.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.22.m.0.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.22.m.0.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.22.m.0.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 825.2¬±608.3 MB/s, size: 77.7 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/labels.cache... 997 images, 3 backgrounds, 1 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 998/998 1.3Mit/s 0.0s0s\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/0db65afcb80a13863a3a8dfc24e7d73a.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/1731534619a61f5dc5f63c0414915afb.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/1c04a053b46c5e78a4a93e76cc80d233.jpg: 5 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/1e82f7995240b5e6e1fc33e649219375.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/20b90386dd79ffcfbd94b9868ac62cc5.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/2db66d2c671556fda7813b34ef672ee3.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/31aab9bb7708794e4777bf399873f529.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/432990e6d8d154c8c60fb4e0570b452a.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/491e62d4bb9e70806db0a13b6abe99ed.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/571d71a897f478abf9fbd6a4bb667ded.jpg: ignoring corrupt image/label: image size (9, 16) <10 pixels\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/5e584bc79700ada6170545cdd5f0628c.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/77449fbf91a6c8f66fc6d24a6d55cb91.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/7bc696523e818d843abe7c38ff7d9588.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/7d08dd1bd72cb79b59b73d90d1487a71.jpg: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/7ee3ed09be312fac168b140fdca3bc75.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/8386b8b7f9b1b19d822eebcb7431edef.jpg: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/87fbc3315fe5abc3142de03b5bb96af0.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/8a0dab3a8810f7aac82708d97d17dcea.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/8b8e121636bf2158a7052e0a297e36a1.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/b89006dfbf0861ad3cf9f5161bb1b16f.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/dfe44d08ed9d881f01ed3d9acd374772.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/e74a8de4f129a1d33c6dc3320d04d8d6.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/eac3d40199892716e311067c141ec999.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 993.6¬±1153.8 MB/s, size: 2921.1 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/tonino/projects/ball segmentation/datasets/ready/full_dataset/val/labels.cache... 47 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 47/47 48.1Kit/s 0.0s\n",
      "Plotting labels to /home/tonino/projects/ball segmentation/runs/segment/ball_person_trashcan_model_v3/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.001, momentum=0.937) with parameter groups 90 weight(decay=0.0), 101 weight(decay=0.0005), 100 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1m/home/tonino/projects/ball segmentation/runs/segment/ball_person_trashcan_model_v3\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       1/50      1.82G     0.8786      2.674      2.241      1.165         27        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 2.1s/it 2:130.8ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2/2 6.9s/it 13.7s23.1s\n",
      "                   all         47         81     0.0033      0.618      0.277      0.257     0.0021      0.382      0.111     0.0512\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       2/50         2G     0.8932      2.529      1.497      1.181         47        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 2.2it/s 28.8s0.2ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2/2 3.1it/s 0.6s1.4s\n",
      "                   all         47         81      0.411      0.478      0.446      0.401      0.411      0.478      0.445      0.216\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       3/50         2G     0.9002      2.536      1.415      1.177         16        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 4.7it/s 13.3s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2/2 3.2it/s 0.6s1.3s\n",
      "                   all         47         81      0.532      0.492      0.495      0.443      0.532      0.492      0.489      0.374\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       4/50      2.01G     0.8897      2.481      1.327      1.176         21        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 5.1it/s 12.3s0.4s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2/2 3.0it/s 0.7s1.3s\n",
      "                   all         47         81       0.49       0.48      0.485       0.44       0.49       0.48      0.478      0.379\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       5/50      2.01G     0.8793      2.398      1.273      1.153         75        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 5.1it/s 12.4s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2/2 3.2it/s 0.6s1.3s\n",
      "                   all         47         81       0.51      0.496      0.498      0.447       0.51      0.496      0.497      0.399\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       6/50      2.01G     0.8958      2.481      1.286       1.17         24        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 4.5it/s 14.0s0.2s\n",
      "\n",
      "============================================================\n",
      "Generating trashcan monitoring visualizations for epoch 5...\n",
      "============================================================\n",
      "‚úì Saved monitoring visualization: runs/segment/ball_person_trashcan_model_v3/trashcan_monitor/epoch_005.jpg\n",
      "============================================================\n",
      "\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2/2 2.5it/s 0.8s2.1s\n",
      "                   all         47         81      0.501      0.451      0.477      0.427      0.501      0.451      0.475      0.366\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       7/50      2.82G     0.8741      2.426      1.229      1.168         45        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 4.8it/s 13.1s0.2s\n",
      "\n",
      "============================================================\n",
      "Generating trashcan monitoring visualizations for epoch 6...\n",
      "============================================================\n",
      "‚úì Saved monitoring visualization: runs/segment/ball_person_trashcan_model_v3/trashcan_monitor/epoch_006.jpg\n",
      "============================================================\n",
      "\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2/2 2.0it/s 1.0s2.5s\n",
      "                   all         47         81      0.529      0.412      0.467      0.433      0.529      0.412      0.466      0.381\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       8/50      2.83G     0.8492      2.365      1.242      1.149         40        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 4.5it/s 14.0s0.2s\n",
      "\n",
      "============================================================\n",
      "Generating trashcan monitoring visualizations for epoch 7...\n",
      "============================================================\n",
      "‚úì Saved monitoring visualization: runs/segment/ball_person_trashcan_model_v3/trashcan_monitor/epoch_007.jpg\n",
      "============================================================\n",
      "\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2/2 2.0it/s 1.0s2.7s\n",
      "                   all         47         81      0.439       0.49      0.494      0.454      0.439       0.49      0.492      0.401\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       9/50      2.84G      0.849      2.371      1.232      1.145         40        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 4.5it/s 13.9s0.2s\n",
      "\n",
      "============================================================\n",
      "Generating trashcan monitoring visualizations for epoch 8...\n",
      "============================================================\n",
      "‚úì Saved monitoring visualization: runs/segment/ball_person_trashcan_model_v3/trashcan_monitor/epoch_008.jpg\n",
      "============================================================\n",
      "\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2/2 2.1it/s 1.0s1.8s\n",
      "                   all         47         81      0.432      0.444      0.475      0.441      0.432      0.444      0.473      0.389\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      10/50      2.84G     0.8296      2.339      1.197       1.14         33        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 4.6it/s 13.8s0.2s\n",
      "\n",
      "============================================================\n",
      "Generating trashcan monitoring visualizations for epoch 9...\n",
      "============================================================\n",
      "‚úì Saved monitoring visualization: runs/segment/ball_person_trashcan_model_v3/trashcan_monitor/epoch_009.jpg\n",
      "============================================================\n",
      "\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2/2 2.3it/s 0.9s2.2s\n",
      "                   all         47         81      0.587       0.41      0.481      0.446      0.587       0.41      0.479       0.39\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      11/50      2.85G     0.8596      2.394      1.194      1.159         47        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 5.1it/s 12.4s0.2s\n",
      "\n",
      "============================================================\n",
      "Generating trashcan monitoring visualizations for epoch 10...\n",
      "============================================================\n",
      "‚úì Saved monitoring visualization: runs/segment/ball_person_trashcan_model_v3/trashcan_monitor/epoch_010.jpg\n",
      "============================================================\n",
      "\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2/2 2.7it/s 0.7s1.8s\n",
      "                   all         47         81       0.49      0.471      0.507      0.466       0.49      0.471      0.501      0.393\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      12/50      2.85G      0.856      2.363      1.184      1.151         41        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 4.9it/s 13.0s0.2s\n",
      "\n",
      "============================================================\n",
      "Generating trashcan monitoring visualizations for epoch 11...\n",
      "============================================================\n",
      "‚úì Saved monitoring visualization: runs/segment/ball_person_trashcan_model_v3/trashcan_monitor/epoch_011.jpg\n",
      "============================================================\n",
      "\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2/2 2.4it/s 0.8s2.1s\n",
      "                   all         47         81      0.536      0.471      0.522      0.486      0.536      0.471       0.52      0.416\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      13/50      2.85G     0.8296      2.395      1.156       1.13         45        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 4.6it/s 13.7s0.2s\n",
      "\n",
      "============================================================\n",
      "Generating trashcan monitoring visualizations for epoch 12...\n",
      "============================================================\n",
      "‚úì Saved monitoring visualization: runs/segment/ball_person_trashcan_model_v3/trashcan_monitor/epoch_012.jpg\n",
      "============================================================\n",
      "\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2/2 1.8it/s 1.1s2.9s\n",
      "                   all         47         81      0.522      0.422      0.473       0.43      0.522      0.422      0.472       0.39\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      14/50       3.4G     0.8123       2.27      1.128       1.12         52        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 4.6it/s 13.6s0.2s\n",
      "\n",
      "============================================================\n",
      "Generating trashcan monitoring visualizations for epoch 13...\n",
      "============================================================\n",
      "‚úì Saved monitoring visualization: runs/segment/ball_person_trashcan_model_v3/trashcan_monitor/epoch_013.jpg\n",
      "============================================================\n",
      "\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2/2 2.3it/s 0.9s2.1s\n",
      "                   all         47         81      0.479      0.434      0.485      0.454      0.479      0.434      0.484      0.406\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      15/50       3.4G     0.7974      2.222      1.107      1.102         34        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 4.5it/s 14.0s0.2s\n",
      "\n",
      "============================================================\n",
      "Generating trashcan monitoring visualizations for epoch 14...\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "head_idx = next((i for i, m in enumerate(model.model.model) if 'Detect' in m.__class__.__name__ or 'Segment' in m.__class__.__name__), len(model.model.model) - 1)\n",
    "\n",
    "results = model.train(\n",
    "    data=str(config_path),\n",
    "    epochs=EPOCHS,\n",
    "    freeze=list(range(head_idx)),\n",
    "    batch=BATCH_SIZE,\n",
    "    imgsz=IMG_SIZE,\n",
    "    device=DEVICE,\n",
    "    project=str(RUNS_DIR),\n",
    "    name=project_name,\n",
    "    exist_ok=True,\n",
    "    \n",
    "    # Checkpointing\n",
    "    save=True,\n",
    "    save_period=5,  # Save every 5 epochs\n",
    "    \n",
    "    # Validation\n",
    "    val=True,\n",
    "    \n",
    "    # Data augmentation (only applied to train)\n",
    "    **AUG_CONFIG,\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer='Adam',\n",
    "    lr0=0.001,\n",
    "    lrf=0.01,\n",
    "    momentum=0.937,\n",
    "    weight_decay=0.0005,\n",
    "    \n",
    "    # Loss weights\n",
    "    box=7.5,\n",
    "    cls=0.5,\n",
    "    dfl=1.5,\n",
    "    \n",
    "    # Other\n",
    "    patience=20,  # Early stopping\n",
    "    workers=8,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573f1e2e",
   "metadata": {},
   "source": [
    "## Step 6: Load Best Model & Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcded3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = RUNS_DIR / project_name / 'weights' / 'best.pt'\n",
    "best_model_path.display()\n",
    "model = YOLO(best_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4317200b",
   "metadata": {},
   "source": [
    "## Step 7: Evaluate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02a0a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation metrics\n",
    "metrics = model.val()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VALIDATION METRICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Box mAP50: {metrics.box.map50:.4f}\")\n",
    "print(f\"Box mAP50-95: {metrics.box.map:.4f}\")\n",
    "print(f\"Mask mAP50: {metrics.seg.map50:.4f}\")\n",
    "print(f\"Mask mAP50-95: {metrics.seg.map:.4f}\")\n",
    "\n",
    "# Per-class metrics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PER-CLASS METRICS (Segmentation)\")\n",
    "print(\"=\"*60)\n",
    "class_names = ['red ball', 'human', 'trashcan']\n",
    "for i, class_name in enumerate(class_names):\n",
    "    try:\n",
    "        map50 = metrics.seg.map50_per_class[i] if hasattr(metrics.seg, 'map50_per_class') else 0\n",
    "        map_val = metrics.seg.map_per_class[i] if hasattr(metrics.seg, 'map_per_class') else 0\n",
    "        print(f\"{class_name:12s}: mAP50={map50:.4f}, mAP50-95={map_val:.4f}\")\n",
    "    except:\n",
    "        print(f\"{class_name:12s}: metrics not available\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d36b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best checkpoint\n",
    "model_dir = RUNS_DIR / project_name\n",
    "best_model = model_dir / 'weights' / 'best.pt'\n",
    "last_model = model_dir / 'weights' / 'last.pt'\n",
    "\n",
    "print(f\"Best model: \")\n",
    "best_model.display()\n",
    "print(f\"Last model: \")\n",
    "last_model.display()\n",
    "print(f\"Results: \")\n",
    "model_dir.display()\n",
    "print(f\"\\nTrashcan monitoring visualizations: \")\n",
    "(model_dir / 'trashcan_monitor').display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f94a189",
   "metadata": {},
   "source": [
    "## Step 8: Visualize Trashcan Progress Evolution\n",
    "\n",
    "Create a comparison showing how trashcan segmentation improved over epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac1cf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all monitoring visualizations\n",
    "monitor_dir = RUNS_DIR / project_name / 'trashcan_monitor'\n",
    "\n",
    "if monitor_dir.exists():\n",
    "    viz_files = sorted(monitor_dir.glob(\"epoch_*.jpg\"))\n",
    "    print(f\"Found {len(viz_files)} monitoring visualizations:\")\n",
    "    for viz_file in viz_files:\n",
    "        print(f\"  - {viz_file.name}\")\n",
    "    \n",
    "    if len(viz_files) > 0:\n",
    "        print(f\"\\nüí° Tip: Open the images in {monitor_dir} to see how trashcan segmentation evolved!\")\n",
    "        print(f\"   You can use an image viewer or VS Code to flip through them chronologically.\")\n",
    "else:\n",
    "    print(\"No monitoring visualizations found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bc9c5e",
   "metadata": {},
   "source": [
    "## Step 9: Test on Sample Images (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f791323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on validation images (sample from val set)\n",
    "test_images = list((YOLO_DATASET / \"val\" / \"images\").glob(\"*\"))[:10]\n",
    "\n",
    "print(f\"Testing on {len(test_images)} sample images...\")\n",
    "\n",
    "for img_path in test_images:\n",
    "    results = model.predict(str(img_path), save=True, conf=0.25)\n",
    "    print(f\"  ‚úì {img_path.name}\")\n",
    "\n",
    "print(f\"\\nResults saved to: {RUNS_DIR / project_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
