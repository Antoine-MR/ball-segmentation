{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58fcdb03",
   "metadata": {},
   "source": [
    "## Step 1: Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2e257a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "from ultralytics.models import YOLO\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.detection import RoboflowDetector\n",
    "from src.segmentation import SAMSegmenter  # SAM 2.1 instead of FastSAM\n",
    "from src.pipeline import img_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d7ba4d",
   "metadata": {},
   "source": [
    "## Step 2: Configure Paths & Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cdccba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input path\n",
    "IRL_RAW = Path(\"datasets/raw/IRL_validation_pictures\")\n",
    "\n",
    "# Output paths\n",
    "IRL_READY = Path(\"datasets/ready/IRL_dataset_sam2\")\n",
    "IRL_IMAGES = IRL_READY / \"images\"\n",
    "IRL_LABELS = IRL_READY / \"labels\"\n",
    "\n",
    "# Intermediate outputs for ball detection+segmentation\n",
    "BALL_DET_OUTPUT = Path(\"datasets/preprocessed/irl_balls_sam2/detection\")\n",
    "BALL_SEG_OUTPUT = Path(\"datasets/preprocessed/irl_balls_sam2/segmentation\")\n",
    "BALL_TXT_OUTPUT = Path(\"datasets/preprocessed/irl_balls_sam2/labels\")\n",
    "\n",
    "# Create directories\n",
    "for dir_path in [IRL_IMAGES, IRL_LABELS, BALL_DET_OUTPUT, BALL_SEG_OUTPUT, BALL_TXT_OUTPUT]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Device\n",
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Input: {IRL_RAW}\")\n",
    "print(f\"Output: {IRL_READY}\")\n",
    "\n",
    "# Get image list\n",
    "img_paths = list(IRL_RAW.glob(\"*.jpg\")) + list(IRL_RAW.glob(\"*.jpeg\")) + \\\n",
    "            list(IRL_RAW.glob(\"*.JPG\")) + list(IRL_RAW.glob(\"*.JPEG\"))\n",
    "print(f\"Found {len(img_paths)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47be9fb",
   "metadata": {},
   "source": [
    "## Step 3: Load Models\n",
    "\n",
    "- **Ball**: Roboflow detector (red-ball-detection-new/1) + **SAM 2.1** segmenter\n",
    "- **Person**: YOLO-seg pretrained (yolo11n-seg.pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392d463f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ball detection + segmentation pipeline (Roboflow + SAM 2.1)\n",
    "# Model ID is hardcoded in RoboflowDetector, API key loaded from .env\n",
    "ball_detector = RoboflowDetector()\n",
    "ball_segmenter = SAMSegmenter()  # Uses SAM 2.1 (sam2.1_b.pt)\n",
    "\n",
    "# Person model (pretrained YOLO-seg)\n",
    "PERSON_MODEL_PATH = Path('models/pretrained/yolo11n-seg.pt')\n",
    "person_model = YOLO(str(PERSON_MODEL_PATH))\n",
    "\n",
    "print(f\"âœ“ Ball detector (Roboflow): {ball_detector.DEFAULT_MODEL_ID}\")\n",
    "print(f\"âœ“ Ball segmenter: SAM 2.1 (sam2.1_b.pt)\")\n",
    "print(f\"âœ“ Person model: {PERSON_MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b60e7e7",
   "metadata": {},
   "source": [
    "## Step 4: Segment Balls (Roboflow â†’ SAM 2.1 â†’ YOLO txt)\n",
    "\n",
    "Process each image through the detection+segmentation pipeline using SAM 2.1 with bbox prompts.\n",
    "\n",
    "**Note:** SAM 2.1 is more accurate but slower than FastSAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77402b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Processing {len(img_paths)} images for ball segmentation...\")\n",
    "print(f\"Pipeline: Roboflow detection â†’ SAM 2.1 segmentation â†’ YOLO txt\")\n",
    "print()\n",
    "\n",
    "for img_path in tqdm(img_paths, desc=\"Ball segmentation (SAM 2.1)\"):\n",
    "    img_pipeline(\n",
    "        img_path,\n",
    "        detect_fn=ball_detector.detect,\n",
    "        segment_fn=ball_segmenter.segment_bbox,\n",
    "        det_output_dir=BALL_DET_OUTPUT,\n",
    "        seg_output_dir=BALL_SEG_OUTPUT,\n",
    "        txt_output_dir=BALL_TXT_OUTPUT,\n",
    "        mode=\"bbox\"\n",
    "    )\n",
    "\n",
    "print(\"âœ“ Ball segmentation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db82f30a",
   "metadata": {},
   "source": [
    "## Step 5: Combine Ball + Person Masks\n",
    "\n",
    "- Parse ball polygons from txt files\n",
    "- Segment persons with YOLO-seg\n",
    "- Combine into PNG masks (ball=0, person=1)\n",
    "- **Ball has priority** over person in overlapping regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ece55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONF_THRESHOLD = 0.5  # Person confidence threshold\n",
    "\n",
    "# Statistics\n",
    "stats = {\"total\": 0, \"with_ball\": 0, \"with_person\": 0, \"empty\": 0}\n",
    "\n",
    "print(f\"Combining ball + person masks...\")\n",
    "print(f\"Confidence threshold (person): {CONF_THRESHOLD}\")\n",
    "print(f\"Class priority: ball > person\")\n",
    "print()\n",
    "\n",
    "for img_path in tqdm(img_paths, desc=\"Combining masks\"):\n",
    "    stats[\"total\"] += 1\n",
    "    \n",
    "    # Load image to get dimensions\n",
    "    img = Image.open(img_path)\n",
    "    h, w = img.height, img.width\n",
    "    \n",
    "    # Initialize combined mask (all background)\n",
    "    combined_mask = np.zeros((h, w), dtype=np.uint8)\n",
    "    has_detections = False\n",
    "    \n",
    "    # --- 1. Parse ball segmentation from txt (if exists) ---\n",
    "    ball_txt_path = BALL_TXT_OUTPUT / (img_path.stem + '.txt')\n",
    "    if ball_txt_path.exists():\n",
    "        with open(ball_txt_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            \n",
    "        for line in lines:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) < 7:  # Need at least class_id + 3 points (6 coords)\n",
    "                continue\n",
    "            \n",
    "            # Extract normalized coordinates\n",
    "            coords = [float(p) for p in parts[1:]]\n",
    "            \n",
    "            # Convert to pixel coordinates\n",
    "            points = []\n",
    "            for i in range(0, len(coords), 2):\n",
    "                x = int(coords[i] * w)\n",
    "                y = int(coords[i+1] * h)\n",
    "                points.append([x, y])\n",
    "            \n",
    "            # Fill polygon with ball class (0)\n",
    "            points_array = np.array(points, dtype=np.int32)\n",
    "            cv2.fillPoly(combined_mask, [points_array], 0)\n",
    "            has_detections = True\n",
    "        \n",
    "        stats[\"with_ball\"] += 1\n",
    "    \n",
    "    # --- 2. Segment Persons (class 1) ---\n",
    "    person_results = person_model.predict(\n",
    "        str(img_path), \n",
    "        classes=[0],  # Person class in COCO\n",
    "        conf=CONF_THRESHOLD, \n",
    "        device=DEVICE,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    if person_results[0].masks is not None:\n",
    "        for mask in person_results[0].masks.data:\n",
    "            mask_np = (mask.cpu().numpy() > 0.5).astype(np.uint8)\n",
    "            \n",
    "            # Resize if needed\n",
    "            if mask_np.shape != (h, w):\n",
    "                mask_np = cv2.resize(mask_np, (w, h), interpolation=cv2.INTER_NEAREST)\n",
    "            \n",
    "            # Add person ONLY where background (ensures ball priority)\n",
    "            person_area = (combined_mask == 0) & (mask_np == 1)\n",
    "            combined_mask[person_area] = 1\n",
    "            has_detections = True\n",
    "        \n",
    "        stats[\"with_person\"] += 1\n",
    "    \n",
    "    # Track empty images\n",
    "    if not has_detections:\n",
    "        stats[\"empty\"] += 1\n",
    "    \n",
    "    # Save mask (even if empty)\n",
    "    mask_img = Image.fromarray(combined_mask, mode='L')\n",
    "    mask_img.save(IRL_LABELS / (img_path.stem + '.png'))\n",
    "    \n",
    "    # Copy original image\n",
    "    shutil.copy(img_path, IRL_IMAGES / img_path.name)\n",
    "\n",
    "print(\"\\nâœ“ Processing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fa88b8",
   "metadata": {},
   "source": [
    "## Step 6: Display Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16f3ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"ðŸ“Š DATASET STATISTICS (SAM 2.1)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total images processed:    {stats['total']}\")\n",
    "print(f\"Images with ball(s):       {stats['with_ball']} ({stats['with_ball']/stats['total']*100:.1f}%)\")\n",
    "print(f\"Images with person(s):     {stats['with_person']} ({stats['with_person']/stats['total']*100:.1f}%)\")\n",
    "print(f\"Images with no detections: {stats['empty']} ({stats['empty']/stats['total']*100:.1f}%)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Verify dataset consistency\n",
    "num_images = len(list(IRL_IMAGES.glob(\"*\")))\n",
    "num_labels = len(list(IRL_LABELS.glob(\"*.png\")))\n",
    "\n",
    "print(f\"\\nâœ“ Dataset consistency check:\")\n",
    "print(f\"  Images: {num_images}\")\n",
    "print(f\"  Labels: {num_labels}\")\n",
    "print(f\"  Match: {'âœ“ YES' if num_images == num_labels else 'âœ— NO'}\")\n",
    "\n",
    "print(f\"\\nâœ“ Dataset ready at: {IRL_READY}\")\n",
    "print(f\"  - images/  ({num_images} files)\")\n",
    "print(f\"  - labels/  ({num_labels} .png masks)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fa6336",
   "metadata": {},
   "source": [
    "## Bonus: Visualize SAM 2.1 vs FastSAM Comparison\n",
    "\n",
    "Compare segmentation quality between the two approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5419b72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Select a sample image\n",
    "sample_img_path = img_paths[0]\n",
    "print(f\"Sample: {sample_img_path.name}\")\n",
    "\n",
    "# Load original image\n",
    "img = Image.open(sample_img_path)\n",
    "\n",
    "# Load SAM 2.1 mask\n",
    "sam2_mask_path = IRL_LABELS / (sample_img_path.stem + '.png')\n",
    "sam2_mask = np.array(Image.open(sam2_mask_path))\n",
    "\n",
    "# Load FastSAM mask (if exists)\n",
    "fastsam_mask_path = Path(\"datasets/ready/IRL_dataset/labels\") / (sample_img_path.stem + '.png')\n",
    "if fastsam_mask_path.exists():\n",
    "    fastsam_mask = np.array(Image.open(fastsam_mask_path))\n",
    "    \n",
    "    # Visualize comparison\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    axes[0].imshow(img)\n",
    "    axes[0].set_title(\"Original Image\")\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(fastsam_mask, cmap='tab10', vmin=0, vmax=9)\n",
    "    axes[1].set_title(\"FastSAM Mask\")\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    axes[2].imshow(sam2_mask, cmap='tab10', vmin=0, vmax=9)\n",
    "    axes[2].set_title(\"SAM 2.1 Mask\")\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"FastSAM masks not found. Run data_preparation_clean.ipynb first for comparison.\")\n",
    "    \n",
    "    # Show only SAM 2.1 result\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    \n",
    "    axes[0].imshow(img)\n",
    "    axes[0].set_title(\"Original Image\")\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(sam2_mask, cmap='tab10', vmin=0, vmax=9)\n",
    "    axes[1].set_title(\"SAM 2.1 Mask\")\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
