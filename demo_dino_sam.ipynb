{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ae74cf5",
   "metadata": {},
   "source": [
    "## Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66dffad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Import des modules du projet\n",
    "from src.detection import GroundingDINODetector\n",
    "from src.segmentation import SAMSegmenter\n",
    "from ultralytics.models import YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f673516",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8b8631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemins des modèles\n",
    "YOLO_MODEL = \"runs/segment/ball_person_model/weights/best.pt\"  # Modèle entraîné\n",
    "\n",
    "# Dossier d'images à tester\n",
    "TEST_IMAGES_DIR = Path(\"datasets/raw/IRL_validation_pictures\")\n",
    "\n",
    "# Prompt pour GroundingDINO\n",
    "DETECTION_PROMPT = \"red ball . human\"\n",
    "\n",
    "\n",
    "# Output\n",
    "OUTPUT_DIR = Path(\"demo_outputs\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab78a212",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa37bbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Seuils de confiance\n",
    "GDINO_BOX_THRESHOLD = 0.116\n",
    "GDINO_TEXT_THRESHOLD = 0.06\n",
    "YOLO_CONF = 0.25\n",
    "\n",
    "gdino = GroundingDINODetector(\n",
    "    box_threshold=GDINO_BOX_THRESHOLD,\n",
    "    text_threshold=GDINO_TEXT_THRESHOLD\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb754ace",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94ebf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_boxes(image, detections, color=(0, 255, 0)):\n",
    "    \"\"\"\n",
    "    Dessine les bounding boxes sur l'image avec adaptation automatique de la taille\n",
    "    \n",
    "    Args:\n",
    "        image: Image numpy (BGR)\n",
    "        detections: Liste de dict avec 'bbox', 'label', 'confidence'\n",
    "        color: Tuple (B, G, R) pour la couleur des boxes\n",
    "    \"\"\"\n",
    "    img_copy = image.copy()\n",
    "    h, w = img_copy.shape[:2]\n",
    "    \n",
    "    # Calculer l'échelle basée sur la taille de l'image (référence: 1000px)\n",
    "    scale_factor = max(h, w) / 1000.0\n",
    "    \n",
    "    # Paramètres adaptatifs ajustés (plus petits)\n",
    "    thickness = max(1, int(2 * scale_factor))\n",
    "    font_scale = max(0.4, 0.6 * scale_factor)\n",
    "    font_thickness = max(1, int(1.5 * scale_factor))\n",
    "    padding = max(2, int(5 * scale_factor))\n",
    "    \n",
    "    for det in detections:\n",
    "        bbox = det['bbox']\n",
    "        label = det['label']\n",
    "        conf = det['confidence']\n",
    "        \n",
    "        x1, y1, x2, y2 = map(int, bbox)\n",
    "        \n",
    "        # Rectangle\n",
    "        cv2.rectangle(img_copy, (x1, y1), (x2, y2), color, thickness)\n",
    "        \n",
    "        # Label\n",
    "        text = f\"{label}: {conf:.2f}\"\n",
    "        (tw, th), baseline = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, font_scale, font_thickness)\n",
    "        \n",
    "        # Position du label\n",
    "        label_y = y1 - padding\n",
    "        \n",
    "        # Si le label sort en haut, le mettre en dessous\n",
    "        if label_y - th - padding < 0:\n",
    "            label_y = y2 + th + padding\n",
    "            \n",
    "        # Fond du label\n",
    "        cv2.rectangle(img_copy, \n",
    "                     (x1, label_y - th - padding), \n",
    "                     (x1 + tw + padding, label_y + padding//2), \n",
    "                     color, -1)\n",
    "        \n",
    "        # Texte en NOIR pour contraste\n",
    "        cv2.putText(img_copy, text, \n",
    "                   (x1 + padding//2, label_y - padding//2), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                   font_scale, (0, 0, 0), font_thickness, cv2.LINE_AA)\n",
    "    \n",
    "    return img_copy\n",
    "\n",
    "\n",
    "def draw_masks(image, masks, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Dessine les masques de segmentation\n",
    "    \n",
    "    Args:\n",
    "        image: Image numpy (BGR)\n",
    "        masks: Liste de masques numpy (H, W) binaires\n",
    "    \"\"\"\n",
    "    img_copy = image.copy()\n",
    "    \n",
    "    # Couleurs différentes pour chaque masque\n",
    "    colors = [\n",
    "        (255, 0, 0),    # Rouge\n",
    "        (0, 255, 0),    # Vert\n",
    "        (0, 0, 255),    # Bleu\n",
    "        (255, 255, 0),  # Cyan\n",
    "        (255, 0, 255),  # Magenta\n",
    "    ]\n",
    "    \n",
    "    for i, mask in enumerate(masks):\n",
    "        color = colors[i % len(colors)]\n",
    "        \n",
    "        # Créer overlay coloré\n",
    "        colored_mask = np.zeros_like(img_copy)\n",
    "        colored_mask[mask > 0] = color\n",
    "        \n",
    "        # Blending\n",
    "        img_copy = cv2.addWeighted(img_copy, 1, colored_mask, alpha, 0)\n",
    "    \n",
    "    return img_copy\n",
    "\n",
    "\n",
    "def visualize_comparison(image_path, gdino_dets, masks=None):\n",
    "    \"\"\"\n",
    "    Visualisation de la détection/segmentation (Image seule)\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path - Chemin vers l'image\n",
    "        gdino_dets: list - Liste des détections GroundingDINO\n",
    "        masks: list (optionnel) - Liste des masques SAM2\n",
    "    \"\"\"\n",
    "    # Charger l'image\n",
    "    image_bgr = cv2.imread(str(image_path))\n",
    "    \n",
    "    # Base image\n",
    "    vis_img = image_bgr.copy()\n",
    "    \n",
    "    # Draw masks if present\n",
    "    if masks is not None and len(masks) > 0:\n",
    "        vis_img = draw_masks(vis_img, masks, alpha=0.4)\n",
    "        \n",
    "    # Draw boxes\n",
    "    vis_img = draw_boxes(vis_img, gdino_dets, color=(0, 255, 0))\n",
    "    \n",
    "    vis_rgb = cv2.cvtColor(vis_img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Affichage en grand\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    plt.imshow(vis_rgb)\n",
    "    plt.axis('off')\n",
    "    plt.title(f'{image_path.name}', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb23ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_test_gdino(image_path, prompt, box_threshold=0.35, text_threshold=0.25):\n",
    "    \"\"\"\n",
    "    Test rapide de GroundingDINO sur une image\n",
    "    \n",
    "    Args:\n",
    "        image_path: str ou Path - Chemin vers l'image\n",
    "        prompt: str - Prompt de détection (ex: \"red ball . person\")\n",
    "        box_threshold: float - Seuil de confiance pour les boxes\n",
    "        text_threshold: float - Seuil de confiance pour le texte\n",
    "    \n",
    "    Returns:\n",
    "        Figure matplotlib avec l'image et les détections\n",
    "    \"\"\"\n",
    "    from pathlib import Path\n",
    "    \n",
    "    # Convertir en Path si nécessaire\n",
    "    img_path = Path(image_path)\n",
    "    \n",
    "    # Créer un détecteur temporaire avec les seuils demandés\n",
    "    detector = GroundingDINODetector(\n",
    "        box_threshold=box_threshold,\n",
    "        text_threshold=text_threshold\n",
    "    )\n",
    "    \n",
    "    # Détection silencieuse\n",
    "    dets_dict = detector.detect(img_path, prompt, return_all_by_label=True, debug=False)\n",
    "    \n",
    "    # Flatten les détections et compter\n",
    "    all_dets = []\n",
    "    counts = {}\n",
    "    for label, detections in dets_dict.items():\n",
    "        all_dets.extend(detections)\n",
    "        counts[label] = len(detections)\n",
    "    \n",
    "    # Afficher le résumé en une ligne\n",
    "    summary = \", \".join(f\"{count} {label}\" for label, count in counts.items())\n",
    "    print(f\"Found: {summary}\")\n",
    "    \n",
    "    # Visualiser\n",
    "    fig = visualize_comparison(img_path, all_dets)\n",
    "    # plt.show()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad30c3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = Path(\"datasets/raw\") / \"IRL_validation_pictures\" / \"IMG_9698.JPG\"\n",
    "filename=\"OIP.0bNjCsWeOxnH2t-3VE75TAHaE7.jpg\"\n",
    "img = Path(\"datasets/raw\") / \"red balls human yahoo\" / filename\n",
    "\n",
    "assert img.exists()\n",
    "quick_test_gdino(img, \n",
    "    \"red ball . person . trashcan\", \n",
    "    box_threshold=0.116, \n",
    "    text_threshold=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0969adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = Path(\"datasets/raw\") / \"IRL_validation_pictures\" / \"IMG_9698.JPG\"\n",
    "filename=\"OIP.yBxkBrvrhiGdxEGDaPCbVgHaFk.jpg\"\n",
    "img = Path(\"datasets/raw/google dated_converted_jpg/trashcans_lens/qjn7MpC.jpg\")\n",
    "\n",
    "assert img.exists()\n",
    "quick_test_gdino(img, \n",
    "    \"human . trashcan\", \n",
    "    box_threshold=0.45, \n",
    "    text_threshold=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e2ad92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = Path(\"datasets/raw\") / \"IRL_validation_pictures\" / \"IMG_9698.JPG\"\n",
    "filename=\"OIP.0QEG5dHl1XzqFM3XaQcucgAAAA.jpg\"\n",
    "img = Path(\"datasets/raw\") / \"red balls human yahoo\" / filename\n",
    "\n",
    "assert img.exists()\n",
    "quick_test_gdino(img, \n",
    "    \"red ball . person\", \n",
    "    box_threshold=0.116, \n",
    "    text_threshold=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a815cdf0",
   "metadata": {},
   "source": [
    "<!-- ## Run Inference on Test Images -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42d78e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Lister toutes les images\n",
    "# image_paths = list(TEST_IMAGES_DIR.glob(\"*.jpg\")) + list(TEST_IMAGES_DIR.glob(\"*.jpeg\"))\n",
    "# image_paths += list(TEST_IMAGES_DIR.glob(\"*.JPG\")) + list(TEST_IMAGES_DIR.glob(\"*.JPEG\"))\n",
    "\n",
    "# print(f\"Found {len(image_paths)} test images\")\n",
    "\n",
    "# # Limiter à un nombre raisonnable pour la démo\n",
    "# MAX_IMAGES = 10\n",
    "# image_paths = image_paths[:MAX_IMAGES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18c31fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Traiter chaque image - Phase 1: GroundingDINO Detection\n",
    "# results = []\n",
    "\n",
    "# print(\"=\" * 80)\n",
    "# print(\"PHASE 1: GroundingDINO Detection\")\n",
    "# print(\"=\" * 80)\n",
    "\n",
    "# for img_path in tqdm(image_paths, desc=\"GroundingDINO detection\"):\n",
    "#     print(f\"\\nProcessing: {img_path.name}\")\n",
    "    \n",
    "#     gdino_dets_dict = gdino.detect(img_path, DETECTION_PROMPT, return_all_by_label=True, debug=False)\n",
    "    \n",
    "#     # Flatten detections\n",
    "#     gdino_dets = []\n",
    "#     for label, dets in gdino_dets_dict.items():\n",
    "#         gdino_dets.extend(dets)\n",
    "    \n",
    "#     print(f\"  → Found {len(gdino_dets)} detections\")\n",
    "    \n",
    "#     # Stocker les résultats\n",
    "#     results.append({\n",
    "#         'path': img_path,\n",
    "#         'gdino_dets': gdino_dets,\n",
    "#         'sam_masks': [],  # Sera rempli plus tard\n",
    "#         'yolo_result': None  # Sera rempli plus tard\n",
    "#     })\n",
    "\n",
    "# print(f\"\\n✅ Phase 1 complete: {sum(len(r['gdino_dets']) for r in results)} total detections\")\n",
    "# print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a570a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Comparaison finale avec tous les modèles\n",
    "# num_to_display = min(3, len(results))\n",
    "\n",
    "# for i in range(num_to_display):\n",
    "#     r = results[i]\n",
    "    \n",
    "#     # Créer la visualisation complète\n",
    "#     fig = visualize_comparison(r['path'], r['gdino_dets'])\n",
    "    \n",
    "#     # Sauvegarder\n",
    "#     output_path = OUTPUT_DIR / f\"{r['path'].stem}_comparison.png\"\n",
    "#     fig.savefig(output_path, dpi=100, bbox_inches='tight')\n",
    "    \n",
    "#     plt.show()\n",
    "#     print(f\"Saved to: {output_path}\\n\")\n",
    "\n",
    "# print(f\"✅ All comparisons saved to {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8dcee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sam = SAMSegmenter()\n",
    "\n",
    "# print(\"\\nLoading YOLOv11...\")\n",
    "# yolo = YOLO(YOLO_MODEL)\n",
    "# print(\"✓ YOLOv11 loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43344782",
   "metadata": {},
   "source": [
    "<!-- ### Comparaison finale: Tous les modèles -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6f589d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Phase 3: YOLOv11 Inference\n",
    "# print(\"=\" * 80)\n",
    "# print(\"PHASE 3: YOLOv11 Detection\")\n",
    "# print(\"=\" * 80)\n",
    "\n",
    "# for r in tqdm(results, desc=\"YOLOv11 inference\"):\n",
    "#     img_path = r['path']\n",
    "    \n",
    "#     yolo_result = yolo(str(img_path), conf=YOLO_CONF, verbose=False)\n",
    "#     r['yolo_result'] = yolo_result\n",
    "    \n",
    "#     num_dets = len(yolo_result[0].boxes) if yolo_result else 0\n",
    "#     print(f\"{img_path.name}: {num_dets} detections\")\n",
    "\n",
    "# total_yolo = sum(len(r['yolo_result'][0].boxes) if r['yolo_result'] else 0 for r in results)\n",
    "# print(f\"\\n✅ Phase 3 complete: {total_yolo} total detections\")\n",
    "# print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f7f6df",
   "metadata": {},
   "source": [
    "<!-- ### Phase 3: YOLOv11 (optionnel) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f494f019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Afficher les résultats GroundingDINO + SAM2\n",
    "# num_to_display = min(5, len(results))\n",
    "\n",
    "# fig, axes = plt.subplots(num_to_display, 3, figsize=(20, 6 * num_to_display))\n",
    "# if num_to_display == 1:\n",
    "#     axes = axes.reshape(1, -1)\n",
    "\n",
    "# for i in range(num_to_display):\n",
    "#     r = results[i]\n",
    "#     img_path = r['path']\n",
    "#     gdino_dets = r['gdino_dets']\n",
    "#     sam_masks = r['sam_masks']\n",
    "    \n",
    "#     # Charger l'image\n",
    "#     image_bgr = cv2.imread(str(img_path))\n",
    "#     image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "#     # Original\n",
    "#     axes[i, 0].imshow(image_rgb)\n",
    "#     axes[i, 0].set_title(f'Original: {img_path.name}')\n",
    "#     axes[i, 0].axis('off')\n",
    "    \n",
    "#     # GroundingDINO boxes\n",
    "#     gdino_vis = draw_boxes(image_bgr, gdino_dets, color=(0, 255, 0), thickness=3)\n",
    "#     gdino_vis_rgb = cv2.cvtColor(gdino_vis, cv2.COLOR_BGR2RGB)\n",
    "#     axes[i, 1].imshow(gdino_vis_rgb)\n",
    "#     axes[i, 1].set_title(f'GroundingDINO: {len(gdino_dets)} boxes')\n",
    "#     axes[i, 1].axis('off')\n",
    "    \n",
    "#     # GroundingDINO + SAM2\n",
    "#     if sam_masks:\n",
    "#         sam_vis = draw_masks(image_bgr, sam_masks, alpha=0.4)\n",
    "#         sam_vis = draw_boxes(sam_vis, gdino_dets, color=(255, 255, 0), thickness=2)\n",
    "#         sam_vis_rgb = cv2.cvtColor(sam_vis, cv2.COLOR_BGR2RGB)\n",
    "#         axes[i, 2].imshow(sam_vis_rgb)\n",
    "#         axes[i, 2].set_title(f'GDINO + SAM2: {len(sam_masks)} masks')\n",
    "#     else:\n",
    "#         axes[i, 2].imshow(image_rgb)\n",
    "#         axes[i, 2].set_title('GDINO + SAM2: no masks')\n",
    "#     axes[i, 2].axis('off')\n",
    "\n",
    "# plt.suptitle('Phase 2: GroundingDINO + SAM2 Segmentation Results', fontsize=16, fontweight='bold')\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9942b9b2",
   "metadata": {},
   "source": [
    "<!-- # Afficher les résultats GroundingDINO + SAM2\n",
    "num_to_display = min(5, len(results))\n",
    "\n",
    "for i in range(num_to_display):\n",
    "    r = results[i]\n",
    "    img_path = r['path']\n",
    "    gdino_dets = r['gdino_dets']\n",
    "    sam_masks = r['sam_masks']\n",
    "    \n",
    "    # Utiliser la nouvelle fonction de visualisation avec masques\n",
    "    print(f\"\\nResult for: {img_path.name}\")\n",
    "    fig = visualize_comparison(img_path, gdino_dets, sam_masks)\n",
    "    plt.show() -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4bb3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Phase 2: SAM2 Segmentation guidée par GroundingDINO\n",
    "# print(\"=\" * 80)\n",
    "# print(\"PHASE 2: SAM2 Segmentation\")\n",
    "# print(\"=\" * 80)\n",
    "\n",
    "# for r in tqdm(results, desc=\"SAM2 segmentation\"):\n",
    "#     img_path = r['path']\n",
    "#     gdino_dets = r['gdino_dets']\n",
    "    \n",
    "#     print(f\"\\nProcessing: {img_path.name}\")\n",
    "    \n",
    "#     if gdino_dets:\n",
    "#         bboxes = [det['bbox'] for det in gdino_dets]\n",
    "#         masks_result = sam.segment(img_path, bboxes)\n",
    "#         if masks_result:\n",
    "#             r['sam_masks'] = masks_result['masks']\n",
    "#             print(f\"  → Generated {len(r['sam_masks'])} masks\")\n",
    "#         else:\n",
    "#             print(\"  → No masks generated\")\n",
    "#     else:\n",
    "#         print(\"  → Skipped (no detections)\")\n",
    "\n",
    "# total_masks = sum(len(r['sam_masks']) for r in results)\n",
    "# print(f\"\\n✅ Phase 2 complete: {total_masks} total masks\")\n",
    "# print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2509cfb",
   "metadata": {},
   "source": [
    "<!-- ### Phase 2: GroundingDINO + SAM2 Segmentation -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ef82c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Afficher les résultats GroundingDINO\n",
    "# num_to_display = min(5, len(results))\n",
    "\n",
    "# for i in range(num_to_display):\n",
    "#     r = results[i]\n",
    "#     img_path = r['path']\n",
    "#     gdino_dets = r['gdino_dets']\n",
    "    \n",
    "#     # Utiliser la nouvelle fonction de visualisation\n",
    "#     print(f\"\\nResult for: {img_path.name}\")\n",
    "#     fig = visualize_comparison(img_path, gdino_dets)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85db286",
   "metadata": {},
   "source": [
    "<!-- ### Phase 1: Visualiser les détections GroundingDINO -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171e0425",
   "metadata": {},
   "source": [
    "<!-- ## Display Results Summary -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86e444d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Statistiques\n",
    "# total_gdino = sum(len(r['gdino_dets']) for r in results)\n",
    "# total_sam = sum(len(r['sam_masks']) for r in results)\n",
    "# total_yolo = sum(len(r['yolo_result'][0].boxes) if r['yolo_result'] else 0 for r in results)\n",
    "\n",
    "# print(\"\\n\" + \"=\"*60)\n",
    "# print(\"SUMMARY\")\n",
    "# print(\"=\"*60)\n",
    "# print(f\"Images processed: {len(results)}\")\n",
    "# print(f\"\\nTotal detections:\")\n",
    "# print(f\"  - GroundingDINO: {total_gdino}\")\n",
    "# print(f\"  - SAM2 masks: {total_sam}\")\n",
    "# print(f\"  - YOLOv11: {total_yolo}\")\n",
    "# print(f\"\\nAverage per image:\")\n",
    "# print(f\"  - GroundingDINO: {total_gdino/len(results):.1f}\")\n",
    "# print(f\"  - SAM2: {total_sam/len(results):.1f}\")\n",
    "# print(f\"  - YOLOv11: {total_yolo/len(results):.1f}\")\n",
    "# print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10b5343",
   "metadata": {},
   "source": [
    "<!-- ## Display Sample Comparisons -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4a819b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Afficher quelques exemples directement dans le notebook\n",
    "# num_to_display = min(3, len(results))\n",
    "\n",
    "# for i in range(num_to_display):\n",
    "#     r = results[i]\n",
    "#     # Visualisation avec masques si disponibles\n",
    "#     fig = visualize_comparison(r['path'], r['gdino_dets'], r['sam_masks'])\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6d25c6",
   "metadata": {},
   "source": [
    "<!-- ## Performance Comparison\n",
    "\n",
    "### GroundingDINO + SAM2\n",
    "**Avantages** :\n",
    "- Zero-shot : fonctionne sans entraînement\n",
    "- Flexible : peut détecter n'importe quelle classe avec un prompt\n",
    "- Segmentation précise avec SAM2\n",
    "\n",
    "**Inconvénients** :\n",
    "- Plus lent (2 modèles)\n",
    "- Nécessite des prompts bien formulés\n",
    "- Peut nécessiter du tuning des seuils\n",
    "\n",
    "### YOLOv11\n",
    "**Avantages** :\n",
    "- Rapide (1 seul modèle)\n",
    "- Détection + segmentation en une passe\n",
    "- Performance spécialisée sur les classes entraînées\n",
    "\n",
    "**Inconvénients** :\n",
    "- Nécessite un entraînement\n",
    "- Limité aux classes du dataset d'entraînement\n",
    "- Moins flexible -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
