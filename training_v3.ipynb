{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c885d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "from ultralytics.models import YOLO\n",
    "import torch\n",
    "import yaml\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b959d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import DisplayPath\n",
    "Path = DisplayPath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a0d47a",
   "metadata": {},
   "source": [
    "## Step 2: Configure Paths & Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5bae1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset path (created by e2e_data_prep.ipynb)\n",
    "YOLO_DATASET = Path(\"datasets/ready/full_dataset\")\n",
    "RUNS_DIR = Path(\"runs/segment\")\n",
    "\n",
    "# Verify dataset exists\n",
    "if not YOLO_DATASET.exists():\n",
    "    raise FileNotFoundError(f\"Dataset not found at {YOLO_DATASET}. Run e2e_data_prep.ipynb first!\")\n",
    "\n",
    "print(\"Dataset:\")\n",
    "YOLO_DATASET.display()\n",
    "print(\"  Train:\")\n",
    "(YOLO_DATASET / 'train').display()\n",
    "print(\"  Val:\")\n",
    "(YOLO_DATASET / 'val').display()\n",
    "print(\"  Test:\")\n",
    "(YOLO_DATASET / 'test').display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dc79bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "BATCH_SIZE = 16\n",
    "IMG_SIZE = 640\n",
    "model_type = \"yolo11n-seg.pt\"\n",
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8976809f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration d'augmentation R√âDUITE pour le training YOLO\n",
    "# Puisqu'on pr√©-augmente massivement les trashcans, on r√©duit l'augmentation\n",
    "# globale pour √©viter de sur-augmenter les red balls et humans\n",
    "AUG_CONFIG = {\n",
    "    'hsv_h': 0.010,  # Hue augmentation (r√©duit de 0.015)\n",
    "    'hsv_s': 0.5,    # Saturation (r√©duit de 0.7)\n",
    "    'hsv_v': 0.3,    # Value (r√©duit de 0.4)\n",
    "    'degrees': 5.0,   # Rotation (r√©duit de 10.0)\n",
    "    'translate': 0.05, # Translation (r√©duit de 0.1)\n",
    "    'scale': 0.3,     # Scaling (r√©duit de 0.5)\n",
    "    'shear': 0.0,     # Shearing\n",
    "    'perspective': 0.0, # Perspective\n",
    "    'flipud': 0.0,    # Vertical flip\n",
    "    'fliplr': 0.5,    # Horizontal flip (maintenu)\n",
    "    'mosaic': 0.5,    # Mosaic augmentation (r√©duit de 1.0)\n",
    "    'mixup': 0.0,     # Mixup augmentation\n",
    "    'copy_paste': 0.3, # üÜï Copy-paste aug pour classes rares\n",
    "}\n",
    "\n",
    "print(\"‚ö†Ô∏è  Augmentation globale R√âDUITE pour √©viter la sur-augmentation\")\n",
    "print(\"   Les trashcans sont pr√©-augment√©es massivement avant le training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8e652f",
   "metadata": {},
   "source": [
    "## Step 3: Verify Dataset Structure\n",
    "\n",
    "Dataset is already prepared by e2e_data_prep.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0e39b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dataset structure\n",
    "print(\"=\"*60)\n",
    "print(\"DATASET VERIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "splits = ['train', 'val', 'test']\n",
    "stats = {}\n",
    "\n",
    "for split in splits:\n",
    "    img_dir = YOLO_DATASET / split / \"images\"\n",
    "    lbl_dir = YOLO_DATASET / split / \"labels\"\n",
    "    \n",
    "    if img_dir.exists() and lbl_dir.exists():\n",
    "        num_images = len(list(img_dir.glob(\"*\")))\n",
    "        num_labels = len(list(lbl_dir.glob(\"*.txt\")))\n",
    "        stats[split] = {'images': num_images, 'labels': num_labels}\n",
    "        print(f\"{split.upper():5s}: {num_images:4d} images, {num_labels:4d} labels\")\n",
    "    else:\n",
    "        stats[split] = {'images': 0, 'labels': 0}\n",
    "        print(f\"{split.upper():5s}: Missing!\")\n",
    "\n",
    "total_images = sum(s['images'] for s in stats.values())\n",
    "total_labels = sum(s['labels'] for s in stats.values())\n",
    "\n",
    "print(f\"{'TOTAL':5s}: {total_images:4d} images, {total_labels:4d} labels\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if total_images == 0:\n",
    "    raise RuntimeError(\"No dataset found! Run e2e_data_prep.ipynb to create the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e32ee53",
   "metadata": {},
   "source": [
    "## Step 3.5: Analyze Class Distribution\n",
    "\n",
    "Check the distribution of classes in the training set to identify imbalances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2371c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze class distribution in training set\n",
    "def count_class_instances(split):\n",
    "    \"\"\"Count instances of each class in a dataset split\"\"\"\n",
    "    label_dir = YOLO_DATASET / split / \"labels\"\n",
    "    class_counts = {0: 0, 1: 0, 2: 0}  # red ball, human, trashcan\n",
    "    \n",
    "    for label_file in label_dir.glob(\"*.txt\"):\n",
    "        try:\n",
    "            with open(label_file, 'r') as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    if parts:\n",
    "                        class_id = int(float(parts[0]))\n",
    "                        class_counts[class_id] = class_counts.get(class_id, 0) + 1\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return class_counts\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CLASS DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "class_names = {0: 'Red Ball', 1: 'Human', 2: 'Trashcan'}\n",
    "\n",
    "for split in ['train', 'val']:\n",
    "    counts = count_class_instances(split)\n",
    "    total = sum(counts.values())\n",
    "    \n",
    "    if total > 0:\n",
    "        print(f\"\\n{split.upper()}:\")\n",
    "        for class_id, count in counts.items():\n",
    "            percentage = (count / total * 100) if total > 0 else 0\n",
    "            print(f\"  {class_names[class_id]:12s} (class {class_id}): {count:5d} instances ({percentage:5.1f}%)\")\n",
    "        print(f\"  {'TOTAL':12s}           : {total:5d} instances\")\n",
    "        \n",
    "        # Calculate imbalance ratio\n",
    "        if counts[2] > 0:  # If trashcans exist\n",
    "            max_count = max(counts.values())\n",
    "            min_count = min(v for v in counts.values() if v > 0)\n",
    "            imbalance_ratio = max_count / min_count\n",
    "            print(f\"  Imbalance ratio: {imbalance_ratio:.1f}x\")\n",
    "            \n",
    "            if imbalance_ratio > 10:\n",
    "                print(f\"  ‚ö†Ô∏è  High class imbalance detected! Using cls=2.0 to compensate.\")\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è  WARNING: No trashcans in {split} set!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6742ed58",
   "metadata": {},
   "source": [
    "## Step 3.6: Augment Trashcan Training Data\n",
    "\n",
    "Since trashcans are under-represented, create multiple augmented copies to balance the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf79fcde",
   "metadata": {},
   "source": [
    "**Strategy:**\n",
    "- üî¥ Red balls & üë§ Humans: Already abundant ‚Üí Light augmentation during training\n",
    "- üóëÔ∏è Trashcans: Rare class ‚Üí **Massive pre-augmentation** (15x copies with strong transforms)\n",
    "- Each trashcan image gets 15 augmented variants with:\n",
    "  - Random rotations (¬±90¬∞, ¬±20¬∞)\n",
    "  - Horizontal/vertical flips\n",
    "  - Brightness/contrast/hue adjustments\n",
    "  - Scaling & translation\n",
    "  - Gaussian noise\n",
    "  \n",
    "**Benefits:**\n",
    "- Balances class distribution without over-augmenting dominant classes\n",
    "- Preserves label quality through geometric transform propagation\n",
    "- Creates diverse trashcan appearances for better generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c5dd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import augmentation utilities\n",
    "from src.augmentation import augment_trashcan_dataset\n",
    "\n",
    "# Execute augmentation\n",
    "AUGMENT_TRASHCANS = True  # Set to False to skip augmentation\n",
    "NUM_AUGMENTATIONS = 15     # Number of augmented copies per trashcan image\n",
    "AUG_STRENGTH = 'strong'    # 'strong' or 'light'\n",
    "\n",
    "if AUGMENT_TRASHCANS:\n",
    "    # TODO: refactor name for more general use\n",
    "    new_images = augment_trashcan_dataset(\n",
    "        dataset_path=YOLO_DATASET,\n",
    "        num_augmentations=NUM_AUGMENTATIONS,\n",
    "        aug_strength=AUG_STRENGTH\n",
    "    )\n",
    "    \n",
    "    if new_images > 0:\n",
    "        print(f\"\\nüí° Tip: Re-run the class distribution analysis cell to see the updated statistics!\")\n",
    "else:\n",
    "    print(\"Trashcan augmentation skipped (AUGMENT_TRASHCANS = False)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b979e79",
   "metadata": {},
   "source": [
    "## Step 4: Create YOLO Configuration File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739a4bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {\n",
    "    'red ball': 0,\n",
    "    'human': 1,\n",
    "    'trashcan': 2\n",
    "}\n",
    "\n",
    "config = {\n",
    "    'path': str(YOLO_DATASET.absolute()),\n",
    "    'train': 'train/images',\n",
    "    'val': 'val/images',\n",
    "    'nc': len(classes),\n",
    "    'names': list(classes.keys())\n",
    "}\n",
    "\n",
    "config_path = YOLO_DATASET / 'data.yaml'\n",
    "with open(config_path, 'w') as f:\n",
    "    yaml.dump(config, f, default_flow_style=False)\n",
    "\n",
    "print(f\"‚úì Configuration saved: {config_path}\")\n",
    "print(\"Dataset structure:\")\n",
    "YOLO_DATASET.display()\n",
    "print(\"  Train:\")\n",
    "(YOLO_DATASET / 'train').display()\n",
    "print(\"  Val:\")\n",
    "(YOLO_DATASET / 'val').display()\n",
    "print(\"  Test:\")\n",
    "(YOLO_DATASET / 'test').display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db1af91",
   "metadata": {},
   "source": [
    "## Step 4.5: Find Trashcan Images for Monitoring\n",
    "\n",
    "Identify validation images containing trashcans (class 2) to monitor training progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8f255d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_trashcan_images(val_labels_dir, val_images_dir, max_images=8):\n",
    "    \"\"\"Find validation images that contain trashcan annotations (class 2)\"\"\"\n",
    "    trashcan_images = []\n",
    "    \n",
    "    # Scan all label files\n",
    "    label_files = list(val_labels_dir.glob(\"*.txt\"))\n",
    "    print(f\"Scanning {len(label_files)} label files for trashcans...\")\n",
    "    \n",
    "    for label_file in label_files:\n",
    "        try:\n",
    "            with open(label_file, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "                \n",
    "            # Check if any line starts with \"2 \" (trashcan class)\n",
    "            has_trashcan = any(line.strip().startswith('2 ') for line in lines)\n",
    "            \n",
    "            if has_trashcan:\n",
    "                # Find corresponding image\n",
    "                img_name = label_file.stem\n",
    "                \n",
    "                # Try different image extensions\n",
    "                for ext in ['.jpg', '.jpeg', '.png', '.JPG', '.JPEG', '.PNG']:\n",
    "                    img_path = val_images_dir / f\"{img_name}{ext}\"\n",
    "                    if img_path.exists():\n",
    "                        trashcan_images.append(str(img_path))\n",
    "                        print(f\"  ‚úì Found trashcan in: {img_path.name}\")\n",
    "                        break\n",
    "                \n",
    "                if len(trashcan_images) >= max_images:\n",
    "                    break\n",
    "        except Exception as e:\n",
    "            # Skip problematic files (e.g., too large)\n",
    "            continue\n",
    "    \n",
    "    return trashcan_images\n",
    "\n",
    "# Find trashcan images\n",
    "val_labels_dir = YOLO_DATASET / 'val' / 'labels'\n",
    "val_images_dir = YOLO_DATASET / 'val' / 'images'\n",
    "\n",
    "TRASHCAN_MONITOR_IMAGES = find_trashcan_images(val_labels_dir, val_images_dir, max_images=8)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Selected {len(TRASHCAN_MONITOR_IMAGES)} images for trashcan monitoring:\")\n",
    "for img_path in TRASHCAN_MONITOR_IMAGES:\n",
    "    print(f\"  - {Path(img_path).name}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "if len(TRASHCAN_MONITOR_IMAGES) == 0:\n",
    "    print(\"‚ö†Ô∏è  Warning: No trashcan images found in validation set!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0cd691",
   "metadata": {},
   "source": [
    "## Step 4.6: Define Trashcan Monitoring Callback\n",
    "\n",
    "Create a callback that visualizes trashcan segmentation progress at each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9e4bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trashcan_monitor_callback(model, monitor_images, output_dir, project_name):\n",
    "    \"\"\"Create a callback to monitor trashcan segmentation progress during training\"\"\"\n",
    "    \n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    def on_train_epoch_end(trainer):\n",
    "        \"\"\"Called at the end of each training epoch\"\"\"\n",
    "        if len(monitor_images) == 0:\n",
    "            return\n",
    "        \n",
    "        epoch = trainer.epoch\n",
    "        \n",
    "        # Skip first few epochs to save time\n",
    "        if epoch < 5:\n",
    "            return\n",
    "        \n",
    "        # Run predictions on monitor images\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Generating trashcan monitoring visualizations for epoch {epoch}...\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Save current training state\n",
    "        was_training = trainer.model.training\n",
    "        \n",
    "        try:\n",
    "            # Put model in eval mode and disable gradients\n",
    "            trainer.model.eval()\n",
    "            \n",
    "            # Create figure with subplots\n",
    "            n_images = len(monitor_images)\n",
    "            n_cols = min(3, n_images)\n",
    "            n_rows = (n_images + n_cols - 1) // n_cols\n",
    "            \n",
    "            fig = plt.figure(figsize=(6 * n_cols, 5 * n_rows))\n",
    "            gs = GridSpec(n_rows, n_cols, figure=fig, hspace=0.3, wspace=0.3)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for idx, img_path in enumerate(monitor_images):\n",
    "                    try:\n",
    "                        # Use the YOLO wrapper's predict method\n",
    "                        # Create temporary YOLO instance with current weights\n",
    "                        temp_model = YOLO(trainer.best if hasattr(trainer, 'best') else trainer.last)\n",
    "                        results = temp_model.predict(img_path, conf=0.25, verbose=False)\n",
    "                        \n",
    "                        # Get the plotted image\n",
    "                        result = results[0]\n",
    "                        plotted_img = result.plot()\n",
    "                        \n",
    "                        # Convert BGR to RGB\n",
    "                        plotted_img = cv2.cvtColor(plotted_img, cv2.COLOR_BGR2RGB)\n",
    "                        \n",
    "                        # Add to subplot\n",
    "                        row = idx // n_cols\n",
    "                        col = idx % n_cols\n",
    "                        ax = fig.add_subplot(gs[row, col])\n",
    "                        ax.imshow(plotted_img)\n",
    "                        ax.axis('off')\n",
    "                        \n",
    "                        # Count detections by class\n",
    "                        if result.masks is not None and len(result.boxes) > 0:\n",
    "                            classes_detected = result.boxes.cls.cpu().numpy()\n",
    "                            n_balls = int((classes_detected == 0).sum())\n",
    "                            n_humans = int((classes_detected == 1).sum())\n",
    "                            n_trashcans = int((classes_detected == 2).sum())\n",
    "                            \n",
    "                            title = f\"{Path(img_path).name}\\n\"\n",
    "                            title += f\"Balls: {n_balls} | Humans: {n_humans} | Trashcans: {n_trashcans}\"\n",
    "                        else:\n",
    "                            title = f\"{Path(img_path).name}\\nNo detections\"\n",
    "                        \n",
    "                        ax.set_title(title, fontsize=10)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"  ‚ö†Ô∏è  Error processing {Path(img_path).name}: {e}\")\n",
    "            \n",
    "            # Add main title\n",
    "            fig.suptitle(f\"Trashcan Monitoring - Epoch {epoch} - {project_name}\", \n",
    "                         fontsize=16, fontweight='bold', y=0.995)\n",
    "            \n",
    "            # Save figure\n",
    "            output_file = output_dir / f\"epoch_{epoch:03d}.jpg\"\n",
    "            plt.savefig(output_file, dpi=150, bbox_inches='tight')\n",
    "            plt.close(fig)\n",
    "            \n",
    "            print(f\"‚úì Saved monitoring visualization:\")\n",
    "            output_dir.display()\n",
    "            print(f\"{'='*60}\\n\")\n",
    "            \n",
    "        finally:\n",
    "            # Restore training mode\n",
    "            if was_training:\n",
    "                trainer.model.train()\n",
    "    \n",
    "    return on_train_epoch_end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853894a7",
   "metadata": {},
   "source": [
    "## Step 5: Train Model with Trashcan Monitoring\n",
    "\n",
    "Train YOLOv11 with:\n",
    "- Data augmentation on train set\n",
    "- Checkpoints saved for best model\n",
    "- Validation after each epoch\n",
    "- **Custom callback to monitor trashcan segmentation progress**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0c5a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained model\n",
    "model = YOLO(model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4f6fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = 'ball_person_trashcan_model_v3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3f23d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup trashcan monitoring\n",
    "monitor_output_dir = RUNS_DIR / project_name / 'trashcan_monitor'\n",
    "print(f\"Trashcan monitoring output: {monitor_output_dir}\")\n",
    "\n",
    "# Add callback\n",
    "callback_fn = create_trashcan_monitor_callback(\n",
    "    model=model,\n",
    "    monitor_images=TRASHCAN_MONITOR_IMAGES,\n",
    "    output_dir=monitor_output_dir,\n",
    "    project_name=project_name\n",
    ")\n",
    "\n",
    "model.add_callback('on_train_epoch_end', callback_fn)\n",
    "print(\"‚úì Trashcan monitoring callback registered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a8ad51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "head_idx = next((i for i, m in enumerate(model.model.model) if 'Detect' in m.__class__.__name__ or 'Segment' in m.__class__.__name__), len(model.model.model) - 1)\n",
    "\n",
    "results = model.train(\n",
    "    data=str(config_path),\n",
    "    epochs=EPOCHS,\n",
    "    freeze=list(range(head_idx)),\n",
    "    batch=BATCH_SIZE,\n",
    "    imgsz=IMG_SIZE,\n",
    "    device=DEVICE,\n",
    "    project=str(RUNS_DIR),\n",
    "    name=project_name,\n",
    "    exist_ok=True,\n",
    "    \n",
    "    # Checkpointing\n",
    "    save=True,\n",
    "    save_period=5,  # Save every 5 epochs\n",
    "    \n",
    "    # Validation\n",
    "    val=True,\n",
    "    \n",
    "    # Data augmentation (R√âDUITE - trashcans pr√©-augment√©es massivement)\n",
    "    **AUG_CONFIG,\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer='Adam',\n",
    "    lr0=0.001,\n",
    "    lrf=0.01,\n",
    "    momentum=0.937,\n",
    "    weight_decay=0.0005,\n",
    "    \n",
    "    # Loss weights - Ajust√© pour dataset avec trashcans augment√©es\n",
    "    # Avec l'augmentation massive des trashcans, on peut r√©duire cls\n",
    "    box=7.5,\n",
    "    cls=1.0,      # R√©duit de 20.0 √† 1.0 car trashcans maintenant bien repr√©sent√©es\n",
    "    dfl=1.5,\n",
    "    \n",
    "    # Other\n",
    "    patience=20,  # Early stopping\n",
    "    workers=8,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573f1e2e",
   "metadata": {},
   "source": [
    "## Step 6: Load Best Model & Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcded3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = RUNS_DIR / project_name / 'weights' / 'best.pt'\n",
    "best_model_path.display()\n",
    "model = YOLO(best_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4317200b",
   "metadata": {},
   "source": [
    "## Step 7: Evaluate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02a0a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation metrics\n",
    "metrics = model.val()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VALIDATION METRICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Box mAP50: {metrics.box.map50:.4f}\")\n",
    "print(f\"Box mAP50-95: {metrics.box.map:.4f}\")\n",
    "print(f\"Mask mAP50: {metrics.seg.map50:.4f}\")\n",
    "print(f\"Mask mAP50-95: {metrics.seg.map:.4f}\")\n",
    "\n",
    "# Per-class metrics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PER-CLASS METRICS (Segmentation)\")\n",
    "print(\"=\"*60)\n",
    "class_names = ['red ball', 'human', 'trashcan']\n",
    "for i, class_name in enumerate(class_names):\n",
    "    try:\n",
    "        map50 = metrics.seg.map50_per_class[i] if hasattr(metrics.seg, 'map50_per_class') else 0\n",
    "        map_val = metrics.seg.map_per_class[i] if hasattr(metrics.seg, 'map_per_class') else 0\n",
    "        print(f\"{class_name:12s}: mAP50={map50:.4f}, mAP50-95={map_val:.4f}\")\n",
    "    except:\n",
    "        print(f\"{class_name:12s}: metrics not available\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d36b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best checkpoint\n",
    "model_dir = RUNS_DIR / project_name\n",
    "best_model = model_dir / 'weights' / 'best.pt'\n",
    "last_model = model_dir / 'weights' / 'last.pt'\n",
    "\n",
    "print(f\"Best model: \")\n",
    "best_model.display()\n",
    "print(f\"Last model: \")\n",
    "last_model.display()\n",
    "print(f\"Results: \")\n",
    "model_dir.display()\n",
    "print(f\"\\nTrashcan monitoring visualizations: \")\n",
    "(model_dir / 'trashcan_monitor').display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f94a189",
   "metadata": {},
   "source": [
    "## Step 8: Visualize Trashcan Progress Evolution\n",
    "\n",
    "Create a comparison showing how trashcan segmentation improved over epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac1cf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all monitoring visualizations\n",
    "monitor_dir = RUNS_DIR / project_name / 'trashcan_monitor'\n",
    "\n",
    "if monitor_dir.exists():\n",
    "    viz_files = sorted(monitor_dir.glob(\"epoch_*.jpg\"))\n",
    "    print(f\"Found {len(viz_files)} monitoring visualizations:\")\n",
    "    for viz_file in viz_files:\n",
    "        print(f\"  - {viz_file.name}\")\n",
    "    \n",
    "    if len(viz_files) > 0:\n",
    "        print(f\"\\nüí° Tip: Open the images in {monitor_dir} to see how trashcan segmentation evolved!\")\n",
    "        print(f\"   You can use an image viewer or VS Code to flip through them chronologically.\")\n",
    "else:\n",
    "    print(\"No monitoring visualizations found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bc9c5e",
   "metadata": {},
   "source": [
    "## Step 9: Test on Sample Images (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f791323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on validation images (sample from val set)\n",
    "test_images = list((YOLO_DATASET / \"val\" / \"images\").glob(\"*\"))[:10]\n",
    "\n",
    "print(f\"Testing on {len(test_images)} sample images...\")\n",
    "\n",
    "for img_path in test_images:\n",
    "    results = model.predict(str(img_path), save=True, conf=0.25)\n",
    "    print(f\"  ‚úì {img_path.name}\")\n",
    "\n",
    "print(f\"\\nResults saved to: {RUNS_DIR / project_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
