{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb0e6ee9",
   "metadata": {},
   "source": [
    "## Step 1: Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "446eb0fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[12/17/25 16:42:09] </span><span style=\"color: #808000; text-decoration-color: #808000\">WARNING </span> Your inference package version <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.62</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> is out of date! Please upgrade to <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">__init__.py:41</span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         version <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.62</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> of inference for the latest features and bug fixes by    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         running `pip install --upgrade inference`.                              <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[12/17/25 16:42:09]\u001b[0m\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Your inference package version \u001b[1;36m0.62\u001b[0m.\u001b[1;36m4\u001b[0m is out of date! Please upgrade to \u001b[2m__init__.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m41\u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         version \u001b[1;36m0.62\u001b[0m.\u001b[1;36m5\u001b[0m of inference for the latest features and bug fixes by    \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         running `pip install --upgrade inference`.                              \u001b[2m              \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tonino/projects/ball segmentation/.venv/lib/python3.12/site-packages/inference/models/utils.py:378: ModelDependencyMissing: Your `inference` configuration does not support SAM model. Use pip install 'inference[sam]' to install missing requirements.To suppress this warning, set CORE_MODEL_SAM_ENABLED to False.\n",
      "  warnings.warn(\n",
      "/home/tonino/projects/ball segmentation/.venv/lib/python3.12/site-packages/inference/models/utils.py:390: ModelDependencyMissing: Your `inference` configuration does not support SAM2 model. Use pip install 'inference[sam]' to install missing requirements.To suppress this warning, set CORE_MODEL_SAM2_ENABLED to False.\n",
      "  warnings.warn(\n",
      "/home/tonino/projects/ball segmentation/.venv/lib/python3.12/site-packages/inference/models/utils.py:411: ModelDependencyMissing: Your `inference` configuration does not support SAM3 model. Install SAM3 dependencies and set CORE_MODEL_SAM3_ENABLED to True.\n",
      "  warnings.warn(\n",
      "/home/tonino/projects/ball segmentation/.venv/lib/python3.12/site-packages/inference/models/utils.py:450: ModelDependencyMissing: Your `inference` configuration does not support Gaze Detection model. Use pip install 'inference[gaze]' to install missing requirements.To suppress this warning, set CORE_MODEL_GAZE_ENABLED to False.\n",
      "  warnings.warn(\n",
      "/home/tonino/projects/ball segmentation/.venv/lib/python3.12/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from src.detection import GroundingDINODetector\n",
    "from src.segmentation import FastSAMSegmenter\n",
    "from src.pipeline import img_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3672e5da",
   "metadata": {},
   "source": [
    "## Step 3: Load Models\n",
    "\n",
    "- **Detector**: Grounding DINO (text prompts: \"red ball\" & \"human\")\n",
    "- **Segmenter**: FastSAM (bbox-guided segmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f52222e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Checkpoint found: models/pretrained/groundingdino_swint_ogc.pth\n"
     ]
    }
   ],
   "source": [
    "# Download Grounding DINO weights if needed\n",
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "checkpoint_path = Path(\"models/pretrained/groundingdino_swint_ogc.pth\")\n",
    "if not checkpoint_path.exists():\n",
    "    print(\"Downloading Grounding DINO checkpoint (~693 MB)...\")\n",
    "    checkpoint_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    url = \"https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth\"\n",
    "    response = requests.get(url, stream=True)\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    \n",
    "    with open(checkpoint_path, 'wb') as f:\n",
    "        downloaded = 0\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "            downloaded += len(chunk)\n",
    "            if total_size > 0:\n",
    "                percent = (downloaded / total_size) * 100\n",
    "                print(f\"\\rProgress: {percent:.1f}%\", end='')\n",
    "    print(f\"\\n✓ Downloaded to {checkpoint_path}\")\n",
    "else:\n",
    "    print(f\"✓ Checkpoint found: {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "beb61f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1203dfc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tonino/projects/ball segmentation/.venv/lib/python3.12/site-packages/torch/functional.py:505: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4317.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n"
     ]
    }
   ],
   "source": [
    "# Grounding DINO detector (unified detector for balls AND persons)\n",
    "# Using lower thresholds to match the online demo behavior\n",
    "from src.segmentation import SAMSegmenter\n",
    "\n",
    "\n",
    "detector = GroundingDINODetector(\n",
    "    model_checkpoint_path=str(checkpoint_path),\n",
    "    box_threshold=0.5,  \n",
    "    text_threshold=0.5,  \n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "segmenter = SAMSegmenter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0aafad",
   "metadata": {},
   "source": [
    "## Step 2: Configure Paths & Device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09097b74",
   "metadata": {},
   "source": [
    "#### Use this one for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db68ce80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input path\n",
    "\n",
    "IRL_RAW = Path(\"datasets/raw/IRL_validation_pictures\")\n",
    "yahoo_balls_raw = Path(\"datasets/cleaned/red_balls_human_yahoo_jpg\")\n",
    "\n",
    "class DatasetPaths:\n",
    "    \"\"\"Manages preprocessing paths with ready subfolder for final outputs\"\"\"\n",
    "    def __init__(self, dataset_name: str, base=Path(\"datasets/preprocessed\"), create=True):\n",
    "        project = base / dataset_name\n",
    "\n",
    "        # Intermediate outputs (visualizations)\n",
    "        self.det_path = project / \"detection\"\n",
    "        self.seg_path = project / \"segmentation\"\n",
    "        self.empty_path = project / \"empty\"\n",
    "        \n",
    "        # Ready folder with images and labels\n",
    "        self.ready_path = project / \"ready\"\n",
    "        self.images_path = self.ready_path / \"images\"\n",
    "        self.label_path = self.ready_path / \"labels\"\n",
    "        \n",
    "        paths = [\n",
    "            project, \n",
    "            self.det_path, \n",
    "            self.seg_path, \n",
    "            self.empty_path,\n",
    "            self.ready_path,\n",
    "            self.images_path,\n",
    "            self.label_path\n",
    "        ]\n",
    "        for path in paths:\n",
    "            if create:\n",
    "                path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "class ReadyDatasetPaths:\n",
    "    \"\"\"Manages final dataset paths (images, labels) - DEPRECATED: use DatasetPaths.ready_path instead\"\"\"\n",
    "    def __init__(self, dataset_name: str, base=Path(\"datasets/ready\"), create=True):\n",
    "        self.root = base / dataset_name\n",
    "        self.images = self.root / \"images\"\n",
    "        self.labels = self.root / \"labels\"\n",
    "        if create:\n",
    "            self.root.mkdir(exist_ok=True, parents=True)\n",
    "            self.images.mkdir(exist_ok=True)\n",
    "            self.labels.mkdir(exist_ok=True)\n",
    "\n",
    "# Intermediate outputs for detection+segmentation\n",
    "yahoo_human_balls_dataset = DatasetPaths(\"yahoo_human_balls\", create=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce19aac4",
   "metadata": {},
   "source": [
    "#### Use this one for mass training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2382e564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34, 598)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get image list\n",
    "\n",
    "irl_img_paths = list(IRL_RAW.glob(\"*.jpg\")) + list(IRL_RAW.glob(\"*.jpeg\")) + \\\n",
    "            list(IRL_RAW.glob(\"*.JPG\")) + list(IRL_RAW.glob(\"*.JPEG\"))           \n",
    "        \n",
    "yahoo_balls_img_paths = list(yahoo_balls_raw.glob(\"*.jpg\"))\n",
    "len(irl_img_paths), len(yahoo_balls_img_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc5ea5e",
   "metadata": {},
   "source": [
    "## Step 4: Initialize Models & Test Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71801c0",
   "metadata": {},
   "source": [
    "## Step 4: Process Images (Multi-Class Pipeline)\n",
    "\n",
    "Process each image through the unified detection+segmentation pipeline with multiple text prompts:\n",
    "- **Prompt format**: \"red ball . human\" (separated by ' . ')\n",
    "- **Output structure**:\n",
    "  - `detection/` - All detected objects with colored bboxes\n",
    "  - `segmentation/` - All segmented objects with colored masks\n",
    "  - `labels/{label_name}/` - YOLO polygon format, one subdirectory per label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6014c371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose dataset to process\n",
    "raw_images = yahoo_balls_img_paths\n",
    "dataset_to_use = yahoo_human_balls_dataset\n",
    "\n",
    "text_prompt = \"red ball . human\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ef5bf3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:   0%|          | 0/598 [00:00<?, ?it/s]/home/tonino/projects/ball segmentation/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:1621: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/home/tonino/projects/ball segmentation/.venv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/tonino/projects/ball segmentation/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/tonino/projects/ball segmentation/.venv/lib/python3.12/site-packages/groundingdino/models/GroundingDINO/transformer.py:862: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "Processing images: 100%|██████████| 598/598 [10:28<00:00,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Multi-class segmentation complete!\n",
      "  - Detection visualizations: datasets/preprocessed/yahoo_human_balls/detection\n",
      "  - Segmentation visualizations: datasets/preprocessed/yahoo_human_balls/segmentation\n",
      "  - Ready dataset: datasets/preprocessed/yahoo_human_balls/ready\n",
      "    - Images: datasets/preprocessed/yahoo_human_balls/ready/images\n",
      "    - Labels by class:\n",
      "      - red ball: 456 files\n",
      "      - human: 457 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Process all images with multi-class pipeline\n",
    "for img_path in tqdm(raw_images, desc=\"Processing images\"):\n",
    "    img_pipeline(\n",
    "        img_path,\n",
    "        detect_fn=lambda p: detector.detect(p, text_prompt=text_prompt, return_all_by_label=True),\n",
    "        segment_fn=segmenter.segment_bbox,\n",
    "        det_output_dir=dataset_to_use.det_path,\n",
    "        seg_output_dir=dataset_to_use.seg_path,\n",
    "        txt_output_dir=dataset_to_use.label_path,\n",
    "        empty_dir=dataset_to_use.empty_path,\n",
    "        images_output_dir=dataset_to_use.images_path\n",
    "    )\n",
    "\n",
    "print(\"\\n✓ Multi-class segmentation complete!\")\n",
    "print(f\"  - Detection visualizations: {dataset_to_use.det_path}\")\n",
    "print(f\"  - Segmentation visualizations: {dataset_to_use.seg_path}\")\n",
    "print(f\"  - Ready dataset: {dataset_to_use.ready_path}\")\n",
    "print(f\"    - Images: {dataset_to_use.images_path}\")\n",
    "print(f\"    - Labels by class:\")\n",
    "\n",
    "# Parse prompts to show stats\n",
    "prompts = [p.strip() for p in text_prompt.split('.') if p.strip()]\n",
    "for prompt in prompts:\n",
    "    label_key = prompt.strip().lower()\n",
    "    label_subdir = dataset_to_use.label_path / label_key\n",
    "    if label_subdir.exists():\n",
    "        num_files = len(list(label_subdir.glob(\"*.txt\")))\n",
    "        print(f\"      - {label_key}: {num_files} files\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
