{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb0e6ee9",
   "metadata": {},
   "source": [
    "## Step 1: Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "446eb0fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[12/16/25 19:19:08] </span><span style=\"color: #808000; text-decoration-color: #808000\">WARNING </span> Your inference package version <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.62</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> is out of date! Please upgrade to <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">__init__.py:41</span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         version <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.62</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> of inference for the latest features and bug fixes by    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         running `pip install --upgrade inference`.                              <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[12/16/25 19:19:08]\u001b[0m\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Your inference package version \u001b[1;36m0.62\u001b[0m.\u001b[1;36m4\u001b[0m is out of date! Please upgrade to \u001b[2m__init__.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m41\u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         version \u001b[1;36m0.62\u001b[0m.\u001b[1;36m5\u001b[0m of inference for the latest features and bug fixes by    \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         running `pip install --upgrade inference`.                              \u001b[2m              \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tonino/projects/ball segmentation/.venv/lib/python3.12/site-packages/inference/models/utils.py:378: ModelDependencyMissing: Your `inference` configuration does not support SAM model. Use pip install 'inference[sam]' to install missing requirements.To suppress this warning, set CORE_MODEL_SAM_ENABLED to False.\n",
      "  warnings.warn(\n",
      "/home/tonino/projects/ball segmentation/.venv/lib/python3.12/site-packages/inference/models/utils.py:390: ModelDependencyMissing: Your `inference` configuration does not support SAM2 model. Use pip install 'inference[sam]' to install missing requirements.To suppress this warning, set CORE_MODEL_SAM2_ENABLED to False.\n",
      "  warnings.warn(\n",
      "/home/tonino/projects/ball segmentation/.venv/lib/python3.12/site-packages/inference/models/utils.py:411: ModelDependencyMissing: Your `inference` configuration does not support SAM3 model. Install SAM3 dependencies and set CORE_MODEL_SAM3_ENABLED to True.\n",
      "  warnings.warn(\n",
      "/home/tonino/projects/ball segmentation/.venv/lib/python3.12/site-packages/inference/models/utils.py:450: ModelDependencyMissing: Your `inference` configuration does not support Gaze Detection model. Use pip install 'inference[gaze]' to install missing requirements.To suppress this warning, set CORE_MODEL_GAZE_ENABLED to False.\n",
      "  warnings.warn(\n",
      "/home/tonino/projects/ball segmentation/.venv/lib/python3.12/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.detection import GroundingDINODetector\n",
    "from src.segmentation import FastSAMSegmenter\n",
    "from src.pipeline import img_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3672e5da",
   "metadata": {},
   "source": [
    "## Step 3: Load Models\n",
    "\n",
    "- **Detector**: Grounding DINO (text prompts: \"red ball\" & \"human\")\n",
    "- **Segmenter**: FastSAM (bbox-guided segmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f52222e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Checkpoint found: models/pretrained/groundingdino_swint_ogc.pth\n"
     ]
    }
   ],
   "source": [
    "# Download Grounding DINO weights if needed\n",
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "checkpoint_path = Path(\"models/pretrained/groundingdino_swint_ogc.pth\")\n",
    "if not checkpoint_path.exists():\n",
    "    print(\"Downloading Grounding DINO checkpoint (~693 MB)...\")\n",
    "    checkpoint_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    url = \"https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth\"\n",
    "    response = requests.get(url, stream=True)\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    \n",
    "    with open(checkpoint_path, 'wb') as f:\n",
    "        downloaded = 0\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "            downloaded += len(chunk)\n",
    "            if total_size > 0:\n",
    "                percent = (downloaded / total_size) * 100\n",
    "                print(f\"\\rProgress: {percent:.1f}%\", end='')\n",
    "    print(f\"\\nâœ“ Downloaded to {checkpoint_path}\")\n",
    "else:\n",
    "    print(f\"âœ“ Checkpoint found: {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "beb61f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1203dfc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tonino/projects/ball segmentation/.venv/lib/python3.12/site-packages/torch/functional.py:505: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4317.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n"
     ]
    }
   ],
   "source": [
    "# Grounding DINO detector (unified detector for balls AND persons)\n",
    "# Using lower thresholds to match the online demo behavior\n",
    "from src.segmentation import SAMSegmenter\n",
    "\n",
    "\n",
    "detector = GroundingDINODetector(\n",
    "    model_checkpoint_path=str(checkpoint_path),\n",
    "    box_threshold=0.20,  # Even lower threshold\n",
    "    text_threshold=0.15,  # Even lower threshold\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "segmenter = SAMSegmenter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0aafad",
   "metadata": {},
   "source": [
    "## Step 2: Configure Paths & Device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09097b74",
   "metadata": {},
   "source": [
    "#### Use this one for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db68ce80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input path\n",
    "\n",
    "IRL_RAW = Path(\"datasets/raw/IRL_validation_pictures\")\n",
    "yahoo_balls_raw = Path(\"datasets/cleaned/red_balls_human_yahoo_jpg\")\n",
    "\n",
    "class DatasetPaths:\n",
    "    \"\"\"Manages intermediate preprocessing paths (detection, segmentation, labels)\"\"\"\n",
    "    def __init__(self, dataset_name: str, base=Path(\"datasets/preprocessed\"), create=True):\n",
    "        project = base / dataset_name\n",
    "        if create:\n",
    "            project.mkdir(exist_ok=True, parents=True)\n",
    "        self.det_path = project / \"detection\"\n",
    "        self.seg_path = project / \"segmentation\"\n",
    "        self.label_path = project / \"labels\"\n",
    "\n",
    "class ReadyDatasetPaths:\n",
    "    \"\"\"Manages final dataset paths (images, labels)\"\"\"\n",
    "    def __init__(self, dataset_name: str, base=Path(\"datasets/ready\"), create=True):\n",
    "        self.root = base / dataset_name\n",
    "        if create:\n",
    "            self.root.mkdir(exist_ok=True, parents=True)\n",
    "        self.images = self.root / \"images\"\n",
    "        self.labels = self.root / \"labels\"\n",
    "        \n",
    "        if create:\n",
    "            self.images.mkdir(exist_ok=True)\n",
    "            self.labels.mkdir(exist_ok=True)\n",
    "\n",
    "# Intermediate outputs for detection+segmentation\n",
    "irl_balls_dataset = DatasetPaths(\"irl_balls\")\n",
    "irl_humans_dataset = DatasetPaths(\"irl_persons\")\n",
    "\n",
    "# Final ready dataset\n",
    "irl_ready = ReadyDatasetPaths(\"IRL_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4222ac4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "yahoo_balls_dataset = DatasetPaths(\"red_balls_human_yahoo\")\n",
    "yahoo_balls_ready = ReadyDatasetPaths(\"yahoo_balls_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27c5b38",
   "metadata": {},
   "source": [
    "#### Use this one for mass training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2382e564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34, 598)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get image list\n",
    "\n",
    "irl_img_paths = list(IRL_RAW.glob(\"*.jpg\")) + list(IRL_RAW.glob(\"*.jpeg\")) + \\\n",
    "            list(IRL_RAW.glob(\"*.JPG\")) + list(IRL_RAW.glob(\"*.JPEG\"))           \n",
    "        \n",
    "yahoo_balls_img_paths = list(yahoo_balls_raw.glob(\"*.jpg\"))\n",
    "len(irl_img_paths), len(yahoo_balls_img_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc5ea5e",
   "metadata": {},
   "source": [
    "## Step 4: Initialize Models & Test Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71801c0",
   "metadata": {},
   "source": [
    "## Step 4: Segment Balls & Persons (Grounding DINO â†’ FastSAM â†’ YOLO txt)\n",
    "\n",
    "Process each image through the detection+segmentation pipeline with text prompts:\n",
    "- **Balls**: prompt=\"red ball\"\n",
    "- **Persons**: prompt=\"human\"\n",
    "\n",
    "Output: YOLO polygon format in separate txt folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6014c371",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_images = yahoo_balls_img_paths\n",
    "dataset_to_use = yahoo_balls_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ef5bf3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ball segmentation:   0%|          | 0/598 [00:00<?, ?it/s]/home/tonino/projects/ball segmentation/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:1621: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/home/tonino/projects/ball segmentation/.venv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/tonino/projects/ball segmentation/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/tonino/projects/ball segmentation/.venv/lib/python3.12/site-packages/groundingdino/models/GroundingDINO/transformer.py:862: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "Ball segmentation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 598/598 [23:20<00:00,  2.34s/it]\n"
     ]
    }
   ],
   "source": [
    "for img_path in tqdm(raw_images, desc=\"Ball segmentation\"):\n",
    "    img_pipeline(\n",
    "        img_path,\n",
    "        detect_fn=lambda p: detector.detect(p, text_prompt=\"red ball\"),\n",
    "        segment_fn=segmenter.segment_bbox,\n",
    "        det_output_dir=dataset_to_use.det_path,\n",
    "        seg_output_dir=dataset_to_use.seg_path,\n",
    "        txt_output_dir=dataset_to_use.label_path,\n",
    "        empty_dir=\"empty_detections\",\n",
    "        mode=\"bbox\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8680d069",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for img_path in tqdm(irl_img_paths, desc=\"Person segmentation\"):\n",
    "    img_pipeline(\n",
    "        img_path,\n",
    "        detect_fn=lambda p: detector.detect(p, text_prompt=\"human\"),\n",
    "        segment_fn=segmenter.segment_bbox,\n",
    "        det_output_dir=irl_humans_dataset.det_path,\n",
    "        seg_output_dir=irl_humans_dataset.seg_path,\n",
    "        txt_output_dir=irl_humans_dataset.label_path,\n",
    "        \n",
    "        empty_dir=\"empty_detections\",\n",
    "        mode=\"bbox\"\n",
    "    )\n",
    "    \n",
    "\n",
    "\n",
    "print(\"\\nâœ“ Segmentation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf17493",
   "metadata": {},
   "source": [
    "## Step 5: Combine Ball + Person Masks\n",
    "\n",
    "- Parse ball polygons from txt files (class 0)\n",
    "- Parse person polygons from txt files (class 1)\n",
    "- Combine into PNG masks (ball=0, person=1)\n",
    "- **Ball has priority** over person in overlapping regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a05559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics\n",
    "stats = {\"total\": 0, \"with_ball\": 0, \"with_person\": 0, \"empty\": 0}\n",
    "\n",
    "print(f\"Combining ball + person masks...\")\n",
    "print(f\"Class priority: ball > person\")\n",
    "print()\n",
    "\n",
    "for img_path in tqdm(irl_img_paths, desc=\"Combining masks\"):\n",
    "    stats[\"total\"] += 1\n",
    "    \n",
    "    # Load image to get dimensions\n",
    "    img = Image.open(img_path)\n",
    "    h, w = img.height, img.width\n",
    "    \n",
    "    # Initialize combined mask (all background)\n",
    "    combined_mask = np.zeros((h, w), dtype=np.uint8)\n",
    "    has_detections = False\n",
    "    \n",
    "    # --- 1. Parse ball segmentation from txt (if exists) ---\n",
    "    ball_txt_path = irl_balls_dataset.label_path / (img_path.stem + '.txt')\n",
    "    if ball_txt_path.exists():\n",
    "        with open(ball_txt_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            \n",
    "        for line in lines:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) < 7:  # Need at least class_id + 3 points (6 coords)\n",
    "                continue\n",
    "            \n",
    "            # Extract normalized coordinates\n",
    "            coords = [float(p) for p in parts[1:]]\n",
    "            \n",
    "            # Convert to pixel coordinates\n",
    "            points = []\n",
    "            for i in range(0, len(coords), 2):\n",
    "                x = int(coords[i] * w)\n",
    "                y = int(coords[i+1] * h)\n",
    "                points.append([x, y])\n",
    "            \n",
    "            # Fill polygon with ball class (0)\n",
    "            points_array = np.array(points, dtype=np.int32)\n",
    "            cv2.fillPoly(combined_mask, [points_array], 0)\n",
    "            has_detections = True\n",
    "        \n",
    "        stats[\"with_ball\"] += 1\n",
    "    \n",
    "    # --- 2. Parse person segmentation from txt (if exists) ---\n",
    "    person_txt_path = irl_humans_dataset.label_path / (img_path.stem + '.txt')\n",
    "    if person_txt_path.exists():\n",
    "        with open(person_txt_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            \n",
    "        for line in lines:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) < 7:\n",
    "                continue\n",
    "            \n",
    "            # Extract normalized coordinates\n",
    "            coords = [float(p) for p in parts[1:]]\n",
    "            \n",
    "            # Convert to pixel coordinates\n",
    "            points = []\n",
    "            for i in range(0, len(coords), 2):\n",
    "                x = int(coords[i] * w)\n",
    "                y = int(coords[i+1] * h)\n",
    "                points.append([x, y])\n",
    "            \n",
    "            # Fill polygon with person class (1) ONLY where background\n",
    "            # This ensures ball priority\n",
    "            temp_mask = np.zeros((h, w), dtype=np.uint8)\n",
    "            points_array = np.array(points, dtype=np.int32)\n",
    "            cv2.fillPoly(temp_mask, [points_array], 1)\n",
    "            \n",
    "            person_area = (combined_mask == 0) & (temp_mask == 1)\n",
    "            combined_mask[person_area] = 1\n",
    "            has_detections = True\n",
    "        \n",
    "        stats[\"with_person\"] += 1\n",
    "    \n",
    "    # Track empty images\n",
    "    if not has_detections:\n",
    "        stats[\"empty\"] += 1\n",
    "    \n",
    "    # Save mask (even if empty)\n",
    "    mask_img = Image.fromarray(combined_mask, mode='L')\n",
    "    mask_img.save(irl_ready.labels / (img_path.stem + '.png'))\n",
    "    \n",
    "    # Copy original image\n",
    "    shutil.copy(img_path, irl_ready.images / img_path.name)\n",
    "\n",
    "print(\"\\nâœ“ Processing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a694291",
   "metadata": {},
   "source": [
    "## Step 6: Display Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd7bdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"ðŸ“Š DATASET STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total images processed:    {stats['total']}\")\n",
    "print(f\"Images with ball(s):       {stats['with_ball']} ({stats['with_ball']/stats['total']*100:.1f}%)\")\n",
    "print(f\"Images with person(s):     {stats['with_person']} ({stats['with_person']/stats['total']*100:.1f}%)\")\n",
    "print(f\"Images with no detections: {stats['empty']} ({stats['empty']/stats['total']*100:.1f}%)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Verify dataset consistency\n",
    "num_images = len(list(irl_ready.images.glob(\"*\")))\n",
    "num_labels = len(list(irl_ready.labels.glob(\"*.png\")))\n",
    "\n",
    "print(f\"\\nâœ“ Dataset consistency check:\")\n",
    "print(f\"  Images: {num_images}\")\n",
    "print(f\"  Labels: {num_labels}\")\n",
    "print(f\"  Match: {'âœ“ YES' if num_images == num_labels else 'âœ— NO'}\")\n",
    "\n",
    "print(f\"\\nâœ“ Dataset ready at: {irl_ready.root}\")\n",
    "print(f\"  - images/  ({num_images} files)\")\n",
    "print(f\"  - labels/  ({num_labels} .png masks)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
