{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5c885d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "from ultralytics.models import YOLO\n",
    "import torch\n",
    "import yaml\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17b959d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import DisplayPath\n",
    "Path = DisplayPath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a0d47a",
   "metadata": {},
   "source": [
    "## Step 2: Configure Paths & Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a5bae1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "[datasets/ready/full_dataset](datasets/ready/full_dataset)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "[datasets/ready/full_dataset/train](datasets/ready/full_dataset/train)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Val:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "[datasets/ready/full_dataset/val](datasets/ready/full_dataset/val)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Test:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "[datasets/ready/full_dataset/test](datasets/ready/full_dataset/test)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Dataset path (created by e2e_data_prep.ipynb)\n",
    "YOLO_DATASET = Path(\"datasets/ready/full_dataset\")\n",
    "RUNS_DIR = Path(\"runs/segment\")\n",
    "\n",
    "# Verify dataset exists\n",
    "if not YOLO_DATASET.exists():\n",
    "    raise FileNotFoundError(f\"Dataset not found at {YOLO_DATASET}. Run e2e_data_prep.ipynb first!\")\n",
    "\n",
    "print(\"Dataset:\")\n",
    "YOLO_DATASET.display()\n",
    "print(\"  Train:\")\n",
    "(YOLO_DATASET / 'train').display()\n",
    "print(\"  Val:\")\n",
    "(YOLO_DATASET / 'val').display()\n",
    "print(\"  Test:\")\n",
    "(YOLO_DATASET / 'test').display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11dc79bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n",
      "GPU: NVIDIA GeForce RTX 3080 Laptop GPU\n",
      "CUDA: 12.8\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "BATCH_SIZE = 16\n",
    "IMG_SIZE = 640\n",
    "model_type = \"yolo11n-seg.pt\"\n",
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8976809f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Configuration d'augmentation R√âDUITE pour le training YOLO\n",
    "# # Puisqu'on pr√©-augmente massivement les trashcans, on r√©duit l'augmentation\n",
    "# # globale pour √©viter de sur-augmenter les red balls et humans\n",
    "# AUG_CONFIG = {\n",
    "#     'hsv_h': 0.010,  # Hue augmentation (r√©duit de 0.015)\n",
    "#     'hsv_s': 0.5,    # Saturation (r√©duit de 0.7)\n",
    "#     'hsv_v': 0.3,    # Value (r√©duit de 0.4)\n",
    "#     'degrees': 5.0,   # Rotation (r√©duit de 10.0)\n",
    "#     'translate': 0.05, # Translation (r√©duit de 0.1)\n",
    "#     'scale': 0.3,     # Scaling (r√©duit de 0.5)\n",
    "#     'shear': 0.0,     # Shearing\n",
    "#     'perspective': 0.0, # Perspective\n",
    "#     'flipud': 0.0,    # Vertical flip\n",
    "#     'fliplr': 0.5,    # Horizontal flip (maintenu)\n",
    "#     'mosaic': 0.5,    # Mosaic augmentation (r√©duit de 1.0)\n",
    "#     'mixup': 0.0,     # Mixup augmentation\n",
    "#     'copy_paste': 0.3, # üÜï Copy-paste aug pour classes rares\n",
    "# }\n",
    "\n",
    "# print(\"Augmentation globale R√âDUITE pour √©viter la sur-augmentation\")\n",
    "# print(\"   Les trashcans sont pr√©-augment√©es massivement avant le training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8e652f",
   "metadata": {},
   "source": [
    "## Step 3: Verify Dataset Structure\n",
    "\n",
    "Dataset is already prepared by e2e_data_prep.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee0e39b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATASET VERIFICATION\n",
      "============================================================\n",
      "TRAIN: 3950 images, 3950 labels\n",
      "VAL  :   47 images,   47 labels\n",
      "TEST :  214 images,  214 labels\n",
      "TOTAL: 4211 images, 4211 labels\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Verify dataset structure\n",
    "print(\"=\"*60)\n",
    "print(\"DATASET VERIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "splits = ['train', 'val', 'test']\n",
    "stats = {}\n",
    "\n",
    "for split in splits:\n",
    "    img_dir = YOLO_DATASET / split / \"images\"\n",
    "    lbl_dir = YOLO_DATASET / split / \"labels\"\n",
    "    \n",
    "    if img_dir.exists() and lbl_dir.exists():\n",
    "        num_images = len(list(img_dir.glob(\"*\")))\n",
    "        num_labels = len(list(lbl_dir.glob(\"*.txt\")))\n",
    "        stats[split] = {'images': num_images, 'labels': num_labels}\n",
    "        print(f\"{split.upper():5s}: {num_images:4d} images, {num_labels:4d} labels\")\n",
    "    else:\n",
    "        stats[split] = {'images': 0, 'labels': 0}\n",
    "        print(f\"{split.upper():5s}: Missing!\")\n",
    "\n",
    "total_images = sum(s['images'] for s in stats.values())\n",
    "total_labels = sum(s['labels'] for s in stats.values())\n",
    "\n",
    "print(f\"{'TOTAL':5s}: {total_images:4d} images, {total_labels:4d} labels\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if total_images == 0:\n",
    "    raise RuntimeError(\"No dataset found! Run e2e_data_prep.ipynb to create the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e32ee53",
   "metadata": {},
   "source": [
    "## Step 3.5: Analyze Class Distribution\n",
    "\n",
    "Check the distribution of classes in the training set to identify imbalances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2371c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CLASS DISTRIBUTION ANALYSIS\n",
      "============================================================\n",
      "\n",
      "TRAIN:\n",
      "  Red Ball     (class 0): 11288 instances ( 85.9%)\n",
      "  Human        (class 1):  1008 instances (  7.7%)\n",
      "  Trashcan     (class 2):   840 instances (  6.4%)\n",
      "  TOTAL                  : 13136 instances\n",
      "  Imbalance ratio: 13.4x\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Analyze class distribution in training set\n",
    "from src.data_utils import count_class_instances\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CLASS DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "class_names = {0: 'Red Ball', 1: 'Human', 2: 'Trashcan'}\n",
    "\n",
    "for split in ['train']:\n",
    "    counts = count_class_instances(YOLO_DATASET, split)\n",
    "    total = sum(counts.values())\n",
    "\n",
    "    print(f\"\\n{split.upper()}:\")\n",
    "    for class_id, count in counts.items():\n",
    "        percentage = (count / total * 100) if total > 0 else 0\n",
    "        print(f\"  {class_names[class_id]:12s} (class {class_id}): {count:5d} instances ({percentage:5.1f}%)\")\n",
    "    print(f\"  {'TOTAL':12s}           : {total:5d} instances\")\n",
    "    \n",
    "    # Calculate imbalance ratio\n",
    "    if counts[2] > 0:  # If trashcans exist\n",
    "        max_count = max(counts.values())\n",
    "        min_count = min(v for v in counts.values() if v > 0)\n",
    "        imbalance_ratio = max_count / min_count\n",
    "        print(f\"  Imbalance ratio: {imbalance_ratio:.1f}x\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5c5dd18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2346/3285217329.py:12: UserWarning: Argument(s) 'var_limit' are not valid for transform GaussNoise\n",
      "  A.GaussNoise(var_limit=(10.0, 50.0), p=0.5),\n"
     ]
    }
   ],
   "source": [
    "# Import augmentation utilities\n",
    "import albumentations as A\n",
    "from src.augmentation import augment_class_dataset\n",
    "\n",
    "# Define custom augmentation pipelines\n",
    "trashcan_pipeline = A.Compose([  # type: ignore\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.3),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.8),\n",
    "    A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.8),\n",
    "    A.GaussNoise(var_limit=(10.0, 50.0), p=0.5),\n",
    "    A.RandomScale(scale_limit=0.3, p=0.7),\n",
    "    A.Affine(rotate=(-20, 20), translate_percent=0.1, scale=(0.8, 1.2), shear=(-10, 10), p=0.7),\n",
    "], keypoint_params=A.KeypointParams(format='xy', remove_invisible=False))\n",
    "\n",
    "# Execute augmentation\n",
    "DO_AUGMENTATION = False \n",
    "if DO_AUGMENTATION:\n",
    "    # Augment Trashcans (Class 2) - Strong augmentation\n",
    "    print(\"Augmenting Trashcans...\")\n",
    "    augment_class_dataset(\n",
    "        dataset_path=YOLO_DATASET,\n",
    "        class_id=2,\n",
    "        num_augmentations=20,\n",
    "        aug_config=trashcan_pipeline\n",
    "    )\n",
    "    \n",
    "    # Example: Augment Balls (Class 0) - Light augmentation \n",
    "    augment_class_dataset(\n",
    "        dataset_path=YOLO_DATASET,\n",
    "        class_id=0,\n",
    "        num_augmentations=3,\n",
    "        aug_config='light'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbed63c",
   "metadata": {},
   "source": [
    "### üîÑ Reset: Clean Augmented Images\n",
    "\n",
    "If you accidentally ran augmentation multiple times, use this to remove all augmented copies and start fresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f63aac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set CLEAN_AUGMENTED = True to remove all augmented images\n"
     ]
    }
   ],
   "source": [
    "from src.augmentation import clean_augmented_images\n",
    "\n",
    "# Execute cleanup (set to True to clean)\n",
    "CLEAN_AUGMENTED = False\n",
    "\n",
    "if CLEAN_AUGMENTED:\n",
    "    removed = clean_augmented_images(YOLO_DATASET)\n",
    "    print(f\"\\nüí° Tip: Re-run the class distribution analysis to see updated statistics!\")\n",
    "else:\n",
    "    print(\"Set CLEAN_AUGMENTED = True to remove all augmented images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b979e79",
   "metadata": {},
   "source": [
    "## Step 4: Create YOLO Configuration File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "739a4bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Configuration saved: datasets/ready/full_dataset/data.yaml\n",
      "Dataset structure:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "[datasets/ready/full_dataset](datasets/ready/full_dataset)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "[datasets/ready/full_dataset/train](datasets/ready/full_dataset/train)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Val:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "[datasets/ready/full_dataset/val](datasets/ready/full_dataset/val)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Test:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "[datasets/ready/full_dataset/test](datasets/ready/full_dataset/test)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classes = {\n",
    "    'red ball': 0,\n",
    "    'human': 1,\n",
    "    'trashcan': 2\n",
    "}\n",
    "\n",
    "config = {\n",
    "    'path': str(YOLO_DATASET.absolute()),\n",
    "    'train': 'train/images',\n",
    "    'val': 'val/images',\n",
    "    'nc': len(classes),\n",
    "    'names': list(classes.keys())\n",
    "}\n",
    "\n",
    "config_path = YOLO_DATASET / 'data.yaml'\n",
    "with open(config_path, 'w') as f:\n",
    "    yaml.dump(config, f, default_flow_style=False)\n",
    "\n",
    "print(f\"‚úì Configuration saved: {config_path}\")\n",
    "print(\"Dataset structure:\")\n",
    "YOLO_DATASET.display()\n",
    "print(\"  Train:\")\n",
    "(YOLO_DATASET / 'train').display()\n",
    "print(\"  Val:\")\n",
    "(YOLO_DATASET / 'val').display()\n",
    "print(\"  Test:\")\n",
    "(YOLO_DATASET / 'test').display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db1af91",
   "metadata": {},
   "source": [
    "## Step 4.5: Select Monitoring Images\n",
    "\n",
    "Select diverse validation images covering all classes to monitor training progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8f255d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SELECTING DIVERSE MONITORING IMAGES\n",
      "============================================================\n",
      "\n",
      "Searching for Red Ball images...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úì Found 3 images with Red Ball\n",
      "    - a6b631525ff8dc9bc26bb53e97481606.jpg\n",
      "    - d634b2ebe37b5735c16949926ae2d7bb.jpg\n",
      "    - 088610ac49bfde8470097a40c8b749d7.jpg\n",
      "\n",
      "Searching for Human images...\n",
      "  ‚úì Found 3 images with Human\n",
      "    - a6b631525ff8dc9bc26bb53e97481606.jpg\n",
      "    - d634b2ebe37b5735c16949926ae2d7bb.jpg\n",
      "    - 088610ac49bfde8470097a40c8b749d7.jpg\n",
      "\n",
      "Searching for Trashcan images...\n",
      "  ‚úì Found 3 images with Trashcan\n",
      "    - de96513c1cdccc864e7b7e809162d06c.jpg\n",
      "    - 1383c4a58a26c7238fbae31ec8e4e660.jpg\n",
      "    - f7c36eabf5a95cf548a382f4a6b49050.jpg\n",
      "\n",
      "Searching for mixed-class images...\n",
      "  ‚úì Mixed: b2dcc2d8cc39ca13b5cf2e24bf036d62.jpg (Red Ball, Human)\n",
      "  ‚úì Mixed: 9666760cde1bedabf437f3dbfc95f891.jpg (Red Ball, Human)\n",
      "  ‚úì Mixed: d8ce4c69557862b45c0f7c18d0d3a412.jpg (Red Ball, Human)\n",
      "\n",
      "============================================================\n",
      "Selected 12 images for monitoring\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src.data_utils import select_diverse_monitoring_images\n",
    "\n",
    "# Select diverse monitoring images\n",
    "val_labels_dir = YOLO_DATASET / 'val' / 'labels'\n",
    "val_images_dir = YOLO_DATASET / 'val' / 'images'\n",
    "\n",
    "MONITOR_IMAGES = select_diverse_monitoring_images(\n",
    "    val_labels_dir, \n",
    "    val_images_dir, \n",
    "    images_per_class=3,\n",
    "    include_mixed=True\n",
    ")\n",
    "\n",
    "if len(MONITOR_IMAGES) == 0:\n",
    "    print(\"‚ö†Ô∏è  Warning: No monitoring images found in validation set!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0cd691",
   "metadata": {},
   "source": [
    "## Step 4.6: Define Multi-Class Monitoring Callback\n",
    "\n",
    "Create a callback that visualizes segmentation progress for all classes at each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9e4bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.visualization import create_monitoring_callback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853894a7",
   "metadata": {},
   "source": [
    "## Step 5: Train Model with Multi-Class Monitoring\n",
    "\n",
    "Train YOLOv11 with:\n",
    "- Data augmentation on train set (reduced for abundant classes)\n",
    "- Trashcan pre-augmentation for class balance\n",
    "- Checkpoints saved for best model\n",
    "- Validation after each epoch\n",
    "- **Custom callback to monitor all classes segmentation progress**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f0c5a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained model\n",
    "model = YOLO(model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a4f6fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = 'ball_person_trashcan_model_v5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd3f23d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-class monitoring output: runs/segment/ball_person_trashcan_model_v4/training_monitor\n",
      "‚úì Multi-class monitoring callback registered\n",
      "   Monitoring 12 images covering all classes\n"
     ]
    }
   ],
   "source": [
    "# Setup multi-class monitoring\n",
    "monitor_output_dir = RUNS_DIR / project_name / 'training_monitor'\n",
    "print(f\"Multi-class monitoring output: {monitor_output_dir}\")\n",
    "\n",
    "# Add callback\n",
    "callback_fn = create_monitoring_callback(\n",
    "    model=model,\n",
    "    monitor_images=MONITOR_IMAGES,\n",
    "    output_dir=monitor_output_dir,\n",
    "    project_name=project_name\n",
    ")\n",
    "\n",
    "model.add_callback('on_train_epoch_end', callback_fn)\n",
    "print(\"‚úì Multi-class monitoring callback registered\")\n",
    "print(f\"   Monitoring {len(MONITOR_IMAGES)} images covering all classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13a8ad51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.235 üöÄ Python-3.12.10 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 3080 Laptop GPU, 8192MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=1.0, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=datasets/ready/full_dataset/data.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22], half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.001, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11n-seg.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=ball_person_trashcan_model_v4, nbs=64, nms=False, opset=None, optimize=False, optimizer=Adam, overlap_mask=True, patience=20, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs/segment, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/home/tonino/projects/ball segmentation/runs/segment/ball_person_trashcan_model_v4, save_frames=False, save_json=False, save_period=5, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=segment, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    684025  ultralytics.nn.modules.head.Segment          [3, 32, 64, [64, 128, 256]]   \n",
      "YOLO11n-seg summary: 203 layers, 2,843,193 parameters, 2,843,177 gradients, 9.7 GFLOPs\n",
      "\n",
      "Transferred 510/561 items from pretrained weights\n",
      "Freezing layer 'model.0.conv.weight'\n",
      "Freezing layer 'model.0.bn.weight'\n",
      "Freezing layer 'model.0.bn.bias'\n",
      "Freezing layer 'model.1.conv.weight'\n",
      "Freezing layer 'model.1.bn.weight'\n",
      "Freezing layer 'model.1.bn.bias'\n",
      "Freezing layer 'model.2.cv1.conv.weight'\n",
      "Freezing layer 'model.2.cv1.bn.weight'\n",
      "Freezing layer 'model.2.cv1.bn.bias'\n",
      "Freezing layer 'model.2.cv2.conv.weight'\n",
      "Freezing layer 'model.2.cv2.bn.weight'\n",
      "Freezing layer 'model.2.cv2.bn.bias'\n",
      "Freezing layer 'model.2.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.2.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.2.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.2.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.2.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.2.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.3.conv.weight'\n",
      "Freezing layer 'model.3.bn.weight'\n",
      "Freezing layer 'model.3.bn.bias'\n",
      "Freezing layer 'model.4.cv1.conv.weight'\n",
      "Freezing layer 'model.4.cv1.bn.weight'\n",
      "Freezing layer 'model.4.cv1.bn.bias'\n",
      "Freezing layer 'model.4.cv2.conv.weight'\n",
      "Freezing layer 'model.4.cv2.bn.weight'\n",
      "Freezing layer 'model.4.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.5.conv.weight'\n",
      "Freezing layer 'model.5.bn.weight'\n",
      "Freezing layer 'model.5.bn.bias'\n",
      "Freezing layer 'model.6.cv1.conv.weight'\n",
      "Freezing layer 'model.6.cv1.bn.weight'\n",
      "Freezing layer 'model.6.cv1.bn.bias'\n",
      "Freezing layer 'model.6.cv2.conv.weight'\n",
      "Freezing layer 'model.6.cv2.bn.weight'\n",
      "Freezing layer 'model.6.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.0.cv3.conv.weight'\n",
      "Freezing layer 'model.6.m.0.cv3.bn.weight'\n",
      "Freezing layer 'model.6.m.0.cv3.bn.bias'\n",
      "Freezing layer 'model.6.m.0.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.0.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.0.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.0.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.0.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.0.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.0.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.0.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.0.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.0.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.0.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.0.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.7.conv.weight'\n",
      "Freezing layer 'model.7.bn.weight'\n",
      "Freezing layer 'model.7.bn.bias'\n",
      "Freezing layer 'model.8.cv1.conv.weight'\n",
      "Freezing layer 'model.8.cv1.bn.weight'\n",
      "Freezing layer 'model.8.cv1.bn.bias'\n",
      "Freezing layer 'model.8.cv2.conv.weight'\n",
      "Freezing layer 'model.8.cv2.bn.weight'\n",
      "Freezing layer 'model.8.cv2.bn.bias'\n",
      "Freezing layer 'model.8.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.8.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.8.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.8.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.8.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.8.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.8.m.0.cv3.conv.weight'\n",
      "Freezing layer 'model.8.m.0.cv3.bn.weight'\n",
      "Freezing layer 'model.8.m.0.cv3.bn.bias'\n",
      "Freezing layer 'model.8.m.0.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.8.m.0.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.8.m.0.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.8.m.0.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.8.m.0.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.8.m.0.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.8.m.0.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.8.m.0.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.8.m.0.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.8.m.0.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.8.m.0.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.8.m.0.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.9.cv1.conv.weight'\n",
      "Freezing layer 'model.9.cv1.bn.weight'\n",
      "Freezing layer 'model.9.cv1.bn.bias'\n",
      "Freezing layer 'model.9.cv2.conv.weight'\n",
      "Freezing layer 'model.9.cv2.bn.weight'\n",
      "Freezing layer 'model.9.cv2.bn.bias'\n",
      "Freezing layer 'model.10.cv1.conv.weight'\n",
      "Freezing layer 'model.10.cv1.bn.weight'\n",
      "Freezing layer 'model.10.cv1.bn.bias'\n",
      "Freezing layer 'model.10.cv2.conv.weight'\n",
      "Freezing layer 'model.10.cv2.bn.weight'\n",
      "Freezing layer 'model.10.cv2.bn.bias'\n",
      "Freezing layer 'model.10.m.0.attn.qkv.conv.weight'\n",
      "Freezing layer 'model.10.m.0.attn.qkv.bn.weight'\n",
      "Freezing layer 'model.10.m.0.attn.qkv.bn.bias'\n",
      "Freezing layer 'model.10.m.0.attn.proj.conv.weight'\n",
      "Freezing layer 'model.10.m.0.attn.proj.bn.weight'\n",
      "Freezing layer 'model.10.m.0.attn.proj.bn.bias'\n",
      "Freezing layer 'model.10.m.0.attn.pe.conv.weight'\n",
      "Freezing layer 'model.10.m.0.attn.pe.bn.weight'\n",
      "Freezing layer 'model.10.m.0.attn.pe.bn.bias'\n",
      "Freezing layer 'model.10.m.0.ffn.0.conv.weight'\n",
      "Freezing layer 'model.10.m.0.ffn.0.bn.weight'\n",
      "Freezing layer 'model.10.m.0.ffn.0.bn.bias'\n",
      "Freezing layer 'model.10.m.0.ffn.1.conv.weight'\n",
      "Freezing layer 'model.10.m.0.ffn.1.bn.weight'\n",
      "Freezing layer 'model.10.m.0.ffn.1.bn.bias'\n",
      "Freezing layer 'model.13.cv1.conv.weight'\n",
      "Freezing layer 'model.13.cv1.bn.weight'\n",
      "Freezing layer 'model.13.cv1.bn.bias'\n",
      "Freezing layer 'model.13.cv2.conv.weight'\n",
      "Freezing layer 'model.13.cv2.bn.weight'\n",
      "Freezing layer 'model.13.cv2.bn.bias'\n",
      "Freezing layer 'model.13.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.13.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.13.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.13.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.13.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.13.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.16.cv1.conv.weight'\n",
      "Freezing layer 'model.16.cv1.bn.weight'\n",
      "Freezing layer 'model.16.cv1.bn.bias'\n",
      "Freezing layer 'model.16.cv2.conv.weight'\n",
      "Freezing layer 'model.16.cv2.bn.weight'\n",
      "Freezing layer 'model.16.cv2.bn.bias'\n",
      "Freezing layer 'model.16.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.16.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.16.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.16.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.16.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.16.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.17.conv.weight'\n",
      "Freezing layer 'model.17.bn.weight'\n",
      "Freezing layer 'model.17.bn.bias'\n",
      "Freezing layer 'model.19.cv1.conv.weight'\n",
      "Freezing layer 'model.19.cv1.bn.weight'\n",
      "Freezing layer 'model.19.cv1.bn.bias'\n",
      "Freezing layer 'model.19.cv2.conv.weight'\n",
      "Freezing layer 'model.19.cv2.bn.weight'\n",
      "Freezing layer 'model.19.cv2.bn.bias'\n",
      "Freezing layer 'model.19.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.19.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.19.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.19.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.19.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.19.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.20.conv.weight'\n",
      "Freezing layer 'model.20.bn.weight'\n",
      "Freezing layer 'model.20.bn.bias'\n",
      "Freezing layer 'model.22.cv1.conv.weight'\n",
      "Freezing layer 'model.22.cv1.bn.weight'\n",
      "Freezing layer 'model.22.cv1.bn.bias'\n",
      "Freezing layer 'model.22.cv2.conv.weight'\n",
      "Freezing layer 'model.22.cv2.bn.weight'\n",
      "Freezing layer 'model.22.cv2.bn.bias'\n",
      "Freezing layer 'model.22.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.22.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.22.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.22.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.22.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.22.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.22.m.0.cv3.conv.weight'\n",
      "Freezing layer 'model.22.m.0.cv3.bn.weight'\n",
      "Freezing layer 'model.22.m.0.cv3.bn.bias'\n",
      "Freezing layer 'model.22.m.0.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.22.m.0.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.22.m.0.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.22.m.0.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.22.m.0.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.22.m.0.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.22.m.0.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.22.m.0.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.22.m.0.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.22.m.0.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.22.m.0.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.22.m.0.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 106.7¬±171.1 MB/s, size: 94.3 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/labels.cache... 3946 images, 3 backgrounds, 4 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3950/3950 3.7Mit/s 0.0s0s\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/0db65afcb80a13863a3a8dfc24e7d73a.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/1731534619a61f5dc5f63c0414915afb.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/1731534619a61f5dc5f63c0414915afb_aug_c0_00_a1e58cdb.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/1c04a053b46c5e78a4a93e76cc80d233.jpg: 5 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/1c04a053b46c5e78a4a93e76cc80d233_aug_c0_00_4cbff56f.jpg: 4 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/1c04a053b46c5e78a4a93e76cc80d233_aug_c0_01_4a762ce6.jpg: 6 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/1c04a053b46c5e78a4a93e76cc80d233_aug_c0_02_9911cbc1.jpg: 5 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/1e82f7995240b5e6e1fc33e649219375.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/1e82f7995240b5e6e1fc33e649219375_aug_c0_00_0cdded39.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/1e82f7995240b5e6e1fc33e649219375_aug_c0_01_95de8e0b.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/1e82f7995240b5e6e1fc33e649219375_aug_c0_02_0e195374.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/20b90386dd79ffcfbd94b9868ac62cc5.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/20b90386dd79ffcfbd94b9868ac62cc5_aug_c0_02_d8842872.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/2db66d2c671556fda7813b34ef672ee3.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/31aab9bb7708794e4777bf399873f529.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/31aab9bb7708794e4777bf399873f529_aug_c0_01_da39276b.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/31aab9bb7708794e4777bf399873f529_aug_c0_02_8af6df78.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/432990e6d8d154c8c60fb4e0570b452a.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/432990e6d8d154c8c60fb4e0570b452a_aug_c0_02_fd637e00.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/491e62d4bb9e70806db0a13b6abe99ed.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/491e62d4bb9e70806db0a13b6abe99ed_aug_c0_00_45a5e3ef.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/491e62d4bb9e70806db0a13b6abe99ed_aug_c0_01_c7fd2f8b.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/491e62d4bb9e70806db0a13b6abe99ed_aug_c0_02_f37d1a88.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/4bd48ebb54d4583e27c3bd77f15a5c33.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/571d71a897f478abf9fbd6a4bb667ded.jpg: ignoring corrupt image/label: image size (9, 16) <10 pixels\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/571d71a897f478abf9fbd6a4bb667ded_aug_c0_00_97b92572.jpg: ignoring corrupt image/label: image size (16, 9) <10 pixels\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/571d71a897f478abf9fbd6a4bb667ded_aug_c0_01_5e287787.jpg: ignoring corrupt image/label: image size (9, 16) <10 pixels\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/571d71a897f478abf9fbd6a4bb667ded_aug_c0_02_e629c270.jpg: ignoring corrupt image/label: image size (9, 16) <10 pixels\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/5e584bc79700ada6170545cdd5f0628c.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/5e584bc79700ada6170545cdd5f0628c_aug_c0_01_8cdeec97.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/680f9964dbac0c69076e3ee5d447a0dc_aug_c0_00_1ff104d1.jpg: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/69a7186fc55e1777a8c5a6eec975f34b_aug_c0_00_b9949d15.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/69a7186fc55e1777a8c5a6eec975f34b_aug_c0_01_91ef4251.jpg: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/69a7186fc55e1777a8c5a6eec975f34b_aug_c0_02_111a9e1e.jpg: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/77449fbf91a6c8f66fc6d24a6d55cb91.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/77449fbf91a6c8f66fc6d24a6d55cb91_aug_c0_00_f8106185.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/77449fbf91a6c8f66fc6d24a6d55cb91_aug_c0_01_90b07184.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/77449fbf91a6c8f66fc6d24a6d55cb91_aug_c0_02_16b02106.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/7bc696523e818d843abe7c38ff7d9588.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/7bc696523e818d843abe7c38ff7d9588_aug_c0_02_4f79bbc5.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/7d08dd1bd72cb79b59b73d90d1487a71.jpg: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/7d08dd1bd72cb79b59b73d90d1487a71_aug_c0_00_98054fb0.jpg: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/7d08dd1bd72cb79b59b73d90d1487a71_aug_c0_01_351177b4.jpg: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/7d08dd1bd72cb79b59b73d90d1487a71_aug_c0_02_db11b38e.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/7ee3ed09be312fac168b140fdca3bc75.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/7ee3ed09be312fac168b140fdca3bc75_aug_c0_00_783ae24a.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/8386b8b7f9b1b19d822eebcb7431edef.jpg: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/8386b8b7f9b1b19d822eebcb7431edef_aug_c0_00_f386cf1a.jpg: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/8386b8b7f9b1b19d822eebcb7431edef_aug_c0_01_3091686d.jpg: 2 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/8386b8b7f9b1b19d822eebcb7431edef_aug_c0_02_ad4252c1.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/87fbc3315fe5abc3142de03b5bb96af0.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/87fbc3315fe5abc3142de03b5bb96af0_aug_c0_01_ee82be03.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/8a0dab3a8810f7aac82708d97d17dcea.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/8b8e121636bf2158a7052e0a297e36a1.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/8b8e121636bf2158a7052e0a297e36a1_aug_c0_00_7e4fe9a1.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/8b8e121636bf2158a7052e0a297e36a1_aug_c0_01_de38e537.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/b89006dfbf0861ad3cf9f5161bb1b16f.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/b89006dfbf0861ad3cf9f5161bb1b16f_aug_c0_01_323c7b08.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/d1727fe5095db7c4de7be38adc6a0fc6_aug_c0_01_4b2978c7.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/dfe44d08ed9d881f01ed3d9acd374772.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/dfe44d08ed9d881f01ed3d9acd374772_aug_c0_02_fe09608c.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/e74a8de4f129a1d33c6dc3320d04d8d6.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/e74a8de4f129a1d33c6dc3320d04d8d6_aug_c0_00_631ca79f.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/eac3d40199892716e311067c141ec999.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/eac3d40199892716e311067c141ec999_aug_c0_01_02875bda.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/tonino/projects/ball segmentation/datasets/ready/full_dataset/train/images/eac3d40199892716e311067c141ec999_aug_c0_02_530efd41.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 477.3¬±78.8 MB/s, size: 2921.1 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/tonino/projects/ball segmentation/datasets/ready/full_dataset/val/labels.cache... 47 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 47/47 58.7Kit/s 0.0s\n",
      "Plotting labels to /home/tonino/projects/ball segmentation/runs/segment/ball_person_trashcan_model_v4/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.001, momentum=0.937) with parameter groups 90 weight(decay=0.0), 101 weight(decay=0.0005), 100 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1m/home/tonino/projects/ball segmentation/runs/segment/ball_person_trashcan_model_v4\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       1/50      1.71G      1.155      3.565      4.014       1.45         54        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 247/247 1.3it/s 3:12<0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2/2 2.6s/it 5.2s8.2s\n",
      "                   all         47         81      0.496      0.581      0.637      0.591      0.496      0.581      0.633      0.411\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       2/50      1.88G      1.151      3.372      3.311      1.438         87        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 247/247 5.4it/s 46.0s<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2/2 2.6it/s 0.8s1.8s\n",
      "                   all         47         81      0.569      0.727      0.715       0.67       0.49      0.542      0.568      0.412\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       3/50      1.88G      1.135      3.344      3.124       1.42         66        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 247/247 5.5it/s 44.6s<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2/2 3.7it/s 0.5s1.1s\n",
      "                   all         47         81      0.554      0.672      0.726      0.674      0.557      0.388        0.4      0.199\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       4/50      1.89G      1.119        3.3      3.066      1.415         49        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 247/247 5.7it/s 43.2s<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2/2 3.5it/s 0.6s1.2s\n",
      "                   all         47         81      0.459      0.575      0.588       0.55      0.454      0.566      0.581      0.459\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       5/50       1.9G      1.099      3.279      3.006      1.401         42        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 247/247 5.6it/s 44.4s<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2/2 3.4it/s 0.6s1.2s\n",
      "                   all         47         81      0.676      0.646      0.722      0.672      0.676      0.646      0.722      0.584\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       6/50       1.9G      1.089      3.261      2.973        1.4         63        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 247/247 5.2it/s 47.5s<0.2s\n",
      "\n",
      "============================================================\n",
      "Generating monitoring visualizations for epoch 5...\n",
      "============================================================\n",
      "‚úì Saved monitoring visualization: epoch_005.jpg\n",
      "============================================================\n",
      "\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2/2 2.0it/s 1.0s2.5s\n",
      "                   all         47         81       0.75      0.669      0.739       0.69       0.75      0.669      0.738      0.639\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       7/50      3.16G      1.075      3.267       2.93       1.39         55        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 247/247 5.2it/s 47.1s<0.1s\n",
      "\n",
      "============================================================\n",
      "Generating monitoring visualizations for epoch 6...\n",
      "============================================================\n",
      "‚úì Saved monitoring visualization: epoch_006.jpg\n",
      "============================================================\n",
      "\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2/2 2.4it/s 0.8s2.2s\n",
      "                   all         47         81      0.722      0.705      0.756      0.699      0.722      0.705      0.755      0.657\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       8/50      3.21G      1.074      3.244      2.922      1.395        101        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 247/247 5.4it/s 46.0s<0.2s\n",
      "\n",
      "============================================================\n",
      "Generating monitoring visualizations for epoch 7...\n",
      "============================================================\n",
      "‚úì Saved monitoring visualization: epoch_007.jpg\n",
      "============================================================\n",
      "\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2/2 2.3it/s 0.9s2.3s\n",
      "                   all         47         81      0.663      0.643      0.693      0.649      0.663      0.643      0.693        0.6\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       9/50      3.21G      1.086      3.287      2.914      1.402         73        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 247/247 5.5it/s 44.6s<0.2s\n",
      "\n",
      "============================================================\n",
      "Generating monitoring visualizations for epoch 8...\n",
      "============================================================\n",
      "‚úì Saved monitoring visualization: epoch_008.jpg\n",
      "============================================================\n",
      "\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2/2 2.5it/s 0.8s2.1s\n",
      "                   all         47         81      0.662       0.62      0.676      0.641      0.662       0.62      0.675      0.587\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      10/50      3.21G      1.069      3.216      2.863      1.386         78        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 247/247 5.4it/s 45.5s<0.2s\n",
      "\n",
      "============================================================\n",
      "Generating monitoring visualizations for epoch 9...\n",
      "============================================================\n",
      "‚úì Saved monitoring visualization: epoch_009.jpg\n",
      "============================================================\n",
      "\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2/2 2.6it/s 0.8s1.9s\n",
      "                   all         47         81      0.659      0.597      0.649      0.624      0.659      0.597      0.648      0.568\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      11/50      3.21G      1.052      3.206      2.816      1.373         38        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 247/247 5.5it/s 45.0s<0.2s\n",
      "\n",
      "============================================================\n",
      "Generating monitoring visualizations for epoch 10...\n",
      "============================================================\n",
      "‚úì Saved monitoring visualization: epoch_010.jpg\n",
      "============================================================\n",
      "\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2/2 2.3it/s 0.9s2.2s\n",
      "                   all         47         81      0.654      0.686      0.701      0.662      0.654      0.686      0.701      0.612\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      12/50      3.21G      1.057       3.19       2.83      1.377         82        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 247/247 5.5it/s 44.9s<0.2s\n",
      "\n",
      "============================================================\n",
      "Generating monitoring visualizations for epoch 11...\n",
      "============================================================\n",
      "‚úì Saved monitoring visualization: epoch_011.jpg\n",
      "============================================================\n",
      "\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2/2 2.4it/s 0.8s1.8s\n",
      "                   all         47         81       0.76      0.588      0.662      0.623       0.76      0.588      0.661      0.578\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      13/50      3.21G       1.05      3.193      2.805      1.372         75        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 247/247 5.0it/s 48.9s<0.2s\n",
      "\n",
      "============================================================\n",
      "Generating monitoring visualizations for epoch 12...\n",
      "============================================================\n",
      "‚úì Saved monitoring visualization: epoch_012.jpg\n",
      "============================================================\n",
      "\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2/2 2.1it/s 0.9s2.3s\n",
      "                   all         47         81      0.676      0.591      0.653       0.62      0.676      0.591      0.652      0.567\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      14/50      3.21G      1.053      3.215      2.816      1.377         61        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 247/247 5.4it/s 45.7s<0.2s\n",
      "\n",
      "============================================================\n",
      "Generating monitoring visualizations for epoch 13...\n",
      "============================================================\n",
      "‚úì Saved monitoring visualization: epoch_013.jpg\n",
      "============================================================\n",
      "\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2/2 2.0it/s 1.0s2.5s\n",
      "                   all         47         81      0.589      0.581      0.645      0.613      0.589      0.581      0.646      0.563\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      15/50      3.21G       1.03      3.141       2.77      1.363        103        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 247/247 5.4it/s 45.8s<0.2s\n",
      "\n",
      "============================================================\n",
      "Generating monitoring visualizations for epoch 14...\n",
      "============================================================\n",
      "‚úì Saved monitoring visualization: epoch_014.jpg\n",
      "============================================================\n",
      "\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2/2 2.2it/s 0.9s2.3s\n",
      "                   all         47         81      0.835      0.601      0.717      0.673      0.835      0.601      0.723      0.624\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      16/50      3.21G      1.032      3.171      2.764      1.362         62        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 247/247 5.5it/s 44.8s<0.2s\n",
      "\n",
      "============================================================\n",
      "Generating monitoring visualizations for epoch 15...\n",
      "============================================================\n",
      "‚úì Saved monitoring visualization: epoch_015.jpg\n",
      "============================================================\n",
      "\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2/2 2.7it/s 0.8s1.8s\n",
      "                   all         47         81      0.549      0.679      0.681      0.648      0.549      0.679       0.68      0.577\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      17/50      3.21G      1.035       3.13       2.76      1.362         58        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 247/247 5.6it/s 44.2s<0.2s\n",
      "\n",
      "============================================================\n",
      "Generating monitoring visualizations for epoch 16...\n",
      "============================================================\n",
      "‚úì Saved monitoring visualization: epoch_016.jpg\n",
      "============================================================\n",
      "\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2/2 1.7it/s 1.2s3.0s\n",
      "                   all         47         81      0.776      0.587      0.692      0.649      0.776      0.587      0.691      0.601\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      18/50      3.21G      1.017      3.146      2.712      1.353         65        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 247/247 5.4it/s 46.0s<0.2s\n",
      "\n",
      "============================================================\n",
      "Generating monitoring visualizations for epoch 17...\n",
      "============================================================\n",
      "‚úì Saved monitoring visualization: epoch_017.jpg\n",
      "============================================================\n",
      "\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2/2 2.5it/s 0.8s1.9s\n",
      "                   all         47         81      0.835      0.564      0.686      0.649      0.835      0.564      0.688      0.602\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      19/50      3.21G      1.031      3.126       2.75      1.357         48        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 247/247 5.4it/s 46.1s<0.2s\n",
      "\n",
      "============================================================\n",
      "Generating monitoring visualizations for epoch 18...\n",
      "============================================================\n",
      "‚úì Saved monitoring visualization: epoch_018.jpg\n",
      "============================================================\n",
      "\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2/2 2.4it/s 0.8s2.1s\n",
      "                   all         47         81      0.731      0.595      0.676      0.646      0.731      0.595      0.678      0.586\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      20/50      3.21G      1.022      3.123      2.729      1.358         92        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 247/247 5.2it/s 47.3s<0.2s\n",
      "\n",
      "============================================================\n",
      "Generating monitoring visualizations for epoch 19...\n",
      "============================================================\n",
      "‚úì Saved monitoring visualization: epoch_019.jpg\n",
      "============================================================\n",
      "\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2/2 2.4it/s 0.9s2.2s\n",
      "                   all         47         81      0.734      0.626      0.713      0.679      0.734      0.626      0.713      0.614\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      21/50      3.21G      1.025      3.139       2.73      1.357        147        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 247/247 5.4it/s 46.0s<0.2s\n",
      "\n",
      "============================================================\n",
      "Generating monitoring visualizations for epoch 20...\n",
      "============================================================\n",
      "‚úì Saved monitoring visualization: epoch_020.jpg\n",
      "============================================================\n",
      "\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2/2 2.5it/s 0.8s1.9s\n",
      "                   all         47         81      0.834      0.578      0.702       0.67      0.834      0.578      0.702      0.609\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      22/50      3.21G      1.025      3.109       2.71      1.362         87        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 247/247 5.6it/s 44.2s<0.2s\n",
      "\n",
      "============================================================\n",
      "Generating monitoring visualizations for epoch 21...\n",
      "============================================================\n",
      "‚úì Saved monitoring visualization: epoch_021.jpg\n",
      "============================================================\n",
      "\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2/2 2.5it/s 0.8s2.0s\n",
      "                   all         47         81      0.725      0.582      0.646      0.617      0.725      0.582      0.648      0.563\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      23/50      3.21G      1.024      3.128      2.731      1.365         50        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 247/247 5.3it/s 46.3s<0.2s\n",
      "\n",
      "============================================================\n",
      "Generating monitoring visualizations for epoch 22...\n",
      "============================================================\n",
      "‚úì Saved monitoring visualization: epoch_022.jpg\n",
      "============================================================\n",
      "\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2/2 2.5it/s 0.8s2.0s\n",
      "                   all         47         81      0.811      0.581      0.665      0.637      0.811      0.581      0.666      0.585\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      24/50      3.21G      1.027      3.146      2.714       1.36         68        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 247/247 5.4it/s 45.6s<0.2s\n",
      "\n",
      "============================================================\n",
      "Generating monitoring visualizations for epoch 23...\n",
      "============================================================\n",
      "‚úì Saved monitoring visualization: epoch_023.jpg\n",
      "============================================================\n",
      "\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2/2 2.5it/s 0.8s1.8s\n",
      "                   all         47         81       0.81      0.556      0.684      0.655       0.81      0.556      0.688      0.603\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      25/50      3.21G     0.9952      3.085      2.677      1.343         65        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 247/247 4.9it/s 50.5s<0.2s\n",
      "\n",
      "============================================================\n",
      "Generating monitoring visualizations for epoch 24...\n",
      "============================================================\n",
      "‚úì Saved monitoring visualization: epoch_024.jpg\n",
      "============================================================\n",
      "\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2/2 2.3it/s 0.9s2.2s\n",
      "                   all         47         81      0.645      0.619      0.659      0.633      0.645      0.619      0.666      0.575\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      26/50      3.21G      1.008      3.078      2.669      1.354         49        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 247/247 5.5it/s 45.2s<0.2s\n",
      "\n",
      "============================================================\n",
      "Generating monitoring visualizations for epoch 25...\n",
      "============================================================\n",
      "‚úì Saved monitoring visualization: epoch_025.jpg\n",
      "============================================================\n",
      "\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2/2 2.5it/s 0.8s1.9s\n",
      "                   all         47         81      0.825      0.574      0.684      0.646      0.825      0.574      0.684        0.6\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      27/50      3.21G      1.005      3.084      2.667      1.355         37        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 247/247 5.6it/s 44.4s<0.2s\n",
      "\n",
      "============================================================\n",
      "Generating monitoring visualizations for epoch 26...\n",
      "============================================================\n",
      "‚úì Saved monitoring visualization: epoch_026.jpg\n",
      "============================================================\n",
      "\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2/2 2.3it/s 0.9s2.1s\n",
      "                   all         47         81      0.841      0.581      0.739      0.693      0.841      0.581      0.742      0.645\n",
      "\u001b[34m\u001b[1mEarlyStopping: \u001b[0mTraining stopped early as no improvement observed in last 20 epochs. Best results observed at epoch 7, best model saved as best.pt.\n",
      "To update EarlyStopping(patience=20) pass a new patience value, i.e. `patience=300` or use `patience=0` to disable EarlyStopping.\n",
      "\n",
      "27 epochs completed in 0.471 hours.\n",
      "Optimizer stripped from /home/tonino/projects/ball segmentation/runs/segment/ball_person_trashcan_model_v4/weights/last.pt, 6.0MB\n",
      "Optimizer stripped from /home/tonino/projects/ball segmentation/runs/segment/ball_person_trashcan_model_v4/weights/best.pt, 6.0MB\n",
      "\n",
      "Validating /home/tonino/projects/ball segmentation/runs/segment/ball_person_trashcan_model_v4/weights/best.pt...\n",
      "Ultralytics 8.3.235 üöÄ Python-3.12.10 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 3080 Laptop GPU, 8192MiB)\n",
      "YOLO11n-seg summary (fused): 113 layers, 2,835,153 parameters, 0 gradients, 9.6 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2/2 1.1it/s 1.8s3.5s\n",
      "                   all         47         81      0.722      0.701      0.757      0.698      0.722      0.701      0.755      0.655\n",
      "              red ball         34         34       0.71      0.433      0.558      0.464       0.71      0.433      0.552      0.436\n",
      "                 human         34         34      0.558          1      0.954      0.881      0.558          1      0.954      0.792\n",
      "              trashcan         13         13      0.897      0.671      0.759       0.75      0.897      0.671      0.759      0.739\n",
      "Speed: 1.0ms preprocess, 9.3ms inference, 0.0ms loss, 4.6ms postprocess per image\n",
      "Results saved to \u001b[1m/home/tonino/projects/ball segmentation/runs/segment/ball_person_trashcan_model_v4\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "head_idx = next((i for i, m in enumerate(model.model.model) if 'Detect' in m.__class__.__name__ or 'Segment' in m.__class__.__name__), len(model.model.model) - 1)\n",
    "\n",
    "results = model.train(\n",
    "    data=str(config_path),\n",
    "    epochs=EPOCHS,\n",
    "    freeze=list(range(head_idx)),\n",
    "    batch=BATCH_SIZE,\n",
    "    imgsz=IMG_SIZE,\n",
    "    device=DEVICE,\n",
    "    project=str(RUNS_DIR),\n",
    "    name=project_name,\n",
    "    exist_ok=True,\n",
    "    \n",
    "    # Checkpointing\n",
    "    save=True,\n",
    "    save_period=5,  # Save every 5 epochs\n",
    "    \n",
    "    # Validation\n",
    "    val=True,\n",
    "    \n",
    "    # Data augmentation\n",
    "    # **AUG_CONFIG,\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer='Adam',\n",
    "    lr0=0.001,\n",
    "    lrf=0.01,\n",
    "    momentum=0.937,\n",
    "    weight_decay=0.0005,\n",
    "    \n",
    "    # Loss weights - Ajust√© pour dataset avec trashcans augment√©es\n",
    "    # Avec l'augmentation massive des trashcans, on peut r√©duire cls\n",
    "    box=7.5,\n",
    "    cls=1.0,      # R√©duit de 20.0 √† 1.0 car trashcans maintenant bien repr√©sent√©es\n",
    "    dfl=1.5,\n",
    "    \n",
    "    # Other\n",
    "    patience=20,  # Early stopping\n",
    "    workers=8,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573f1e2e",
   "metadata": {},
   "source": [
    "## Step 6: Load Best Model & Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcded3d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "[runs/segment/ball_person_trashcan_model_v4/weights/best.pt](runs/segment/ball_person_trashcan_model_v4/weights/best.pt)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_model_path = RUNS_DIR / project_name / 'weights' / 'best.pt'\n",
    "best_model_path.display()\n",
    "model = YOLO(best_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4317200b",
   "metadata": {},
   "source": [
    "## Step 7: Evaluate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a02a0a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.235 üöÄ Python-3.12.10 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 3080 Laptop GPU, 8192MiB)\n",
      "YOLO11n-seg summary (fused): 113 layers, 2,835,153 parameters, 0 gradients, 9.6 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 463.9¬±29.2 MB/s, size: 2696.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/tonino/projects/ball segmentation/datasets/ready/full_dataset/val/labels.cache... 47 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 47/47 55.8Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 6.6s/it 19.9s5.5s0\n",
      "                   all         47         81      0.723      0.705      0.757        0.7      0.723      0.705      0.755      0.655\n",
      "              red ball         34         34      0.711      0.435      0.558      0.467      0.711      0.435      0.553      0.433\n",
      "                 human         34         34      0.558          1      0.954      0.882      0.558          1      0.954      0.792\n",
      "              trashcan         13         13      0.898      0.681      0.759       0.75      0.898      0.681      0.759      0.739\n",
      "Speed: 9.5ms preprocess, 32.4ms inference, 0.0ms loss, 13.6ms postprocess per image\n",
      "Results saved to \u001b[1m/home/tonino/projects/ball segmentation/runs/segment/val18\u001b[0m\n",
      "\n",
      "============================================================\n",
      "VALIDATION METRICS\n",
      "============================================================\n",
      "Box mAP50: 0.7571\n",
      "Box mAP50-95: 0.6995\n",
      "Mask mAP50: 0.7553\n",
      "Mask mAP50-95: 0.6548\n",
      "\n",
      "============================================================\n",
      "PER-CLASS METRICS (Segmentation)\n",
      "============================================================\n",
      "red ball    : mAP50=0.0000, mAP50-95=0.0000\n",
      "human       : mAP50=0.0000, mAP50-95=0.0000\n",
      "trashcan    : mAP50=0.0000, mAP50-95=0.0000\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Validation metrics\n",
    "metrics = model.val()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VALIDATION METRICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Box mAP50: {metrics.box.map50:.4f}\")\n",
    "print(f\"Box mAP50-95: {metrics.box.map:.4f}\")\n",
    "print(f\"Mask mAP50: {metrics.seg.map50:.4f}\")\n",
    "print(f\"Mask mAP50-95: {metrics.seg.map:.4f}\")\n",
    "\n",
    "# Per-class metrics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PER-CLASS METRICS (Segmentation)\")\n",
    "print(\"=\"*60)\n",
    "class_names = ['red ball', 'human', 'trashcan']\n",
    "for i, class_name in enumerate(class_names):\n",
    "    try:\n",
    "        map50 = metrics.seg.map50_per_class[i] if hasattr(metrics.seg, 'map50_per_class') else 0\n",
    "        map_val = metrics.seg.map_per_class[i] if hasattr(metrics.seg, 'map_per_class') else 0\n",
    "        print(f\"{class_name:12s}: mAP50={map50:.4f}, mAP50-95={map_val:.4f}\")\n",
    "    except:\n",
    "        print(f\"{class_name:12s}: metrics not available\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3d36b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "[runs/segment/ball_person_trashcan_model_v4/weights/best.pt](runs/segment/ball_person_trashcan_model_v4/weights/best.pt)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last model: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "[runs/segment/ball_person_trashcan_model_v4/weights/last.pt](runs/segment/ball_person_trashcan_model_v4/weights/last.pt)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "[runs/segment/ball_person_trashcan_model_v4](runs/segment/ball_person_trashcan_model_v4)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Multi-class monitoring visualizations: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "[runs/segment/ball_person_trashcan_model_v4/training_monitor](runs/segment/ball_person_trashcan_model_v4/training_monitor)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Find best checkpoint\n",
    "model_dir = RUNS_DIR / project_name\n",
    "best_model = model_dir / 'weights' / 'best.pt'\n",
    "last_model = model_dir / 'weights' / 'last.pt'\n",
    "\n",
    "print(f\"Best model: \")\n",
    "best_model.display()\n",
    "print(f\"Last model: \")\n",
    "last_model.display()\n",
    "print(f\"Results: \")\n",
    "model_dir.display()\n",
    "print(f\"\\nMulti-class monitoring visualizations: \")\n",
    "(model_dir / 'training_monitor').display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f94a189",
   "metadata": {},
   "source": [
    "## Step 8: Visualize Training Progress Evolution\n",
    "\n",
    "Review monitoring visualizations showing how segmentation improved for all classes over epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac1cf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all monitoring visualizations\n",
    "monitor_dir = RUNS_DIR / project_name / 'training_monitor'\n",
    "\n",
    "if monitor_dir.exists():\n",
    "    viz_files = sorted(monitor_dir.glob(\"epoch_*.jpg\"))\n",
    "    print(f\"Found {len(viz_files)} monitoring visualizations:\")\n",
    "    for viz_file in viz_files:\n",
    "        viz_file.display()\n",
    "    \n",
    "    if len(viz_files) > 0:\n",
    "        print(f\"\\nüí° Tip: Open the images in {monitor_dir} to see how segmentation evolved for all classes!\")\n",
    "        print(f\"   You can use an image viewer or VS Code to flip through them chronologically.\")\n",
    "        print(f\"   Each image shows detections with: Balls | Humans | Trashcans\")\n",
    "else:\n",
    "    print(\"No monitoring visualizations found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bc9c5e",
   "metadata": {},
   "source": [
    "## Step 9: Test on Sample Images (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f791323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on 47 sample images...\n",
      "\n",
      "image 1/1 /home/tonino/projects/ball segmentation/datasets/ready/full_dataset/val/images/366acb21b00b40588372736b95776fac.jpg: 640x480 1 red ball, 1 human, 31.5ms\n",
      "Speed: 4.4ms preprocess, 31.5ms inference, 6.8ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Results saved to \u001b[1m/home/tonino/projects/ball segmentation/runs/segment/predict10\u001b[0m\n",
      "  ‚úì 366acb21b00b40588372736b95776fac.jpg\n",
      "\n",
      "image 1/1 /home/tonino/projects/ball segmentation/datasets/ready/full_dataset/val/images/0d4db7c113776bc0a401d833d556df84.jpg: 640x480 1 red ball, 1 human, 33.6ms\n",
      "Speed: 2.4ms preprocess, 33.6ms inference, 6.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Results saved to \u001b[1m/home/tonino/projects/ball segmentation/runs/segment/predict10\u001b[0m\n",
      "  ‚úì 0d4db7c113776bc0a401d833d556df84.jpg\n",
      "\n",
      "image 1/1 /home/tonino/projects/ball segmentation/datasets/ready/full_dataset/val/images/a6b631525ff8dc9bc26bb53e97481606.jpg: 640x480 1 human, 12.2ms\n",
      "Speed: 2.2ms preprocess, 12.2ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Results saved to \u001b[1m/home/tonino/projects/ball segmentation/runs/segment/predict10\u001b[0m\n",
      "  ‚úì a6b631525ff8dc9bc26bb53e97481606.jpg\n",
      "\n",
      "image 1/1 /home/tonino/projects/ball segmentation/datasets/ready/full_dataset/val/images/6ea339039c57d22328a8a9097181b4cb.jpg: 640x480 1 human, 32.8ms\n",
      "Speed: 2.2ms preprocess, 32.8ms inference, 5.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Results saved to \u001b[1m/home/tonino/projects/ball segmentation/runs/segment/predict10\u001b[0m\n",
      "  ‚úì 6ea339039c57d22328a8a9097181b4cb.jpg\n",
      "\n",
      "image 1/1 /home/tonino/projects/ball segmentation/datasets/ready/full_dataset/val/images/fc28e08f6ae00529bd6e6f3092bc589b.jpg: 640x480 1 human, 12.9ms\n",
      "Speed: 2.1ms preprocess, 12.9ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Results saved to \u001b[1m/home/tonino/projects/ball segmentation/runs/segment/predict10\u001b[0m\n",
      "  ‚úì fc28e08f6ae00529bd6e6f3092bc589b.jpg\n",
      "\n",
      "image 1/1 /home/tonino/projects/ball segmentation/datasets/ready/full_dataset/val/images/256c86658031f676fe03b3516f9a899b.jpg: 640x480 1 human, 23.5ms\n",
      "Speed: 2.4ms preprocess, 23.5ms inference, 4.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Results saved to \u001b[1m/home/tonino/projects/ball segmentation/runs/segment/predict10\u001b[0m\n",
      "  ‚úì 256c86658031f676fe03b3516f9a899b.jpg\n",
      "\n",
      "image 1/1 /home/tonino/projects/ball segmentation/datasets/ready/full_dataset/val/images/de96513c1cdccc864e7b7e809162d06c.jpg: 640x480 1 red ball, 1 trashcan, 13.0ms\n",
      "Speed: 2.3ms preprocess, 13.0ms inference, 2.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Results saved to \u001b[1m/home/tonino/projects/ball segmentation/runs/segment/predict10\u001b[0m\n",
      "  ‚úì de96513c1cdccc864e7b7e809162d06c.jpg\n",
      "\n",
      "image 1/1 /home/tonino/projects/ball segmentation/datasets/ready/full_dataset/val/images/b6fe1fc46e4ad868193c424070da1e34.jpg: 640x480 1 red ball, 1 trashcan, 20.4ms\n",
      "Speed: 2.5ms preprocess, 20.4ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Results saved to \u001b[1m/home/tonino/projects/ball segmentation/runs/segment/predict10\u001b[0m\n",
      "  ‚úì b6fe1fc46e4ad868193c424070da1e34.jpg\n",
      "\n",
      "image 1/1 /home/tonino/projects/ball segmentation/datasets/ready/full_dataset/val/images/088610ac49bfde8470097a40c8b749d7.jpg: 640x480 1 red ball, 1 human, 25.1ms\n",
      "Speed: 2.1ms preprocess, 25.1ms inference, 4.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Results saved to \u001b[1m/home/tonino/projects/ball segmentation/runs/segment/predict10\u001b[0m\n",
      "  ‚úì 088610ac49bfde8470097a40c8b749d7.jpg\n",
      "\n",
      "image 1/1 /home/tonino/projects/ball segmentation/datasets/ready/full_dataset/val/images/0ac39c0cadb518e8bcc8de34579b625f.jpg: 640x480 1 human, 12.4ms\n",
      "Speed: 2.9ms preprocess, 12.4ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Results saved to \u001b[1m/home/tonino/projects/ball segmentation/runs/segment/predict10\u001b[0m\n",
      "  ‚úì 0ac39c0cadb518e8bcc8de34579b625f.jpg\n",
      "\n",
      "image 1/1 /home/tonino/projects/ball segmentation/datasets/ready/full_dataset/val/images/042b9e09b3832319fe998ffb4bf44edd.jpg: 640x480 1 red ball, 2 humans, 23.4ms\n",
      "Speed: 2.1ms preprocess, 23.4ms inference, 5.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Results saved to \u001b[1m/home/tonino/projects/ball segmentation/runs/segment/predict10\u001b[0m\n",
      "  ‚úì 042b9e09b3832319fe998ffb4bf44edd.jpg\n",
      "\n",
      "image 1/1 /home/tonino/projects/ball segmentation/datasets/ready/full_dataset/val/images/9666760cde1bedabf437f3dbfc95f891.jpg: 640x480 1 human, 31.9ms\n",
      "Speed: 2.9ms preprocess, 31.9ms inference, 5.8ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Results saved to \u001b[1m/home/tonino/projects/ball segmentation/runs/segment/predict10\u001b[0m\n",
      "  ‚úì 9666760cde1bedabf437f3dbfc95f891.jpg\n",
      "\n",
      "image 1/1 /home/tonino/projects/ball segmentation/datasets/ready/full_dataset/val/images/d8c811abffbf5da563ee451592a24dcd.jpg: 640x480 2 red balls, 1 trashcan, 12.7ms\n",
      "Speed: 2.3ms preprocess, 12.7ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Results saved to \u001b[1m/home/tonino/projects/ball segmentation/runs/segment/predict10\u001b[0m\n",
      "  ‚úì d8c811abffbf5da563ee451592a24dcd.jpg\n",
      "\n",
      "image 1/1 /home/tonino/projects/ball segmentation/datasets/ready/full_dataset/val/images/a0efc5e34c785282ee484a5e64e1c8ea.jpg: 640x480 1 red ball, 4 humans, 12.4ms\n",
      "Speed: 2.8ms preprocess, 12.4ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Results saved to \u001b[1m/home/tonino/projects/ball segmentation/runs/segment/predict10\u001b[0m\n",
      "  ‚úì a0efc5e34c785282ee484a5e64e1c8ea.jpg\n",
      "\n",
      "image 1/1 /home/tonino/projects/ball segmentation/datasets/ready/full_dataset/val/images/018c4798e1caea89fb7e38ab66fce0f9.jpg: 640x480 5 humans, 1 trashcan, 12.3ms\n",
      "Speed: 2.4ms preprocess, 12.3ms inference, 2.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Results saved to \u001b[1m/home/tonino/projects/ball segmentation/runs/segment/predict10\u001b[0m\n",
      "  ‚úì 018c4798e1caea89fb7e38ab66fce0f9.jpg\n",
      "\n",
      "image 1/1 /home/tonino/projects/ball segmentation/datasets/ready/full_dataset/val/images/daa535a4bc8e19b7dd0f0bdf7d891459.jpg: 640x480 1 red ball, 1 human, 12.6ms\n",
      "Speed: 2.5ms preprocess, 12.6ms inference, 2.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Results saved to \u001b[1m/home/tonino/projects/ball segmentation/runs/segment/predict10\u001b[0m\n",
      "  ‚úì daa535a4bc8e19b7dd0f0bdf7d891459.jpg\n",
      "\n",
      "image 1/1 /home/tonino/projects/ball segmentation/datasets/ready/full_dataset/val/images/1ff6f7af4e054a1c879c6485194e44cb.jpg: 640x480 2 red balls, 1 human, 15.0ms\n",
      "Speed: 2.6ms preprocess, 15.0ms inference, 3.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Results saved to \u001b[1m/home/tonino/projects/ball segmentation/runs/segment/predict10\u001b[0m\n",
      "  ‚úì 1ff6f7af4e054a1c879c6485194e44cb.jpg\n",
      "\n",
      "image 1/1 /home/tonino/projects/ball segmentation/datasets/ready/full_dataset/val/images/1383c4a58a26c7238fbae31ec8e4e660.jpg: 640x480 1 red ball, 1 trashcan, 12.6ms\n",
      "Speed: 2.9ms preprocess, 12.6ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Results saved to \u001b[1m/home/tonino/projects/ball segmentation/runs/segment/predict10\u001b[0m\n",
      "  ‚úì 1383c4a58a26c7238fbae31ec8e4e660.jpg\n",
      "\n",
      "image 1/1 /home/tonino/projects/ball segmentation/datasets/ready/full_dataset/val/images/2b9a10a9f16323fd28c304e1b18f4787.jpg: 640x480 1 red ball, 4 humans, 12.2ms\n",
      "Speed: 2.8ms preprocess, 12.2ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Results saved to \u001b[1m/home/tonino/projects/ball segmentation/runs/segment/predict10\u001b[0m\n",
      "  ‚úì 2b9a10a9f16323fd28c304e1b18f4787.jpg\n",
      "\n",
      "image 1/1 /home/tonino/projects/ball segmentation/datasets/ready/full_dataset/val/images/bbabaf39558fe91afb1a7ed8f1693e04.jpg: 640x480 1 trashcan, 12.1ms\n",
      "Speed: 2.8ms preprocess, 12.1ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Results saved to \u001b[1m/home/tonino/projects/ball segmentation/runs/segment/predict10\u001b[0m\n",
      "  ‚úì bbabaf39558fe91afb1a7ed8f1693e04.jpg\n",
      "\n",
      "image 1/1 /home/tonino/projects/ball segmentation/datasets/ready/full_dataset/val/images/19d18a004fb8ffcae5740d3bc9f87d78.jpg: 640x480 1 human, 11.9ms\n",
      "Speed: 2.2ms preprocess, 11.9ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Results saved to \u001b[1m/home/tonino/projects/ball segmentation/runs/segment/predict10\u001b[0m\n",
      "  ‚úì 19d18a004fb8ffcae5740d3bc9f87d78.jpg\n",
      "\n",
      "image 1/1 /home/tonino/projects/ball segmentation/datasets/ready/full_dataset/val/images/9d5cda5391d6ae193428fbb451d0c905.jpg: 640x480 1 human, 12.3ms\n",
      "Speed: 2.7ms preprocess, 12.3ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Results saved to \u001b[1m/home/tonino/projects/ball segmentation/runs/segment/predict10\u001b[0m\n",
      "  ‚úì 9d5cda5391d6ae193428fbb451d0c905.jpg\n",
      "\n",
      "image 1/1 /home/tonino/projects/ball segmentation/datasets/ready/full_dataset/val/images/f7c36eabf5a95cf548a382f4a6b49050.jpg: 640x480 1 red ball, 1 trashcan, 12.0ms\n",
      "Speed: 2.7ms preprocess, 12.0ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Results saved to \u001b[1m/home/tonino/projects/ball segmentation/runs/segment/predict10\u001b[0m\n",
      "  ‚úì f7c36eabf5a95cf548a382f4a6b49050.jpg\n",
      "\n",
      "image 1/1 /home/tonino/projects/ball segmentation/datasets/ready/full_dataset/val/images/b2dcc2d8cc39ca13b5cf2e24bf036d62.jpg: 640x480 1 human, 12.9ms\n",
      "Speed: 2.7ms preprocess, 12.9ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Results saved to \u001b[1m/home/tonino/projects/ball segmentation/runs/segment/predict10\u001b[0m\n",
      "  ‚úì b2dcc2d8cc39ca13b5cf2e24bf036d62.jpg\n",
      "\n",
      "image 1/1 /home/tonino/projects/ball segmentation/datasets/ready/full_dataset/val/images/205637f0b04837de28f8d5c031b263bc.jpg: 640x480 1 red ball, 1 human, 13.0ms\n",
      "Speed: 2.0ms preprocess, 13.0ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Results saved to \u001b[1m/home/tonino/projects/ball segmentation/runs/segment/predict10\u001b[0m\n",
      "  ‚úì 205637f0b04837de28f8d5c031b263bc.jpg\n",
      "\n",
      "image 1/1 /home/tonino/projects/ball segmentation/datasets/ready/full_dataset/val/images/eb36d794d60789ff4af3a2b30beb228e.jpg: 640x480 1 red ball, 8 humans, 12.0ms\n",
      "Speed: 2.4ms preprocess, 12.0ms inference, 11.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Results saved to \u001b[1m/home/tonino/projects/ball segmentation/runs/segment/predict10\u001b[0m\n",
      "  ‚úì eb36d794d60789ff4af3a2b30beb228e.jpg\n",
      "\n",
      "image 1/1 /home/tonino/projects/ball segmentation/datasets/ready/full_dataset/val/images/e912cfb70231fd6c25c18171bdee8674.jpg: 640x480 1 red ball, 2 humans, 12.2ms\n",
      "Speed: 2.3ms preprocess, 12.2ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Results saved to \u001b[1m/home/tonino/projects/ball segmentation/runs/segment/predict10\u001b[0m\n",
      "  ‚úì e912cfb70231fd6c25c18171bdee8674.jpg\n",
      "\n",
      "image 1/1 /home/tonino/projects/ball segmentation/datasets/ready/full_dataset/val/images/268049ede7cfd5c42204b885785fcfc1.jpg: 640x480 1 red ball, 3 humans, 24.7ms\n",
      "Speed: 2.7ms preprocess, 24.7ms inference, 3.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Results saved to \u001b[1m/home/tonino/projects/ball segmentation/runs/segment/predict10\u001b[0m\n",
      "  ‚úì 268049ede7cfd5c42204b885785fcfc1.jpg\n",
      "\n",
      "image 1/1 /home/tonino/projects/ball segmentation/datasets/ready/full_dataset/val/images/cbbcee9485e646dbaf274f323e512852.jpg: 640x480 1 red ball, 1 human, 12.2ms\n",
      "Speed: 2.5ms preprocess, 12.2ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Results saved to \u001b[1m/home/tonino/projects/ball segmentation/runs/segment/predict10\u001b[0m\n",
      "  ‚úì cbbcee9485e646dbaf274f323e512852.jpg\n",
      "\n",
      "image 1/1 /home/tonino/projects/ball segmentation/datasets/ready/full_dataset/val/images/a56abe595542342e6049fb8dc0fccf8c.jpg: 640x480 1 red ball, 5 humans, 12.1ms\n",
      "Speed: 3.5ms preprocess, 12.1ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Results saved to \u001b[1m/home/tonino/projects/ball segmentation/runs/segment/predict10\u001b[0m\n",
      "  ‚úì a56abe595542342e6049fb8dc0fccf8c.jpg\n",
      "\n",
      "image 1/1 /home/tonino/projects/ball segmentation/datasets/ready/full_dataset/val/images/f6a796ac0c1825320b64a4d3ece90068.jpg: 640x480 2 red balls, 6 humans, 12.3ms\n",
      "Speed: 2.5ms preprocess, 12.3ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Results saved to \u001b[1m/home/tonino/projects/ball segmentation/runs/segment/predict10\u001b[0m\n",
      "  ‚úì f6a796ac0c1825320b64a4d3ece90068.jpg\n",
      "\n",
      "image 1/1 /home/tonino/projects/ball segmentation/datasets/ready/full_dataset/val/images/0af6fd9d3636fd6403c7ada6c8a10530.jpg: 640x480 1 red ball, 3 humans, 12.2ms\n",
      "Speed: 2.4ms preprocess, 12.2ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Results saved to \u001b[1m/home/tonino/projects/ball segmentation/runs/segment/predict10\u001b[0m\n",
      "  ‚úì 0af6fd9d3636fd6403c7ada6c8a10530.jpg\n",
      "\n",
      "image 1/1 /home/tonino/projects/ball segmentation/datasets/ready/full_dataset/val/images/11cb69ea752cefa3a8cad413598028de.jpg: 640x480 1 red ball, 7 humans, 12.6ms\n",
      "Speed: 2.2ms preprocess, 12.6ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Results saved to \u001b[1m/home/tonino/projects/ball segmentation/runs/segment/predict10\u001b[0m\n",
      "  ‚úì 11cb69ea752cefa3a8cad413598028de.jpg\n",
      "\n",
      "image 1/1 /home/tonino/projects/ball segmentation/datasets/ready/full_dataset/val/images/380fca853fd511698abe9f0c24285f68.jpg: 640x480 1 red ball, 1 trashcan, 13.0ms\n",
      "Speed: 2.4ms preprocess, 13.0ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Results saved to \u001b[1m/home/tonino/projects/ball segmentation/runs/segment/predict10\u001b[0m\n",
      "  ‚úì 380fca853fd511698abe9f0c24285f68.jpg\n",
      "\n",
      "image 1/1 /home/tonino/projects/ball segmentation/datasets/ready/full_dataset/val/images/7dbbf64f4372e8af631d5b3261e5d6aa.jpg: 640x480 1 human, 13.6ms\n",
      "Speed: 2.4ms preprocess, 13.6ms inference, 2.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Results saved to \u001b[1m/home/tonino/projects/ball segmentation/runs/segment/predict10\u001b[0m\n",
      "  ‚úì 7dbbf64f4372e8af631d5b3261e5d6aa.jpg\n",
      "\n",
      "image 1/1 /home/tonino/projects/ball segmentation/datasets/ready/full_dataset/val/images/29b6473d55641a2d6a06276b9357090f.jpg: 640x480 1 human, 12.6ms\n",
      "Speed: 2.7ms preprocess, 12.6ms inference, 3.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Results saved to \u001b[1m/home/tonino/projects/ball segmentation/runs/segment/predict10\u001b[0m\n",
      "  ‚úì 29b6473d55641a2d6a06276b9357090f.jpg\n",
      "\n",
      "image 1/1 /home/tonino/projects/ball segmentation/datasets/ready/full_dataset/val/images/9ecb763ade7cb05b25ab6f784d29744f.jpg: 640x480 1 red ball, 3 humans, 15.7ms\n",
      "Speed: 2.5ms preprocess, 15.7ms inference, 3.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Results saved to \u001b[1m/home/tonino/projects/ball segmentation/runs/segment/predict10\u001b[0m\n",
      "  ‚úì 9ecb763ade7cb05b25ab6f784d29744f.jpg\n",
      "\n",
      "image 1/1 /home/tonino/projects/ball segmentation/datasets/ready/full_dataset/val/images/f6448893a6bb9e4fa65a51298149599d.jpg: 640x480 1 human, 12.9ms\n",
      "Speed: 2.5ms preprocess, 12.9ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Results saved to \u001b[1m/home/tonino/projects/ball segmentation/runs/segment/predict10\u001b[0m\n",
      "  ‚úì f6448893a6bb9e4fa65a51298149599d.jpg\n",
      "\n",
      "image 1/1 /home/tonino/projects/ball segmentation/datasets/ready/full_dataset/val/images/d3ca9f15b02c341da0ca6cbb8d763a09.jpg: 640x480 2 trashcans, 13.4ms\n",
      "Speed: 2.6ms preprocess, 13.4ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Results saved to \u001b[1m/home/tonino/projects/ball segmentation/runs/segment/predict10\u001b[0m\n",
      "  ‚úì d3ca9f15b02c341da0ca6cbb8d763a09.jpg\n",
      "\n",
      "image 1/1 /home/tonino/projects/ball segmentation/datasets/ready/full_dataset/val/images/1104093f8577602bb0425f1ccd118de9.jpg: 640x480 1 red ball, 3 humans, 12.5ms\n",
      "Speed: 2.3ms preprocess, 12.5ms inference, 2.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Results saved to \u001b[1m/home/tonino/projects/ball segmentation/runs/segment/predict10\u001b[0m\n",
      "  ‚úì 1104093f8577602bb0425f1ccd118de9.jpg\n",
      "\n",
      "image 1/1 /home/tonino/projects/ball segmentation/datasets/ready/full_dataset/val/images/98a670c874d1cefcacd71b855d29769a.jpg: 640x480 1 red ball, 1 trashcan, 13.0ms\n",
      "Speed: 2.6ms preprocess, 13.0ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Results saved to \u001b[1m/home/tonino/projects/ball segmentation/runs/segment/predict10\u001b[0m\n",
      "  ‚úì 98a670c874d1cefcacd71b855d29769a.jpg\n",
      "\n",
      "image 1/1 /home/tonino/projects/ball segmentation/datasets/ready/full_dataset/val/images/d8ce4c69557862b45c0f7c18d0d3a412.jpg: 640x480 2 red balls, 1 human, 29.7ms\n",
      "Speed: 2.5ms preprocess, 29.7ms inference, 5.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Results saved to \u001b[1m/home/tonino/projects/ball segmentation/runs/segment/predict10\u001b[0m\n",
      "  ‚úì d8ce4c69557862b45c0f7c18d0d3a412.jpg\n",
      "\n",
      "image 1/1 /home/tonino/projects/ball segmentation/datasets/ready/full_dataset/val/images/e1117a3b7a26ccff17879b5789f02d0b.jpg: 640x480 2 humans, 13.3ms\n",
      "Speed: 2.4ms preprocess, 13.3ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Results saved to \u001b[1m/home/tonino/projects/ball segmentation/runs/segment/predict10\u001b[0m\n",
      "  ‚úì e1117a3b7a26ccff17879b5789f02d0b.jpg\n",
      "\n",
      "image 1/1 /home/tonino/projects/ball segmentation/datasets/ready/full_dataset/val/images/d634b2ebe37b5735c16949926ae2d7bb.jpg: 640x480 4 humans, 14.3ms\n",
      "Speed: 2.8ms preprocess, 14.3ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Results saved to \u001b[1m/home/tonino/projects/ball segmentation/runs/segment/predict10\u001b[0m\n",
      "  ‚úì d634b2ebe37b5735c16949926ae2d7bb.jpg\n",
      "\n",
      "image 1/1 /home/tonino/projects/ball segmentation/datasets/ready/full_dataset/val/images/8dad2db8365784c4b162442bd59eb5c7.jpg: 640x480 1 trashcan, 18.7ms\n",
      "Speed: 2.5ms preprocess, 18.7ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Results saved to \u001b[1m/home/tonino/projects/ball segmentation/runs/segment/predict10\u001b[0m\n",
      "  ‚úì 8dad2db8365784c4b162442bd59eb5c7.jpg\n",
      "\n",
      "image 1/1 /home/tonino/projects/ball segmentation/datasets/ready/full_dataset/val/images/7efdcca6d152b996a788f1694d0f62f9.jpg: 640x480 1 red ball, 1 trashcan, 13.8ms\n",
      "Speed: 2.6ms preprocess, 13.8ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Results saved to \u001b[1m/home/tonino/projects/ball segmentation/runs/segment/predict10\u001b[0m\n",
      "  ‚úì 7efdcca6d152b996a788f1694d0f62f9.jpg\n",
      "\n",
      "image 1/1 /home/tonino/projects/ball segmentation/datasets/ready/full_dataset/val/images/b5a9eceece731289df933a2d89cd24db.jpg: 640x480 1 red ball, 1 human, 13.5ms\n",
      "Speed: 2.4ms preprocess, 13.5ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Results saved to \u001b[1m/home/tonino/projects/ball segmentation/runs/segment/predict10\u001b[0m\n",
      "  ‚úì b5a9eceece731289df933a2d89cd24db.jpg\n",
      "\n",
      "Results saved to:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "[runs/segment/ball_person_trashcan_model_v4](runs/segment/ball_person_trashcan_model_v4)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test on validation images (sample from val set)\n",
    "test_images = list((YOLO_DATASET / \"val\" / \"images\").glob(\"*\"))\n",
    "\n",
    "print(f\"Testing on {len(test_images)} sample images...\")\n",
    "\n",
    "for img_path in test_images:\n",
    "    results = model.predict(str(img_path), save=True, conf=0.1)\n",
    "    print(f\"  ‚úì {img_path.name}\")\n",
    "\n",
    "print(f\"\\nResults saved to:\")\n",
    "(RUNS_DIR / project_name).display()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
